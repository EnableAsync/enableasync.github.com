<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>幸福论</title>
    <url>/uncategorized/happiness/</url>
    <content><![CDATA[<h1 id="Async-的一些想法"><a href="#Async-的一些想法" class="headerlink" title="Async 的一些想法"></a>Async 的一些想法</h1><p>文明其精神，野蛮其体魄。</p>
<p>在这篇文章里记录关于幸福的观点和思考。</p>
<p>幸福不是一件容易的事：她很难求之于自身，但要想在别处得到则不可能。——尚福尔</p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>在这里介绍的内容主要从平常、实用的角度出发，所以保留了与此角度相关的谬误。</p>
<p>一些补充书籍：卡丹奴斯本的《论逆境》。</p>
<p>一般来说，各个时代的智者们，都说过同样的话语，而愚人们——也就是各个时代数不胜数的大多数人——也做着恰恰相反的同一样事情。</p>
<h1 id="一些基本的划分"><a href="#一些基本的划分" class="headerlink" title="一些基本的划分"></a>一些基本的划分</h1><p>亚里士多德把人生能够得到的好处分为三类——外在之物、人的灵魂和人的身体。</p>
<p>在这里，叔本华认为决定凡人命运的根本差别在于三项内容：</p>
<ul>
<li>人的<strong>自身</strong>，最广泛意义上属于人的个性的东西。健康、力量、外貌、气质、道德品格、精神智力、潜在发展。</li>
<li>人所拥有的<strong>身外之物</strong>，财产和其他占有物。</li>
<li>人向其他人所<strong>显示的样子</strong>，人在其他人眼中的样子，亦即人们对他的看法。他人的看法包括：<ul>
<li>名誉</li>
<li>地位</li>
<li>名声</li>
</ul>
</li>
</ul>
<p>其中，最重要的是第一项，也就是<strong>人的自身</strong>，是因为第一项的差别是由大自然决定的，而后两项只是出自人为的划分。</p>
<p>对于一个人的幸福，甚至对于他的整个存在方式，最重要的就是他自身的内在素质，内在素质直接决定了这个人能否得到内心的幸福，因为内心的快乐和痛苦首先是人的感情、意欲和思想的产物。</p>
<p>第二项和第三项属于自身以外的所有事物，他们想要起作用的方式是间接的，通过影响我们的自身感情来实现的。因此，同一样外在的事物和同一样的境遇，对于我们每个人的影响都不尽相同；即使是生活在同一个环境中的每一个人，都生活在不同的世界中。因为与一个人直接相关的是一个人对事物的看法、他的情感和意欲活动。</p>
]]></content>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>Internlm-01-书生·浦语大模型全链路开源体系</title>
    <url>/internlm/internlm-01/</url>
    <content><![CDATA[<h1 id="书生·浦语大模型全链路开源体系"><a href="#书生·浦语大模型全链路开源体系" class="headerlink" title="书生·浦语大模型全链路开源体系"></a>书生·浦语大模型全链路开源体系</h1><h2 id="大模型概述"><a href="#大模型概述" class="headerlink" title="大模型概述"></a>大模型概述</h2><h3 id="大模型成为了热门关键词"><a href="#大模型成为了热门关键词" class="headerlink" title="大模型成为了热门关键词"></a>大模型成为了热门关键词</h3><p>大模型成为发展通用人工智能的重要途经，图中展示了大模型的热度增长和 OpenAI GPT 系列的迭代过程。</p>
<p><img  src="image.png"  ><span class="image-caption">大模型成为热门关键词</span></p>
<h3 id="大模型成为了发展通用人工智能的重要途径"><a href="#大模型成为了发展通用人工智能的重要途径" class="headerlink" title="大模型成为了发展通用人工智能的重要途径"></a>大模型成为了发展通用人工智能的重要途径</h3><p><img  src="image-20240104194412083.png"  ><span class="image-caption">大模型成为了发展通用人工智能的重要途径</span></p>
<h2 id="书生·浦语大模型"><a href="#书生·浦语大模型" class="headerlink" title="书生·浦语大模型"></a>书生·浦语大模型</h2><h3 id="开源历程"><a href="#开源历程" class="headerlink" title="开源历程"></a>开源历程</h3><p><img  src="image-20240104194442165.png"  ><span class="image-caption">书生·浦语的开源历程</span></p>
<p>书生·浦语大模型从 6.7 开始，开源了 InternLM-7B、InternLM-Chat-7B、Lagent、InternLM-20B 等项目。</p>
<h3 id="系列模型"><a href="#系列模型" class="headerlink" title="系列模型"></a>系列模型</h3><p><img  src="image-20240104194623711.png"  ><span class="image-caption">书生·浦语系列模型</span></p>
<h3 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h3><p><img  src="image-20240104194715248.png"  ><span class="image-caption">性能比较</span></p>
<p>可以看出，书生·浦语 20B 模型全面领先相近量级的开源模型，并且达到了和 Llama2-70B 相近的水平。</p>
<h2 id="书生·浦语全链条开源体系"><a href="#书生·浦语全链条开源体系" class="headerlink" title="书生·浦语全链条开源体系"></a>书生·浦语全链条开源体系</h2><h3 id="体系概述"><a href="#体系概述" class="headerlink" title="体系概述"></a>体系概述</h3><p><strong><a href="https://github.com/orgs/InternLM/repositories?type=all">InternLM (github.com)</a></strong></p>
<p><img  src="image-20240104194911061.png"  ><span class="image-caption">体系概述</span></p>
<h3 id="数据开源"><a href="#数据开源" class="headerlink" title="数据开源"></a>数据开源</h3><p><img  src="image-20240104194948553.png"  ><span class="image-caption">数据开源</span></p>
<h3 id="预训练开源"><a href="#预训练开源" class="headerlink" title="预训练开源"></a>预训练开源</h3><p><img  src="image-20240104195041865.png"  ><span class="image-caption">预训练开源</span></p>
<h3 id="微调开源"><a href="#微调开源" class="headerlink" title="微调开源"></a>微调开源</h3><p><strong><a href="https://github.com/InternLM/xtuner">InternLM/xtuner: A toolkit for efficiently fine-tuning LLM (InternLM, Llama, Baichuan, Qwen, ChatGLM) (github.com)</a></strong></p>
<p><img  src="image-20240104195250624.png"  ><span class="image-caption">微调开源</span></p>
<h3 id="评测开源"><a href="#评测开源" class="headerlink" title="评测开源"></a>评测开源</h3><p><strong><a href="https://github.com/open-compass/OpenCompass/">open-compass/opencompass: OpenCompass is an LLM evaluation platform, supporting a wide range of models (LLaMA, LLaMa2, ChatGLM2, ChatGPT, Claude, etc) over 50+ datasets. (github.com)</a></strong></p>
<p><img  src="image-20240104195322676.png"  ><span class="image-caption">评测开源-题目类型</span></p>
<p><img  src="image-20240104195342281.png"  ><span class="image-caption">评测开源-OpenCompass</span></p>
<p><img  src="image-20240104195354357.png"  ><span class="image-caption">OpenCompass 架构</span></p>
<h3 id="部署开源"><a href="#部署开源" class="headerlink" title="部署开源"></a>部署开源</h3><p><strong><a href="https://github.com/InternLM/lmdeploy">InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs. (github.com)</a></strong></p>
<p><img  src="image-20240104195614821.png"  ><span class="image-caption">部署开源</span></p>
<h3 id="智能体开源"><a href="#智能体开源" class="headerlink" title="智能体开源"></a>智能体开源</h3><p><strong><a href="https://github.com/InternLM/lagent">InternLM/lagent: A lightweight framework for building LLM-based agents (github.com)</a></strong></p>
<p><img  src="image-20240104195658280.png"  ><span class="image-caption">智能体开源</span></p>
<h3 id="智能体工具箱"><a href="#智能体工具箱" class="headerlink" title="智能体工具箱"></a>智能体工具箱</h3><p><strong><a href="https://github.com/InternLM/agentlego">InternLM/agentlego: Enhance LLM agents with versatile tool APIs (github.com)</a></strong></p>
<p><img  src="image-20240104195750076.png"  ><span class="image-caption">智能体工具箱</span></p>
<h2 id="从模型到应用"><a href="#从模型到应用" class="headerlink" title="从模型到应用"></a>从模型到应用</h2><p><img  src="image-20240104194844022.png"  ><span class="image-caption">如何从模型到应用</span></p>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>Internlm-02-浦语大模型趣味 Demo</title>
    <url>/internlm/internlm-02/</url>
    <content><![CDATA[<h1 id="浦语大模型趣味-Demo"><a href="#浦语大模型趣味-Demo" class="headerlink" title="浦语大模型趣味 Demo"></a>浦语大模型趣味 Demo</h1><h2 id="大模型及-InternLM-模型简介"><a href="#大模型及-InternLM-模型简介" class="headerlink" title="大模型及 InternLM 模型简介"></a>大模型及 InternLM 模型简介</h2><h3 id="什么是大模型"><a href="#什么是大模型" class="headerlink" title="什么是大模型"></a>什么是大模型</h3><p>大模型是指在机器学习或人工智能领域中具有巨大参数数量和强大计算能力的模型。它们利用海量数据进行训练，拥有数十亿甚至数千亿个参数。大模型的崛起归因于数据量增长、计算能力提升和算法优化等因素。它们在自然语言处理、计算机视觉、语音识别等任务中展现出惊人性能，常采用深度神经网络结构，如Transformer、BERT、GPT等。</p>
<p>这些模型的优势在于能够捕捉和理解数据中更复杂、抽象的特征和关系。通过大规模参数的学习，它们可以提高泛化能力，在未经大量特定领域数据训练的情况下表现优异。然而，它们也面临着挑战，如巨大计算资源需求、高昂训练成本、对大规模数据的依赖和可解释性等问题。因此，在性能、成本和道德等方面需要权衡考量其应用和发展。</p>
<h2 id="InternLM-模型全链条开源"><a href="#InternLM-模型全链条开源" class="headerlink" title="InternLM 模型全链条开源"></a>InternLM 模型全链条开源</h2><p>包括了 InternLM、Lagent、浦语·灵笔等项目，详情可见：<br><a href="https://github.com/InternLM/InternLM">InternLM</a><br><a href="https://enableasync.github.io/internlm/internlm-01/">EnableAsync 的博客</a></p>
<h2 id="InternLM-Chat-7B-智能对话-Demo"><a href="#InternLM-Chat-7B-智能对话-Demo" class="headerlink" title="InternLM-Chat-7B 智能对话 Demo"></a>InternLM-Chat-7B 智能对话 Demo</h2><p>InternLM已经开源了一个70亿参数的基础模型和一个专为实际场景量身定制的聊天模型。该模型具有以下特点：</p>
<ul>
<li>它利用数万亿高质量标记进行训练，建立了强大的知识库。</li>
<li>支持8,000的上下文窗口长度，能够处理更长的输入序列并具备更强的推理能力。</li>
<li>为用户提供了多功能工具集，灵活构建自己的工作流程。</li>
</ul>
<h3 id="demo-代码"><a href="#demo-代码" class="headerlink" title="demo 代码"></a>demo 代码</h3><p>最简单的 <code>cli_demo</code> 代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM<br><br><br>model_name_or_path = <span class="hljs-string">&quot;/root/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span><br><br>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=<span class="hljs-literal">True</span>)<br>model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=<span class="hljs-literal">True</span>, torch_dtype=torch.bfloat16, device_map=<span class="hljs-string">&#x27;auto&#x27;</span>)<br>model = model.<span class="hljs-built_in">eval</span>()<br><br>system_prompt = <span class="hljs-string">&quot;&quot;&quot;You are an AI assistant whose name is InternLM (书生·浦语).</span><br><span class="hljs-string">- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span><br><span class="hljs-string">- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>messages = [(system_prompt, <span class="hljs-string">&#x27;&#x27;</span>)]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;=============Welcome to InternLM chatbot, type &#x27;exit&#x27; to exit.=============&quot;</span>)<br><br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>    input_text = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;User  &gt;&gt;&gt; &quot;</span>)<br>    input_text.replace(<span class="hljs-string">&#x27; &#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    <span class="hljs-keyword">if</span> input_text == <span class="hljs-string">&quot;exit&quot;</span>:<br>        <span class="hljs-keyword">break</span><br>    response, history = model.chat(tokenizer, input_text, history=messages)<br>    messages.append((input_text, response))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;robot &gt;&gt;&gt; <span class="hljs-subst">&#123;response&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></p>
<h3 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h3><p>运行效果如下图所示：</p>
<p><img  src="story.png"  ><span class="image-caption">InternLM生成小故事</span></p>
<h2 id="Lagent-智能体工具调用-Demo"><a href="#Lagent-智能体工具调用-Demo" class="headerlink" title="Lagent 智能体工具调用 Demo"></a>Lagent 智能体工具调用 Demo</h2><p>Lagent 是一个轻量级、开源的基于大语言模型的智能体（agent）框架，支持用户快速地将一个大语言模型转变为多种类型的智能体，并提供了一些典型工具为大语言模型赋能。通过 Lagent 框架可以更好的发挥 InternLM 的全部性能。</p>
<h3 id="demo-代码-1"><a href="#demo-代码-1" class="headerlink" title="demo 代码"></a>demo 代码</h3><p>教程中提供了一个 web demo 如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> copy<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st<br><span class="hljs-keyword">from</span> streamlit.logger <span class="hljs-keyword">import</span> get_logger<br><br><span class="hljs-keyword">from</span> lagent.actions <span class="hljs-keyword">import</span> ActionExecutor, GoogleSearch, PythonInterpreter<br><span class="hljs-keyword">from</span> lagent.agents.react <span class="hljs-keyword">import</span> ReAct<br><span class="hljs-keyword">from</span> lagent.llms <span class="hljs-keyword">import</span> GPTAPI<br><span class="hljs-keyword">from</span> lagent.llms.huggingface <span class="hljs-keyword">import</span> HFTransformerCasualLM<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SessionState</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize session state variables.&quot;&quot;&quot;</span><br>        st.session_state[<span class="hljs-string">&#x27;assistant&#x27;</span>] = []<br>        st.session_state[<span class="hljs-string">&#x27;user&#x27;</span>] = []<br><br>        <span class="hljs-comment">#action_list = [PythonInterpreter(), GoogleSearch()]</span><br>        action_list = [PythonInterpreter()]<br>        st.session_state[<span class="hljs-string">&#x27;plugin_map&#x27;</span>] = &#123;<br>            action.name: action<br>            <span class="hljs-keyword">for</span> action <span class="hljs-keyword">in</span> action_list<br>        &#125;<br>        st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>] = &#123;&#125;<br>        st.session_state[<span class="hljs-string">&#x27;model_selected&#x27;</span>] = <span class="hljs-literal">None</span><br>        st.session_state[<span class="hljs-string">&#x27;plugin_actions&#x27;</span>] = <span class="hljs-built_in">set</span>()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">clear_state</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Clear the existing session state.&quot;&quot;&quot;</span><br>        st.session_state[<span class="hljs-string">&#x27;assistant&#x27;</span>] = []<br>        st.session_state[<span class="hljs-string">&#x27;user&#x27;</span>] = []<br>        st.session_state[<span class="hljs-string">&#x27;model_selected&#x27;</span>] = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;chatbot&#x27;</span> <span class="hljs-keyword">in</span> st.session_state:<br>            st.session_state[<span class="hljs-string">&#x27;chatbot&#x27;</span>]._session_history = []<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">StreamlitUI</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, session_state: SessionState</span>):<br>        self.init_streamlit()<br>        self.session_state = session_state<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_streamlit</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize Streamlit&#x27;s UI settings.&quot;&quot;&quot;</span><br>        st.set_page_config(<br>            layout=<span class="hljs-string">&#x27;wide&#x27;</span>,<br>            page_title=<span class="hljs-string">&#x27;lagent-web&#x27;</span>,<br>            page_icon=<span class="hljs-string">&#x27;./docs/imgs/lagent_icon.png&#x27;</span>)<br>        <span class="hljs-comment"># st.header(&#x27;:robot_face: :blue[Lagent] Web Demo &#x27;, divider=&#x27;rainbow&#x27;)</span><br>        st.sidebar.title(<span class="hljs-string">&#x27;模型控制&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_sidebar</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Setup the sidebar for model and plugin selection.&quot;&quot;&quot;</span><br>        model_name = st.sidebar.selectbox(<br>            <span class="hljs-string">&#x27;模型选择：&#x27;</span>, options=[<span class="hljs-string">&#x27;gpt-3.5-turbo&#x27;</span>,<span class="hljs-string">&#x27;internlm&#x27;</span>])<br>        <span class="hljs-keyword">if</span> model_name != st.session_state[<span class="hljs-string">&#x27;model_selected&#x27;</span>]:<br>            model = self.init_model(model_name)<br>            self.session_state.clear_state()<br>            st.session_state[<span class="hljs-string">&#x27;model_selected&#x27;</span>] = model_name<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;chatbot&#x27;</span> <span class="hljs-keyword">in</span> st.session_state:<br>                <span class="hljs-keyword">del</span> st.session_state[<span class="hljs-string">&#x27;chatbot&#x27;</span>]<br>        <span class="hljs-keyword">else</span>:<br>            model = st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>][model_name]<br><br>        plugin_name = st.sidebar.multiselect(<br>            <span class="hljs-string">&#x27;插件选择&#x27;</span>,<br>            options=<span class="hljs-built_in">list</span>(st.session_state[<span class="hljs-string">&#x27;plugin_map&#x27;</span>].keys()),<br>            default=[<span class="hljs-built_in">list</span>(st.session_state[<span class="hljs-string">&#x27;plugin_map&#x27;</span>].keys())[<span class="hljs-number">0</span>]],<br>        )<br><br>        plugin_action = [<br>            st.session_state[<span class="hljs-string">&#x27;plugin_map&#x27;</span>][name] <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> plugin_name<br>        ]<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;chatbot&#x27;</span> <span class="hljs-keyword">in</span> st.session_state:<br>            st.session_state[<span class="hljs-string">&#x27;chatbot&#x27;</span>]._action_executor = ActionExecutor(<br>                actions=plugin_action)<br>        <span class="hljs-keyword">if</span> st.sidebar.button(<span class="hljs-string">&#x27;清空对话&#x27;</span>, key=<span class="hljs-string">&#x27;clear&#x27;</span>):<br>            self.session_state.clear_state()<br>        uploaded_file = st.sidebar.file_uploader(<br>            <span class="hljs-string">&#x27;上传文件&#x27;</span>, <span class="hljs-built_in">type</span>=[<span class="hljs-string">&#x27;png&#x27;</span>, <span class="hljs-string">&#x27;jpg&#x27;</span>, <span class="hljs-string">&#x27;jpeg&#x27;</span>, <span class="hljs-string">&#x27;mp4&#x27;</span>, <span class="hljs-string">&#x27;mp3&#x27;</span>, <span class="hljs-string">&#x27;wav&#x27;</span>])<br>        <span class="hljs-keyword">return</span> model_name, model, plugin_action, uploaded_file<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_model</span>(<span class="hljs-params">self, option</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the model based on the selected option.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> option <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>]:<br>            <span class="hljs-keyword">if</span> option.startswith(<span class="hljs-string">&#x27;gpt&#x27;</span>):<br>                st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>][option] = GPTAPI(<br>                    model_type=option)<br>            <span class="hljs-keyword">else</span>:<br>                st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>][option] = HFTransformerCasualLM(<br>                    <span class="hljs-string">&#x27;/root/model/Shanghai_AI_Laboratory/internlm-chat-7b&#x27;</span>)<br>        <span class="hljs-keyword">return</span> st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>][option]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_chatbot</span>(<span class="hljs-params">self, model, plugin_action</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the chatbot with the given model and plugin actions.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> ReAct(<br>            llm=model, action_executor=ActionExecutor(actions=plugin_action))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render_user</span>(<span class="hljs-params">self, prompt: <span class="hljs-built_in">str</span></span>):<br>        <span class="hljs-keyword">with</span> st.chat_message(<span class="hljs-string">&#x27;user&#x27;</span>):<br>            st.markdown(prompt)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render_assistant</span>(<span class="hljs-params">self, agent_return</span>):<br>        <span class="hljs-keyword">with</span> st.chat_message(<span class="hljs-string">&#x27;assistant&#x27;</span>):<br>            <span class="hljs-keyword">for</span> action <span class="hljs-keyword">in</span> agent_return.actions:<br>                <span class="hljs-keyword">if</span> (action):<br>                    self.render_action(action)<br>            st.markdown(agent_return.response)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render_action</span>(<span class="hljs-params">self, action</span>):<br>        <span class="hljs-keyword">with</span> st.expander(action.<span class="hljs-built_in">type</span>, expanded=<span class="hljs-literal">True</span>):<br>            st.markdown(<br>                <span class="hljs-string">&quot;&lt;p style=&#x27;text-align: left;display:flex;&#x27;&gt; &lt;span style=&#x27;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#x27;&gt;插    件&lt;/span&gt;&lt;span style=&#x27;width:14px;text-align:left;display:block;&#x27;&gt;:&lt;/span&gt;&lt;span style=&#x27;flex:1;&#x27;&gt;&quot;</span>  <span class="hljs-comment"># noqa E501</span><br>                + action.<span class="hljs-built_in">type</span> + <span class="hljs-string">&#x27;&lt;/span&gt;&lt;/p&gt;&#x27;</span>,<br>                unsafe_allow_html=<span class="hljs-literal">True</span>)<br>            st.markdown(<br>                <span class="hljs-string">&quot;&lt;p style=&#x27;text-align: left;display:flex;&#x27;&gt; &lt;span style=&#x27;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#x27;&gt;思考步骤&lt;/span&gt;&lt;span style=&#x27;width:14px;text-align:left;display:block;&#x27;&gt;:&lt;/span&gt;&lt;span style=&#x27;flex:1;&#x27;&gt;&quot;</span>  <span class="hljs-comment"># noqa E501</span><br>                + action.thought + <span class="hljs-string">&#x27;&lt;/span&gt;&lt;/p&gt;&#x27;</span>,<br>                unsafe_allow_html=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">isinstance</span>(action.args, <span class="hljs-built_in">dict</span>) <span class="hljs-keyword">and</span> <span class="hljs-string">&#x27;text&#x27;</span> <span class="hljs-keyword">in</span> action.args):<br>                st.markdown(<br>                    <span class="hljs-string">&quot;&lt;p style=&#x27;text-align: left;display:flex;&#x27;&gt;&lt;span style=&#x27;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#x27;&gt; 执行内容&lt;/span&gt;&lt;span style=&#x27;width:14px;text-align:left;display:block;&#x27;&gt;:&lt;/span&gt;&lt;/p&gt;&quot;</span>,  <span class="hljs-comment"># noqa E501</span><br>                    unsafe_allow_html=<span class="hljs-literal">True</span>)<br>                st.markdown(action.args[<span class="hljs-string">&#x27;text&#x27;</span>])<br>            self.render_action_results(action)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render_action_results</span>(<span class="hljs-params">self, action</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Render the results of action, including text, images, videos, and</span><br><span class="hljs-string">        audios.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">isinstance</span>(action.result, <span class="hljs-built_in">dict</span>)):<br>            st.markdown(<br>                <span class="hljs-string">&quot;&lt;p style=&#x27;text-align: left;display:flex;&#x27;&gt;&lt;span style=&#x27;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#x27;&gt; 执行结果&lt;/span&gt;&lt;span style=&#x27;width:14px;text-align:left;display:block;&#x27;&gt;:&lt;/span&gt;&lt;/p&gt;&quot;</span>,  <span class="hljs-comment"># noqa E501</span><br>                unsafe_allow_html=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;text&#x27;</span> <span class="hljs-keyword">in</span> action.result:<br>                st.markdown(<br>                    <span class="hljs-string">&quot;&lt;p style=&#x27;text-align: left;&#x27;&gt;&quot;</span> + action.result[<span class="hljs-string">&#x27;text&#x27;</span>] +<br>                    <span class="hljs-string">&#x27;&lt;/p&gt;&#x27;</span>,<br>                    unsafe_allow_html=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;image&#x27;</span> <span class="hljs-keyword">in</span> action.result:<br>                image_path = action.result[<span class="hljs-string">&#x27;image&#x27;</span>]<br>                image_data = <span class="hljs-built_in">open</span>(image_path, <span class="hljs-string">&#x27;rb&#x27;</span>).read()<br>                st.image(image_data, caption=<span class="hljs-string">&#x27;Generated Image&#x27;</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;video&#x27;</span> <span class="hljs-keyword">in</span> action.result:<br>                video_data = action.result[<span class="hljs-string">&#x27;video&#x27;</span>]<br>                video_data = <span class="hljs-built_in">open</span>(video_data, <span class="hljs-string">&#x27;rb&#x27;</span>).read()<br>                st.video(video_data)<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;audio&#x27;</span> <span class="hljs-keyword">in</span> action.result:<br>                audio_data = action.result[<span class="hljs-string">&#x27;audio&#x27;</span>]<br>                audio_data = <span class="hljs-built_in">open</span>(audio_data, <span class="hljs-string">&#x27;rb&#x27;</span>).read()<br>                st.audio(audio_data)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    logger = get_logger(__name__)<br>    <span class="hljs-comment"># Initialize Streamlit UI and setup sidebar</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ui&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:<br>        session_state = SessionState()<br>        session_state.init_state()<br>        st.session_state[<span class="hljs-string">&#x27;ui&#x27;</span>] = StreamlitUI(session_state)<br><br>    <span class="hljs-keyword">else</span>:<br>        st.set_page_config(<br>            layout=<span class="hljs-string">&#x27;wide&#x27;</span>,<br>            page_title=<span class="hljs-string">&#x27;lagent-web&#x27;</span>,<br>            page_icon=<span class="hljs-string">&#x27;./docs/imgs/lagent_icon.png&#x27;</span>)<br>        <span class="hljs-comment"># st.header(&#x27;:robot_face: :blue[Lagent] Web Demo &#x27;, divider=&#x27;rainbow&#x27;)</span><br>    model_name, model, plugin_action, uploaded_file = st.session_state[<br>        <span class="hljs-string">&#x27;ui&#x27;</span>].setup_sidebar()<br><br>    <span class="hljs-comment"># Initialize chatbot if it is not already initialized</span><br>    <span class="hljs-comment"># or if the model has changed</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;chatbot&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state <span class="hljs-keyword">or</span> model != st.session_state[<br>            <span class="hljs-string">&#x27;chatbot&#x27;</span>]._llm:<br>        st.session_state[<span class="hljs-string">&#x27;chatbot&#x27;</span>] = st.session_state[<br>            <span class="hljs-string">&#x27;ui&#x27;</span>].initialize_chatbot(model, plugin_action)<br><br>    <span class="hljs-keyword">for</span> prompt, agent_return <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(st.session_state[<span class="hljs-string">&#x27;user&#x27;</span>],<br>                                    st.session_state[<span class="hljs-string">&#x27;assistant&#x27;</span>]):<br>        st.session_state[<span class="hljs-string">&#x27;ui&#x27;</span>].render_user(prompt)<br>        st.session_state[<span class="hljs-string">&#x27;ui&#x27;</span>].render_assistant(agent_return)<br>    <span class="hljs-comment"># User input form at the bottom (this part will be at the bottom)</span><br>    <span class="hljs-comment"># with st.form(key=&#x27;my_form&#x27;, clear_on_submit=True):</span><br><br>    <span class="hljs-keyword">if</span> user_input := st.chat_input(<span class="hljs-string">&#x27;&#x27;</span>):<br>        st.session_state[<span class="hljs-string">&#x27;ui&#x27;</span>].render_user(user_input)<br>        st.session_state[<span class="hljs-string">&#x27;user&#x27;</span>].append(user_input)<br>        <span class="hljs-comment"># Add file uploader to sidebar</span><br>        <span class="hljs-keyword">if</span> uploaded_file:<br>            file_bytes = uploaded_file.read()<br>            file_type = uploaded_file.<span class="hljs-built_in">type</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;image&#x27;</span> <span class="hljs-keyword">in</span> file_type:<br>                st.image(file_bytes, caption=<span class="hljs-string">&#x27;Uploaded Image&#x27;</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;video&#x27;</span> <span class="hljs-keyword">in</span> file_type:<br>                st.video(file_bytes, caption=<span class="hljs-string">&#x27;Uploaded Video&#x27;</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;audio&#x27;</span> <span class="hljs-keyword">in</span> file_type:<br>                st.audio(file_bytes, caption=<span class="hljs-string">&#x27;Uploaded Audio&#x27;</span>)<br>            <span class="hljs-comment"># Save the file to a temporary location and get the path</span><br>            file_path = os.path.join(root_dir, uploaded_file.name)<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> tmpfile:<br>                tmpfile.write(file_bytes)<br>            st.write(<span class="hljs-string">f&#x27;File saved at: <span class="hljs-subst">&#123;file_path&#125;</span>&#x27;</span>)<br>            user_input = <span class="hljs-string">&#x27;我上传了一个图像，路径为: &#123;file_path&#125;. &#123;user_input&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                file_path=file_path, user_input=user_input)<br>        agent_return = st.session_state[<span class="hljs-string">&#x27;chatbot&#x27;</span>].chat(user_input)<br>        st.session_state[<span class="hljs-string">&#x27;assistant&#x27;</span>].append(copy.deepcopy(agent_return))<br>        logger.info(agent_return.inner_steps)<br>        st.session_state[<span class="hljs-string">&#x27;ui&#x27;</span>].render_assistant(agent_return)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))<br>    root_dir = os.path.join(root_dir, <span class="hljs-string">&#x27;tmp_dir&#x27;</span>)<br>    os.makedirs(root_dir, exist_ok=<span class="hljs-literal">True</span>)<br>    main()<br><br></code></pre></td></tr></table></figure></p>
<h3 id="demo-运行方式"><a href="#demo-运行方式" class="headerlink" title="demo 运行方式"></a>demo 运行方式</h3><p>demo 运行方式如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">streamlit run /root/code/lagent/examples/react_web_demo.py --server.address 127.0.0.1 --server.port 6006<br></code></pre></td></tr></table></figure>
<h3 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h3><p><img  src="lagent-1.png"  ><span class="image-caption">Lagent效果展示1</span></p>
<p><img  src="lagent-2.png"  ><span class="image-caption">Lagent效果展示2</span></p>
<p><img  src="lagent-3.png"  ><span class="image-caption">Lagent效果展示3</span></p>
<p><img  src="lagent-4.png"  ><span class="image-caption">Lagent效果展示4</span></p>
<p>可以看到，lagent能够通过 python 代码解决一些数学问题，而对于很困难的数学问题，解决起来会出现一些问题。</p>
<h2 id="浦语·灵笔图文理解创作-Demo"><a href="#浦语·灵笔图文理解创作-Demo" class="headerlink" title="浦语·灵笔图文理解创作 Demo"></a>浦语·灵笔图文理解创作 Demo</h2><p><strong>浦语·灵笔</strong>是基于<a href="https://github.com/InternLM/InternLM/tree/main">书生·浦语</a>大语言模型研发的视觉-语言大模型，提供出色的图文理解和创作能力，具有多项优势：</p>
<ul>
<li><p><strong>图文交错创作</strong>: 浦语·灵笔可以为用户打造图文并貌的专属文章。生成的文章文采斐然，图文相得益彰，提供沉浸式的阅读体验。这一能力由以下步骤实现：</p>
<ol>
<li><strong>理解用户指令，创作符合要求的长文章</strong>。</li>
<li><strong>智能分析文章，自动规划插图的理想位置，确定图像内容需求。</strong></li>
<li><strong>多层次智能筛选，从图库中锁定最完美的图片。</strong></li>
</ol>
</li>
<li><p><strong>基于丰富多模态知识的图文理解</strong>: 浦语·灵笔设计了高效的训练策略，为模型注入海量的多模态概念和知识数据，赋予其强大的图文理解和对话能力。</p>
</li>
<li><strong>杰出性能</strong>: 浦语·灵笔在多项视觉语言大模型的主流评测上均取得了最佳性能，包括<a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation">MME Benchmark</a> (英文评测), <a href="https://opencompass.org.cn/leaderboard-multimodal">MMBench</a> (英文评测), <a href="https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard">Seed-Bench</a> (英文评测), <a href="https://opencompass.org.cn/leaderboard-multimodal">CCBench</a>(中文评测), <a href="https://opencompass.org.cn/leaderboard-multimodal">MMBench-CN</a> (中文评测)。<h3 id="demo-代码-2"><a href="#demo-代码-2" class="headerlink" title="demo 代码"></a>demo 代码</h3></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/code<br>git <span class="hljs-built_in">clone</span> https://gitee.com/internlm/InternLM-XComposer.git<br><span class="hljs-built_in">cd</span> /root/code/InternLM-XComposer<br>git checkout 3e8c79051a1356b9c388a6447867355c0634932d  <span class="hljs-comment"># 最好保证和教程的 commit 版本一致</span><br></code></pre></td></tr></table></figure>
<h3 id="demo-运行方式-1"><a href="#demo-运行方式-1" class="headerlink" title="demo 运行方式"></a>demo 运行方式</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/code/InternLM-XComposer<br>python examples/web_demo.py  \<br>    --folder /root/model/Shanghai_AI_Laboratory/internlm-xcomposer-7b \<br>    --num_gpus 1 \<br>    --port 6006<br></code></pre></td></tr></table></figure>
<h3 id="效果展示-1"><a href="#效果展示-1" class="headerlink" title="效果展示"></a>效果展示</h3><p><img  src="灵笔-1.png"  ><span class="image-caption">浦语·灵笔效果展示1</span></p>
<p><img  src="灵笔-2.png"  ><span class="image-caption">浦语·灵笔效果展示2</span></p>
<p><img  src="灵笔-3.png"  ><span class="image-caption">浦语·灵笔效果展示3</span></p>
<h2 id="huggingface-hub-下载文件"><a href="#huggingface-hub-下载文件" class="headerlink" title="huggingface_hub 下载文件"></a>huggingface_hub 下载文件</h2><p><img  src="huggingface_hub_download.png"  ><span class="image-caption">使用 huggingface_hub 库下载文件</span></p>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>Kind 的一些使用心得</title>
    <url>/uncategorized/kind/</url>
    <content><![CDATA[<h1 id="Kind-的一些使用心得"><a href="#Kind-的一些使用心得" class="headerlink" title="Kind 的一些使用心得"></a>Kind 的一些使用心得</h1><p>因为 Kind 启动相比于 Minikube 更快，而且支持多 Node，所以现在换成了 Kind，这里记录一些 Kind 的使用心得。</p>
<h2 id="1-Kind-安装"><a href="#1-Kind-安装" class="headerlink" title="1. Kind 安装"></a>1. Kind 安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64<br><span class="hljs-built_in">chmod</span> +x ./kind<br><span class="hljs-built_in">mv</span> ./kind /usr/bin/kind<br></code></pre></td></tr></table></figure>
<h2 id="2-使用-Kind-创建含有两个-Node-的-kubernetes-集群"><a href="#2-使用-Kind-创建含有两个-Node-的-kubernetes-集群" class="headerlink" title="2. 使用 Kind 创建含有两个 Node 的 kubernetes 集群"></a>2. 使用 Kind 创建含有两个 Node 的 kubernetes 集群</h2><h3 id="1-创建配置文件"><a href="#1-创建配置文件" class="headerlink" title="1. 创建配置文件"></a>1. 创建配置文件</h3><p>这里我创建了两个 Node，使用以下配置文件，并将其命名为 <code>kind.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># a cluster with 1 control-plane nodes and 2 workers</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span><br><span class="hljs-attr">nodes:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br></code></pre></td></tr></table></figure>
<h3 id="2-创建集群"><a href="#2-创建集群" class="headerlink" title="2. 创建集群"></a>2. 创建集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">sudo kind create cluster --config kind.yaml<br></code></pre></td></tr></table></figure>
<p>这里需要注意的点有：</p>
<ol>
<li>不要设置集群 name，在我本地，如果设置了 name 会导致 kubeconfig 无法导出。</li>
<li>要使用 sudo，在我本地，如果不使用 sudo 会导致无法创建集群，原因未知。</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">Creating cluster <span class="hljs-string">&quot;kind&quot;</span> ...<br> ✓ Ensuring node image (kindest/node:v1.21.1) 🖼<br> ✓ Preparing nodes 📦 📦 📦  <br> ✓ Writing configuration 📜 <br> ✓ Starting control-plane 🕹️ <br> ✓ Installing CNI 🔌 <br> ✓ Installing StorageClass 💾 <br> ✓ Joining worker nodes 🚜 <br>Set kubectl context to <span class="hljs-string">&quot;kind-kind&quot;</span><br>You can now use your cluster with:<br><br>kubectl cluster-info --context kind-kind<br></code></pre></td></tr></table></figure>
<p>如果出现以上信息表示创建成功，可以进行下一步。</p>
<h3 id="3-导出-kubeconfig"><a href="#3-导出-kubeconfig" class="headerlink" title="3. 导出 kubeconfig"></a>3. 导出 kubeconfig</h3><figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">kind export kubeconfig<br></code></pre></td></tr></table></figure>
<p>如果不使用这一步，会导致使用 <code>kubectl</code> 的时候必须加上 <code>sudo</code>，否则无法连接到 kubernetes。</p>
<h2 id="3-安装-kubernetes-dashboard"><a href="#3-安装-kubernetes-dashboard" class="headerlink" title="3. 安装 kubernetes-dashboard"></a>3. 安装 kubernetes-dashboard</h2><h3 id="1-使用-helm-安装-dashboard"><a href="#1-使用-helm-安装-dashboard" class="headerlink" title="1. 使用 helm 安装 dashboard"></a>1. 使用 helm 安装 dashboard</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Add kubernetes-dashboard repository</span><br>helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/<br>helm repo update<br><span class="hljs-comment"># Deploy a Helm Release named &quot;dashboard&quot; using the kubernetes-dashboard chart</span><br>helm install dashboard kubernetes-dashboard/kubernetes-dashboard<br></code></pre></td></tr></table></figure>
<h3 id="2-转发-dashboard-pod"><a href="#2-转发-dashboard-pod" class="headerlink" title="2. 转发 dashboard pod"></a>2. 转发 dashboard pod</h3><p>这一步的目的是在本地访问部署了 dashboard 的 pod</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> POD_NAME=$(kubectl get pods -n default -l <span class="hljs-string">&quot;app.kubernetes.io/name=kubernetes-dashboard,app.kubernetes.io/instance=dashboard&quot;</span> -o jsonpath=<span class="hljs-string">&quot;&#123;.items[0].metadata.name&#125;&quot;</span>)<br>  <span class="hljs-built_in">echo</span> https://127.0.0.1:8443/<br>  kubectl -n default port-forward <span class="hljs-variable">$POD_NAME</span> 8443:8443<br></code></pre></td></tr></table></figure>
<p>之后会提示</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">Forwarding from 127.0.0.1:8443 -&gt; 8443<br>Forwarding from [::1]:8443 -&gt; 8443<br></code></pre></td></tr></table></figure>
<p>说明转发成功，此时访问 <a href="https://127.0.0.1:8443/">https://127.0.0.1:8443/</a> ，注意是 https</p>
<h3 id="2-1-或者可以不转发使用-service-暴露服务"><a href="#2-1-或者可以不转发使用-service-暴露服务" class="headerlink" title="2.1 或者可以不转发使用 service 暴露服务"></a>2.1 或者可以不转发使用 service 暴露服务</h3><p>这里为了测试使用了 NodePort 方式暴露</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">kubectl expose deploy dashboard-kubernetes-dashboard --name dashboard-nodeport --port 8443 --target-port=8443 --<span class="hljs-built_in">type</span>=NodePort<br></code></pre></td></tr></table></figure>
<h3 id="3-生成-token"><a href="#3-生成-token" class="headerlink" title="3. 生成 token"></a>3. 生成 token</h3><p>不出意外 dashboard 需要 token 来登录，使用以下步骤来生成 token：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">kubectl create serviceaccount dashboard -n default<br>kubectl create rolebinding def-ns-admin --clusterrole=admin --serviceaccount=default:def-ns-admin<br>kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=default:dashboard<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">kubectl describe sa dashboard<br>Name:                dashboard<br>Namespace:           default<br>Labels:              &lt;none&gt;<br>Annotations:         &lt;none&gt;<br>Image pull secrets:  &lt;none&gt;<br>Mountable secrets:   dashboard-token-vzzjn<br>Tokens:              dashboard-token-vzzjn<br>Events:              &lt;none&gt;<br></code></pre></td></tr></table></figure>
<p>这里可以看到 <code>dashboard-token-vzzjn</code> 就是我们需要的 token，使用以下命令显示具体内容：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">kubectl describe secret dashboard-token-vzzjn<br></code></pre></td></tr></table></figure>
<p>之后就可以将具体的 token 粘贴在 dashboard 中登录。</p>
]]></content>
      <tags>
        <tag>k8s, kind</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 的一些使用心得</title>
    <url>/linux/linux/</url>
    <content><![CDATA[<h1 id="conda-init-fish-之后-fish-崩溃"><a href="#conda-init-fish-之后-fish-崩溃" class="headerlink" title="conda init fish 之后 fish 崩溃"></a>conda init fish 之后 fish 崩溃</h1><p>原因是 ubuntu 默认的 fish 是 2.x 版本，而 conda init fish 对应的脚本对应 fish 3.x 版本，所以在安装 fish 的时候需要安装 fish 3 版本。<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">sudo apt-add-repository ppa:fish-shell/release-3<br>sudo apt update<br>sudo apt install fish<br></code></pre></td></tr></table></figure></p>
<h1 id="关闭-kde-文件索引程序"><a href="#关闭-kde-文件索引程序" class="headerlink" title="关闭 kde 文件索引程序"></a>关闭 kde 文件索引程序</h1><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">balooctl <span class="hljs-built_in">suspend</span><br>balooctl <span class="hljs-built_in">disable</span><br></code></pre></td></tr></table></figure>
<h1 id="Linux-下抓-HTTPS-包"><a href="#Linux-下抓-HTTPS-包" class="headerlink" title="Linux 下抓 HTTPS 包"></a>Linux 下抓 HTTPS 包</h1><h2 id="使用-MITMProxy"><a href="#使用-MITMProxy" class="headerlink" title="使用 MITMProxy"></a>使用 MITMProxy</h2><ol>
<li>运行 MITMProxy</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">docker run --<span class="hljs-built_in">rm</span> -it -p 18080:8080 -p 127.0.0.1:8081:8081 -v ~/.mitmproxy:/home/mitmproxy/.mitmproxy  mitmproxy/mitmproxy mitmweb --web-host 0.0.0.0 --<span class="hljs-built_in">set</span> block_global=<span class="hljs-literal">false</span> --<span class="hljs-built_in">set</span> ssl_insecure=<span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure>
<ol>
<li>导入证书至浏览器或其他工具</li>
<li>使用代理访问 HTTPS 页面</li>
</ol>
<h1 id="更新-ubuntu-22-04-之后网易云音乐无法使用"><a href="#更新-ubuntu-22-04-之后网易云音乐无法使用" class="headerlink" title="更新 ubuntu 22.04 之后网易云音乐无法使用"></a>更新 ubuntu 22.04 之后网易云音乐无法使用</h1><p>修改 <code>/opt/netease/netease-cloud-music/netease-cloud-music.bash</code> 为以下内容<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/sh</span><br>HERE=<span class="hljs-string">&quot;<span class="hljs-subst">$(dirname <span class="hljs-string">&quot;<span class="hljs-subst">$(readlink -f <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;0&#125;</span>&quot;</span>)</span>&quot;</span>)</span>&quot;</span><br><span class="hljs-built_in">export</span> LD_LIBRARY_PATH=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;HERE&#125;</span>&quot;</span>/libs<br><span class="hljs-built_in">export</span> QT_PLUGIN_PATH=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;HERE&#125;</span>&quot;</span>/plugins <br><span class="hljs-built_in">export</span> QT_QPA_PLATFORM_PLUGIN_PATH=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;HERE&#125;</span>&quot;</span>/plugins/platforms<br><span class="hljs-built_in">cd</span> /lib/x86_64-linux-gnu/<br><span class="hljs-built_in">exec</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;HERE&#125;</span>&quot;</span>/netease-cloud-music <span class="hljs-variable">$@</span><br><br></code></pre></td></tr></table></figure></p>
<h1 id="关闭无用启动项"><a href="#关闭无用启动项" class="headerlink" title="关闭无用启动项"></a>关闭无用启动项</h1><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看启动项</span><br><span class="hljs-built_in">ls</span> -l /etc/xdg/autostart<br><br><span class="hljs-comment"># 重命名</span><br>sudo <span class="hljs-built_in">mv</span> something something.bak<br></code></pre></td></tr></table></figure>
<h1 id="Vmware-更新内核失败"><a href="#Vmware-更新内核失败" class="headerlink" title="Vmware 更新内核失败"></a>Vmware 更新内核失败</h1><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/mkubecek/vmware-host-modules.git<br>git checkout &lt;your_version&gt;<br>sudo make<br>sudo make install<br></code></pre></td></tr></table></figure>
<h1 id="双系统-Windows-更新失败"><a href="#双系统-Windows-更新失败" class="headerlink" title="双系统 Windows 更新失败"></a>双系统 Windows 更新失败</h1><p>我这里双系统 Windows 更新失败的原因是 Windows 引导出现了问题，可以进入 Windows 输入 <code>msconfig</code> 查看引导选项卡下是否有内容，我是用过 systemd boot 来引导的 Windows，所以没有出现内容。</p>
<p>在 BIOS 中更改成直接引导 Windows 之后便可以正常更新了。</p>
<h1 id="按时间降序最近安装的程序"><a href="#按时间降序最近安装的程序" class="headerlink" title="按时间降序最近安装的程序"></a>按时间降序最近安装的程序</h1><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> $(<span class="hljs-built_in">ls</span> -1t /var/log/dpkg.log*); <span class="hljs-keyword">do</span> <br>      zcat -f <span class="hljs-variable">$x</span> |<span class="hljs-built_in">tac</span> |grep -e <span class="hljs-string">&quot; install &quot;</span> -e <span class="hljs-string">&quot; upgrade &quot;</span>; <br><span class="hljs-keyword">done</span> | awk -F <span class="hljs-string">&quot;:a&quot;</span> <span class="hljs-string">&#x27;&#123;print $1 &quot; :a&quot; $2&#125;&#x27;</span> |column -t<br></code></pre></td></tr></table></figure>
<h1 id="常用的一些-gnome-extensions"><a href="#常用的一些-gnome-extensions" class="headerlink" title="常用的一些 gnome extensions"></a>常用的一些 gnome extensions</h1><h2 id="Unite"><a href="#Unite" class="headerlink" title="Unite"></a>Unite</h2><p>最大化时隐藏标题栏</p>
<h2 id="Clear-Top-Bar"><a href="#Clear-Top-Bar" class="headerlink" title="Clear Top Bar"></a>Clear Top Bar</h2><p>状态栏变成透明的</p>
<h2 id="ddterm"><a href="#ddterm" class="headerlink" title="ddterm"></a>ddterm</h2><p>按 <code>F10</code> 快速启动命令行，再按 <code>F10</code> 隐藏，十分方便</p>
<h2 id="Desktop-Icons-NG-DING"><a href="#Desktop-Icons-NG-DING" class="headerlink" title="Desktop Icons NG(DING)"></a>Desktop Icons NG(DING)</h2><p>在桌面上显示图标</p>
<h2 id="Lock-Keys"><a href="#Lock-Keys" class="headerlink" title="Lock Keys"></a>Lock Keys</h2><p>可以显示当前大小写状况</p>
<h2 id="NetSpeed"><a href="#NetSpeed" class="headerlink" title="NetSpeed"></a>NetSpeed</h2><p>显示当前网速</p>
<h2 id="TopIcons-Plus（在-gnome-40-之后使用-Ubuntu-Appindicators-替代）"><a href="#TopIcons-Plus（在-gnome-40-之后使用-Ubuntu-Appindicators-替代）" class="headerlink" title="TopIcons Plus（在 gnome 40 之后使用 Ubuntu Appindicators 替代）"></a>TopIcons Plus（在 gnome 40 之后使用 Ubuntu Appindicators 替代）</h2><p>在顶部显示图标</p>
<h2 id="Dash-to-Dock"><a href="#Dash-to-Dock" class="headerlink" title="Dash to Dock"></a>Dash to Dock</h2><p>在底部智能显示一个 Dock</p>
<h1 id="换-MAC-地址"><a href="#换-MAC-地址" class="headerlink" title="换 MAC 地址"></a>换 MAC 地址</h1><p>有的时候需要更换 linux 的 ip 地址：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">sudo ifconfig eth0 down<br>sudo ifconfig wlo1 hw ether 02:42:41:7d:b7:6e<br>sudo ifconfig wlo1 up<br></code></pre></td></tr></table></figure>
<p>这里 <code>eth0</code> 是网络 interface，ether 之后的参数就是 MAC 地址</p>
<h1 id="输入法"><a href="#输入法" class="headerlink" title="输入法"></a>输入法</h1><h2 id="Fcitx-失效"><a href="#Fcitx-失效" class="headerlink" title="Fcitx 失效"></a>Fcitx 失效</h2><ul>
<li><p>使用 im-config 修复</p>
</li>
<li><p>可能是 fcitx 没有正常启动，即还是 ibus，可以修改 ~/.pam_environment</p>
</li>
<li><p>使用 <code>fcitx5-diagnose</code> 命令根据提示设置环境变量</p>
</li>
<li><p>删除 <code>/etc/profile.d/pop-im-ibus.sh</code> （pop os）</p>
<p> <code>/etc/profile.d/pop-im-ibus.sh</code> （源文件： /etc/gdm3/Xsession ）设置了环境变量 <code>XMODIFIERS</code> ，在 <code>/etc/X11/Xsession.d/70im-config_launch</code> 中有如下代码：</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">if</span> [ -z <span class="hljs-string">&quot;<span class="hljs-variable">$XMODIFIERS</span>&quot;</span> ] &amp;&amp; \  <span class="hljs-comment"># 如果环境变量 XMODIFIERS 没有被设置</span><br>   ...<br>   <span class="hljs-comment"># 设置环境变量以启动用户指定的输入法</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure>
<p> 因为 <code>XMODIFIERS</code> 被设置了，所以 <code>设置环境变量以启动用户指定的输入法</code> 没有执行，所以 fcitx 没有被启动。</p>
<p> <code>/etc/profile.d/pop-im-ibus.sh</code> 第一次出现于 <code>pop-os_20.10_amd64_intel_4.iso</code> （发布于 2020 年 12 月中旬）</p>
<p> 相关 issue，<a href="https://github.com/pop-os/pop/issues/1445">https://github.com/pop-os/pop/issues/1445</a></p>
</li>
</ul>
<h1 id="Dash-to-dock"><a href="#Dash-to-dock" class="headerlink" title="Dash to dock"></a>Dash to dock</h1><h2 id="Dash-to-dock-重叠问题"><a href="#Dash-to-dock-重叠问题" class="headerlink" title="Dash to dock 重叠问题"></a>Dash to dock 重叠问题</h2><p>   Pop os 自带的 Dock 与 Dash to dock 发生了重叠</p>
   <figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">cd /usr/share/gnome-shell/extensions<br>sudo mv cosmic-dock@system76.com cosmic-dock@system76.com.bak # 关闭自带的 dock<br></code></pre></td></tr></table></figure>
<p>   之后重启 gnome 即可解决</p>
<h1 id="Alt-Tab-时阻止相同应用叠加"><a href="#Alt-Tab-时阻止相同应用叠加" class="headerlink" title="Alt + Tab 时阻止相同应用叠加"></a>Alt + Tab 时阻止相同应用叠加</h1><p>在 gnome 设置中，打开 keyboard shortcut，将 <code>Switch windows</code> 设置为 <code>Alt + Tab</code> ，而不是默认的 <code>Switch applications</code>。</p>
<p>参考：<a href="https://superuser.com/questions/394376/how-to-prevent-gnome-shells-alttab-from-grouping-windows-from-similar-apps">https://superuser.com/questions/394376/how-to-prevent-gnome-shells-alttab-from-grouping-windows-from-similar-apps</a></p>
<h1 id="fluxion"><a href="#fluxion" class="headerlink" title="fluxion"></a>fluxion</h1><h2 id="扫描不到热点"><a href="#扫描不到热点" class="headerlink" title="扫描不到热点"></a>扫描不到热点</h2><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">sudo airmon-ng<br>sudo airmon-ng start fluxwl0<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> FLUXIONAirmonNG=1<br></code></pre></td></tr></table></figure>
<p>执行上述命令后再运行 fluxion 即可。</p>
<h2 id="解除-53-端口被-systemd-resolved-占用"><a href="#解除-53-端口被-systemd-resolved-占用" class="headerlink" title="解除 53 端口被 systemd-resolved 占用"></a>解除 53 端口被 systemd-resolved 占用</h2><ol>
<li>先停用 systemd-resolved 服务</li>
</ol>
<figure class="highlight nsis"><table><tr><td class="code"><pre><code class="hljs nsis"><span class="hljs-params">system</span>ctl stop <span class="hljs-params">system</span>d-resolved<br></code></pre></td></tr></table></figure>
<ol>
<li>编辑 /etc/systemd/resolved.conf 文件</li>
</ol>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">vi <span class="hljs-regexp">/etc/</span>systemd/resolved.conf<br></code></pre></td></tr></table></figure>
<ol>
<li>换下面说明更改，然后按一下“esc”键，再输入“:wq”（不要输入引号），回车保存即可。</li>
</ol>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[Resolve]</span><br><span class="hljs-attr">DNS</span>=<span class="hljs-number">8.8</span>.<span class="hljs-number">8.8</span>  <span class="hljs-comment">#取消注释，增加dns</span><br><span class="hljs-comment">#FallbackDNS=</span><br><span class="hljs-comment">#Domains=</span><br><span class="hljs-comment">#LLMNR=no</span><br><span class="hljs-comment">#MulticastDNS=no</span><br><span class="hljs-comment">#DNSSEC=no</span><br><span class="hljs-comment">#Cache=yes</span><br><span class="hljs-attr">DNSStubListener</span>=<span class="hljs-literal">no</span>  <span class="hljs-comment">#取消注释，把yes改为no</span><br></code></pre></td></tr></table></figure>
<ol>
<li>最后运行下面命令即可。</li>
</ol>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">ln -sf <span class="hljs-regexp">/run/</span>systemd<span class="hljs-regexp">/resolve/</span>resolv.conf <span class="hljs-regexp">/etc/</span>resolv.conf<br></code></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title>一些有意思的图</title>
    <url>/uncategorized/meme/</url>
    <content><![CDATA[<h1 id="关于是否写脚本自动化"><a href="#关于是否写脚本自动化" class="headerlink" title="关于是否写脚本自动化"></a>关于是否写脚本自动化</h1><p><img  src="v2-7012fc4ee97f0b520094f731152eaac5_b.jpg"  ><span class="image-caption">img</span></p>
]]></content>
  </entry>
  <entry>
    <title>人性的弱点笔记</title>
    <url>/uncategorized/people/</url>
    <content><![CDATA[<h1 id="来自-Async-的-Tips"><a href="#来自-Async-的-Tips" class="headerlink" title="来自 Async 的 Tips"></a>来自 Async 的 Tips</h1><p>注意，在使用这些技巧的时候，不要过度，我们不必要太过迁就和顺从任何人。</p>
<p>人们不会珍惜轻易得到的东西。</p>
<h1 id="一、基本技巧"><a href="#一、基本技巧" class="headerlink" title="一、基本技巧"></a>一、基本技巧</h1><p>1、不要批评（待人接物的第一大忌）</p>
<p>2、自我价值感、对尊重和真诚欣赏的渴望（这种说话方式非常值钱）</p>
<p>3、站在对方的角度思考问题。诉诸对方的内在需要。（让整个世界前来为你助力）</p>
<h1 id="二、如何让大家都喜欢你"><a href="#二、如何让大家都喜欢你" class="headerlink" title="二、如何让大家都喜欢你"></a>二、如何让大家都喜欢你</h1><p>4、真的喜欢别人、注意小节。我们都喜欢那些喜欢我们的人。（这样做，谁会不喜欢你？）</p>
<p>5、一笑值千金。（这能留下美好的第一印象）</p>
<p>6、记住名字。（这会让人一下子喜欢上你）</p>
<p>7、全心关注，是一种微妙的恭维，很少有人，很少有人抵抗得住。倾听，鼓励对方多聊自己。（怎样才叫“会聊天”）</p>
<p>8、聊对方感兴趣的话题。（提高你的影响力）</p>
<p>9、和人们聊他们自己，他们会听几个小时都不烦。使对方感到自己是重要的，要真心实意。（一句话焐热人心）</p>
<h1 id="三、如何有逻辑地说服他人"><a href="#三、如何有逻辑地说服他人" class="headerlink" title="三、如何有逻辑地说服他人"></a>三、如何有逻辑地说服他人</h1><p>10、不要信任自己的第一直觉冲动。在意见不合时，我们的第一自然反应是抗辩。小心！保持冷静，小心你的第一反应。那是你最糟糕的瞬间，而不是最好的时候。试着建立理解的桥梁，不要构建误解的深渊。<br>赢得争论的唯一方法是避免争论。（第一大忌：伤敌一万，自损八千）</p>
<p>11、我们偶尔知道自己会毫无阻力地改变主意，没有什么沉重心情，但如果有人说我们错了，我们就会因为对指责的憎恨而铁下心来。我们在形成信念时很随意，随意得叫人吃惊，但当任何人提出要剥离这些信念时，我们就会充满保卫它们的可怕激情。显然，并非那些想法很重要，而是我们的自尊受到了威胁……<br>尊重对方的观点，永远别说“你不对”。（第二大忌：一句惹怒全世界的话）</p>
<p>12、有勇气承认自己的错误，会产生某种满足感。它不仅能让内疚感和自卫感一扫而光，还常能解决错误所带来的问题。<br>假如你错了，立刻真心诚意地承认。（拆招卸力）</p>
<p>13、先说友善的话，气氛对了之后再说别的。（开口前结局已定）</p>
<p>14、先让对方回答“对”“是的”。（如何引导对方的思路）</p>
<p>15、尽量让对方多说。（贵人语迟）</p>
<p>16、我们喜欢别人问我们希望什么，我们需要什么，我们有什么观点。<br>让对方感觉，那是他自己的主意。（洗脑术：如何让人深信不疑）</p>
<p>17、努力从对方的视角看问题，要诚实。（如何让别人关注你）</p>
<p>18、你现在这样感觉，我没有任何理由怪你，如果我是你，我一定也会是那个感觉。<br>真正体会和感受对方的观点和渴望。（提高你的人格魅力）</p>
<p>19、我们所有人在心底都是理想家，喜欢认为自己拥有高尚的情操。<br>诉诸高尚的情操。（如何让人乐意服你）</p>
<p>20、把你的观点，演成故事。（说不透时，卖个故事）</p>
<p>21、抛出挑战。（最后一招，下封战书）</p>
<h1 id="四、成为一个领袖：如何改变他人而不冒犯人或引起反感、愤怒或怨恨"><a href="#四、成为一个领袖：如何改变他人而不冒犯人或引起反感、愤怒或怨恨" class="headerlink" title="四、成为一个领袖：如何改变他人而不冒犯人或引起反感、愤怒或怨恨"></a>四、成为一个领袖：如何改变他人而不冒犯人或引起反感、愤怒或怨恨</h1><p>22、先要懂得欣赏，不做作地称赞。（如何让大家都尊重你）</p>
<p>23、让人们注意自己的错误时，间接一点儿。（批评但不招人恨）</p>
<p>24、批评对方之前，不妨先说说自己犯过的错。（服人而不得罪人）</p>
<p>25、不要直接下命令，而要利用问句。（世界上没有任何人能指使我）</p>
<p>26、让对方保全颜面。（第一大忌：犯众怒）</p>
<p>27、每个人都喜欢称赞，但表扬必须具体，否则就特别假，成了说好话哄骗人。记住，我们都渴望赞美和认可，会不惜一切去争取。但没有人喜欢虚假，所有人都讨厌奉承。<br>称赞最细小的进步，称赞每一个进步。不要吝啬你的嘉许和赞扬，要真诚。（水涨船高：激励下属成事儿）</p>
<p>28、给人一个无法抗拒、无法辜负的好评。（你的期望永远不会落空）</p>
<p>29、学会鼓励人，鼓励会使改变看起来很容易。（让人乐意照你说的去改）</p>
<p>30、一个高产能的领袖，在改变别人的态度和行为时，应该牢记以下原则：</p>
<ul>
<li>要真诚。做不到的承诺，不要承诺。忘记对自己的好处，关注对方的利益。</li>
<li>精确地知道你想让对方做什么。</li>
<li>要共情。问自己：对方想要的到底是什么？</li>
<li>想想如果按照你说的做，对方会得到什么。</li>
<li>把这么做所能带来的好处，和对方的需要进行匹配。</li>
<li>发出要求时，要更改方式，要传达给对方一个观念：对方本人将是受益者。我们不能这么粗鲁地下令：“约翰，明天有客户来，我需要一个整洁的仓库。所以，去打扫干净，把货架上的货物摆放整齐，把货柜擦亮。”我们可以换一个方式来说同一件事，告诉约翰完成这个任务对他会有什么好处：“约翰，我们有件事得马上做好。现在就做，回头就不用手忙脚乱了。我明天会带几个客户来看我们的设施。我想带他们去看看仓库，但现在乱糟糟的。如果你愿意清理一下，把货架上的货物摆放整齐，把货柜擦一擦，我们就会显得更干练，你就给公司形象贡献了自己的那份力量。”</li>
<li>使人们乐意去做你要他们做的事。（让人乐意照你说的去做）</li>
</ul>
]]></content>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>Rust的一些学习心得</title>
    <url>/rust/rust/</url>
    <content><![CDATA[<h1 id="Rust-标准库-trait"><a href="#Rust-标准库-trait" class="headerlink" title="Rust 标准库 trait"></a>Rust 标准库 trait</h1><p>假设有以下变量：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">let</span> <span class="hljs-variable">t</span> = T::<span class="hljs-title function_ invoke__">new</span>()<br></code></pre></td></tr></table></figure>
<h2 id="impl-From-lt-U-gt-for-T"><a href="#impl-From-lt-U-gt-for-T" class="headerlink" title="impl From&lt;U&gt; for T"></a><code>impl From&lt;U&gt; for T</code></h2><p>如果为 <code>T</code> 实现了 <code>From&lt;U&gt;</code> 则可以通过 <code>T::from(U)</code> 得到 <code>T</code>。</p>
<p>例如 <code>String</code> 实现了 <code>From&lt;&amp;str&gt;</code>，所以 <code>String</code> 可以从 <code>&amp;str</code> 生成。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">let</span> <span class="hljs-variable">string</span> = <span class="hljs-string">&quot;hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><span class="hljs-keyword">let</span> <span class="hljs-variable">other_string</span> = <span class="hljs-type">String</span>::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;hello&quot;</span>);<br><br><span class="hljs-built_in">assert_eq!</span>(string, other_string);<br></code></pre></td></tr></table></figure>
<p><code>impl Into&lt;U&gt; for T</code></p>
<p>如果为 <code>T</code> 实现了 <code>Into&lt;U&gt;</code> 则可以通过 <code>t.into()</code> 消耗自己得到 <code>U</code>。</p>
<p>例如 <code>String</code> 类型实现了 <code>Into&lt;Vec&lt;u8&gt;&gt;</code>。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">is_hello</span>&lt;T: <span class="hljs-built_in">Into</span>&lt;<span class="hljs-type">Vec</span>&lt;<span class="hljs-type">u8</span>&gt;&gt;&gt;(s: T) &#123;<br>   <span class="hljs-keyword">let</span> <span class="hljs-variable">bytes</span> = <span class="hljs-string">b&quot;hello&quot;</span>.<span class="hljs-title function_ invoke__">to_vec</span>();<br>   <span class="hljs-built_in">assert_eq!</span>(bytes, s.<span class="hljs-title function_ invoke__">into</span>());<br>&#125;<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = <span class="hljs-string">&quot;hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><span class="hljs-title function_ invoke__">is_hello</span>(s);<br></code></pre></td></tr></table></figure>
<p>在实际编程中，用来接收多种类型的参数，如 <code>Into&lt;String&gt;</code> 可以同时接收 <code>String</code> 和 <code>&amp;str</code>。</p>
<h2 id="impl-AsRef-lt-U-gt-for-T"><a href="#impl-AsRef-lt-U-gt-for-T" class="headerlink" title="impl AsRef&lt;U&gt; for T"></a><code>impl AsRef&lt;U&gt; for T</code></h2><p>如果为 <code>T</code> 实现了 <code>AsRef&lt;U&gt;</code> 则可以通过 <code>t.as_ref()</code> 得到 <code>&amp;U</code>。</p>
<p>注：</p>
<ol>
<li>与 <code>Into&lt;U&gt;</code> 不同的是，<code>AsRef&lt;U&gt;</code> 只是类型转换，<code>t</code> 对象本身没有被消耗；</li>
<li><code>T: AsRef&lt;U&gt;</code> 中的 <code>T</code>，可以接受 资源拥有者（owned）类型，共享引用（shared referrence）类型 ，可变引用（mutable referrence）类型。</li>
</ol>
<p>例如 <code>String</code> 和 <code>&amp;str</code> 都实现了 <code>AsRef&lt;str&gt;</code>：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">is_hello</span>&lt;T: <span class="hljs-built_in">AsRef</span>&lt;<span class="hljs-type">str</span>&gt;&gt;(s: T) &#123;<br>   <span class="hljs-built_in">assert_eq!</span>(<span class="hljs-string">&quot;hello&quot;</span>, s.<span class="hljs-title function_ invoke__">as_ref</span>());<br>&#125;<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = <span class="hljs-string">&quot;hello&quot;</span>;<br><span class="hljs-title function_ invoke__">is_hello</span>(s);<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = <span class="hljs-string">&quot;hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><span class="hljs-title function_ invoke__">is_hello</span>(s);<br></code></pre></td></tr></table></figure>
<h2 id="impl-AsMut-lt-U-gt-for-T"><a href="#impl-AsMut-lt-U-gt-for-T" class="headerlink" title="impl AsMut&lt;U&gt; for T"></a><code>impl AsMut&lt;U&gt; for T</code></h2><p>如果为 <code>T</code> 实现了 <code>AsRef&lt;U&gt;</code> 则可以通过 <code>t.as_mut()</code> 得到 <code>&amp;mut U</code>。</p>
<h2 id="impl-Borror-lt-U-gt-for-T"><a href="#impl-Borror-lt-U-gt-for-T" class="headerlink" title="impl Borror&lt;U&gt; for T"></a><code>impl Borror&lt;U&gt; for T</code></h2><p>如果 <code>T</code> 实现了 <code>Borrow&lt;U&gt;</code>，那么，<code>t</code> 可执行 <code>.borrow()</code> 操作，即 <code>t.borrow()</code>。操作的结果，我们得到了一个类型为 <code>&amp;U</code> 的新引用。</p>
<p><code>Borrow</code> 可以认为是 <code>AsRef</code> 的严格版本，它对普适引用操作的前后类型之间附加了一些其它限制。</p>
<p><code>Borrow</code> 的前后类型之间要求必须有内部等价性。不具有这个等价性的两个类型之间，不能实现 <code>Borrow</code>。</p>
<p><code>AsRef</code> 更通用，更普遍，覆盖类型更多，是 <code>Borrow</code> 的超集。</p>
<p>举例：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::borrow::Borrow;<br><br><span class="hljs-keyword">fn</span> <span class="hljs-title function_">check</span>&lt;T: Borrow&lt;<span class="hljs-type">str</span>&gt;&gt;(s: T) &#123;<br>    <span class="hljs-built_in">assert_eq!</span>(<span class="hljs-string">&quot;Hello&quot;</span>, s.<span class="hljs-title function_ invoke__">borrow</span>());<br>&#125;<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = <span class="hljs-string">&quot;Hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><br><span class="hljs-title function_ invoke__">check</span>(s);<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = <span class="hljs-string">&quot;Hello&quot;</span>;<br><br><span class="hljs-title function_ invoke__">check</span>(s);<br></code></pre></td></tr></table></figure>
<h2 id="impl-BorrowMut-lt-U-gt-for-T"><a href="#impl-BorrowMut-lt-U-gt-for-T" class="headerlink" title="impl BorrowMut&lt;U&gt; for T"></a><code>impl BorrowMut&lt;U&gt; for T</code></h2><p>如果 <code>T</code> 实现了 <code>BorrowMut&lt;U&gt;</code>，那么，<code>t</code> 可执行 <code>.borrow_mut()</code> 操作，即 <code>t.borrow_mut()</code>。操作的结果我们得到类型为 <code>&amp;mut U</code> 的一个可变（mutable）引用。</p>
<h2 id="impl-ToOwned-for-T"><a href="#impl-ToOwned-for-T" class="headerlink" title="impl ToOwned for T"></a><code>impl ToOwned for T</code></h2><p><code>ToOwned</code> 为 <code>Clone</code> 的普适版本。它提供了 <code>.to_owned()</code> 方法，用于类型转换。</p>
<p>有些实现了 <code>Clone</code> 的类型 <code>T</code> 可以从引用状态实例 <code>&amp;T</code> 通过 <code>.clone()</code> 方法，生成具有所有权的 <code>T</code> 的实例。但是它只能由 <code>&amp;T</code> 生成 <code>T</code>。而对于其它形式的引用，<code>Clone</code> 就无能为力了。</p>
<p>而 <code>ToOwned</code> trait 能够从任意引用类型实例，生成具有所有权的类型实例。</p>
<h2 id="impl-Deref-for-T"><a href="#impl-Deref-for-T" class="headerlink" title="impl Deref for T"></a><code>impl Deref for T</code></h2><p><code>Deref</code> 是 <code>deref</code> 操作符 <code>*</code> 的 trait，比如 <code>*v</code>。</p>
<p>一般理解，<code>*t</code> 操作，是 <code>&amp;t</code> 的反向操作，即试图由资源的引用获取到资源的拷贝（如果资源类型实现了 <code>Copy</code>），或所有权（资源类型没有实现 <code>Copy</code>）。</p>
<p>Rust 中，本操作符行为可以重载。这也是 Rust 操作符的基本特点。本身没有什么特别的。</p>
<h3 id="强制隐式转换（coercion）"><a href="#强制隐式转换（coercion）" class="headerlink" title="强制隐式转换（coercion）"></a>强制隐式转换（coercion）</h3><p><code>Deref</code> 神奇的地方并不在本身 <code>解引</code> 这个意义上，Rust 的设计者在它之上附加了一个特性：<code>强制隐式转换</code>，这才是它神奇之处。</p>
<p>这种隐式转换的规则为：</p>
<p>一个类型为 <code>T</code> 的对象 <code>t</code>，如果 <code>T: Deref&lt;Target=U&gt;</code>，那么，相关 <code>t</code> 的某个智能指针或引用（比如 <code>&amp;foo</code>）在应用的时候会自动转换成 <code>&amp;U</code>。</p>
<p>粗看这条规则，貌似有点类似于 <code>AsRef</code>，而跟 <code>解引</code> 似乎风马牛不相及。实际里面有些玄妙之处。</p>
<p>Rust 编译器会在做 <code>*v</code> 操作的时候，自动先把 <code>v</code> 做引用归一化操作，即转换成内部通用引用的形式 <code>&amp;v</code>，整个表达式就变成 <code>*&amp;v</code>。这里面有两种情况：</p>
<ol>
<li>把其它类型的指针（比如在库中定义的，<code>Box</code>, <code>Rc</code>, <code>Arc</code>, <code>Cow</code> 等），转成内部标准形式 <code>&amp;v</code>；</li>
<li>把多重 <code>&amp;</code> （比如：<code>&amp;&amp;&amp;&amp;&amp;&amp;&amp;v</code>），简化成 <code>&amp;v</code>（通过插入足够数量的 <code>*</code> 进行解引）。</li>
</ol>
<p>所以，它实际上在解引用之前做了一个引用的归一化操作。</p>
<p>为什么要转呢？ 因为编译器设计的能力是，只能够对 <code>&amp;v</code> 这种引用进行解引用。其它形式的它不认识，所以要做引用归一化操作。</p>
<p>使用引用进行过渡也是为了能够防止不必要的拷贝。</p>
<p>下面举一些例子：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">foo</span>(s: &amp;<span class="hljs-type">str</span>) &#123;<br>    <span class="hljs-comment">// borrow a string for a second</span><br>&#125;<br><br><span class="hljs-comment">// String implements Deref&lt;Target=str&gt;</span><br><span class="hljs-keyword">let</span> <span class="hljs-variable">owned</span> = <span class="hljs-string">&quot;Hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><br><span class="hljs-comment">// therefore, this works:</span><br><span class="hljs-title function_ invoke__">foo</span>(&amp;owned);<br></code></pre></td></tr></table></figure>
<p>因为 <code>String</code> 实现了 <code>Deref&lt;Target=str&gt;</code>。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::rc::Rc;<br><br><span class="hljs-keyword">fn</span> <span class="hljs-title function_">foo</span>(s: &amp;<span class="hljs-type">str</span>) &#123;<br>    <span class="hljs-comment">// borrow a string for a second</span><br>&#125;<br><br><span class="hljs-comment">// String implements Deref&lt;Target=str&gt;</span><br><span class="hljs-keyword">let</span> <span class="hljs-variable">owned</span> = <span class="hljs-string">&quot;Hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><span class="hljs-keyword">let</span> <span class="hljs-variable">counted</span> = Rc::<span class="hljs-title function_ invoke__">new</span>(owned);<br><br><span class="hljs-comment">// therefore, this works:</span><br><span class="hljs-title function_ invoke__">foo</span>(&amp;counted);<br></code></pre></td></tr></table></figure>
<p>因为 <code>Rc&lt;T&gt;</code> 实现了 <code>Deref&lt;Target=T&gt;</code>。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">foo</span>(s: &amp;[<span class="hljs-type">i32</span>]) &#123;<br>    <span class="hljs-comment">// borrow a slice for a second</span><br>&#125;<br><br><span class="hljs-comment">// Vec&lt;T&gt; implements Deref&lt;Target=[T]&gt;</span><br><span class="hljs-keyword">let</span> <span class="hljs-variable">owned</span> = <span class="hljs-built_in">vec!</span>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>];<br><br><span class="hljs-title function_ invoke__">foo</span>(&amp;owned);<br></code></pre></td></tr></table></figure>
<p>因为 <code>Vec&lt;T&gt;</code> 实现了 <code>Deref&lt;Target=[T]&gt;</code>。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Foo</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Foo</span> &#123;<br>    <span class="hljs-keyword">fn</span> <span class="hljs-title function_">foo</span>(&amp;<span class="hljs-keyword">self</span>) &#123; <span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;Foo&quot;</span>); &#125;<br>&#125;<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">f</span> = &amp;&amp;Foo;<br><br>f.<span class="hljs-title function_ invoke__">foo</span>();<br>(&amp;f).<span class="hljs-title function_ invoke__">foo</span>();<br>(&amp;&amp;f).<span class="hljs-title function_ invoke__">foo</span>();<br>(&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;f).<span class="hljs-title function_ invoke__">foo</span>();<br></code></pre></td></tr></table></figure>
<p>上面那几种函数的调用，效果是一样的。</p>
<p><code>coercion</code> 的设计，是 Rust 中仅有的类型隐式转换，设计它的目的，是为了简化程序的书写，让代码不至于过于繁琐。把人从无尽的类型细节中解脱出来，让书写 Rust 代码变成一件快乐的事情。</p>
<h2 id="Cow"><a href="#Cow" class="headerlink" title="Cow"></a><code>Cow</code></h2><p><code>Clone-on-write</code>，即写时克隆。本质上是一个智能指针。</p>
<p>它有两个可选值：</p>
<ul>
<li><code>Borrowed</code>，用于包裹对象的引用（通用引用）；</li>
<li><code>Owned</code>，用于包裹对象的所有者；</li>
</ul>
<p><code>Cow</code> 提供</p>
<ol>
<li>对此对象的不可变访问（比如可直接调用此对象原有的不可变方法）；</li>
<li>如果遇到需要修改此对象，或者需要获得此对象的所有权的情况，<code>Cow</code> 提供方法做克隆处理，并避免多次重复克隆。</li>
</ol>
<p><code>Cow</code> 的设计目的是提高性能（减少复制）同时增加灵活性，因为大部分情况下，业务场景都是读多写少。利用 <code>Cow</code>，可以用统一，规范的形式实现，需要写的时候才做一次对象复制。这样就可能会大大减少复制的次数。</p>
<p>它有以下几个要点需要掌握：</p>
<ol>
<li><code>Cow&lt;T&gt;</code> 能直接调用 <code>T</code> 的不可变方法，因为 <code>Cow</code> 这个枚举，实现了 <code>Deref</code>；</li>
<li>在需要写 <code>T</code>的时候，可以使用 <code>.to_mut()</code> 方法得到一个具有所有权的值的可变借用；<ol>
<li>注意，调用 <code>.to_mut()</code> 不一定会产生克隆；</li>
<li>在已经具有所有权的情况下，调用 <code>.to_mut()</code> 有效，但是不会产生新的克隆；</li>
<li>多次调用 <code>.to_mut()</code> 只会产生一次克隆。</li>
</ol>
</li>
<li>在需要写 <code>T</code> 的时候，可以使用 <code>.into_owned()</code> 创建新的拥有所有权的对象，这个过程往往意味着内存拷贝并创建新对象；<ol>
<li>如果之前 <code>Cow</code> 中的值是借用状态，调用此操作将执行克隆；</li>
<li>本方法，参数是<code>self</code>类型，它会“吃掉”原先的那个对象，调用之后原先的对象的生命周期就截止了，在 <code>Cow</code> 上不能调用多次；</li>
</ol>
</li>
</ol>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p><code>.to_mut()</code> 举例</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::borrow::Cow;<br><br><span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">cow</span>: Cow&lt;[_]&gt; = Cow::<span class="hljs-title function_ invoke__">Owned</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]);<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">hello</span> = cow.<span class="hljs-title function_ invoke__">to_mut</span>();<br><br><span class="hljs-built_in">assert_eq!</span>(hello, &amp;[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]);<br></code></pre></td></tr></table></figure>
<p><code>.into_owned()</code> 举例</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::borrow::Cow;<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">cow</span>: Cow&lt;[_]&gt; = Cow::<span class="hljs-title function_ invoke__">Owned</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]);<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">hello</span> = cow.<span class="hljs-title function_ invoke__">into_owned</span>();<br><br><span class="hljs-built_in">assert_eq!</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], hello);<br></code></pre></td></tr></table></figure>
<p>综合举例</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::borrow::Cow;<br><br><span class="hljs-keyword">fn</span> <span class="hljs-title function_">abs_all</span>(input: &amp;<span class="hljs-keyword">mut</span> Cow&lt;[<span class="hljs-type">i32</span>]&gt;) &#123;<br>    <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..input.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">v</span> = input[i];<br>        <span class="hljs-keyword">if</span> v &lt; <span class="hljs-number">0</span> &#123;<br>            <span class="hljs-comment">// clones into a vector the first time (if not already owned)</span><br>            input.<span class="hljs-title function_ invoke__">to_mut</span>()[i] = -v;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h3 id="更多的例子"><a href="#更多的例子" class="headerlink" title="更多的例子"></a>更多的例子</h3><p>题目：写一个函数，过滤掉输入的字符串中的所有空格字符，并返回过滤后的字符串。</p>
<p>对这个简单的问题，不用思考，我们都可以很快写出代码：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">remove_spaces</span>(input: &amp;<span class="hljs-type">str</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">String</span> &#123;<br>   <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">buf</span> = <span class="hljs-type">String</span>::<span class="hljs-title function_ invoke__">with_capacity</span>(input.<span class="hljs-title function_ invoke__">len</span>());<br><br>   <span class="hljs-keyword">for</span> <span class="hljs-variable">c</span> <span class="hljs-keyword">in</span> input.<span class="hljs-title function_ invoke__">chars</span>() &#123;<br>      <span class="hljs-keyword">if</span> c != <span class="hljs-string">&#x27; &#x27;</span> &#123;<br>         buf.<span class="hljs-title function_ invoke__">push</span>(c);<br>      &#125;<br>   &#125;<br><br>   buf<br>&#125;<br></code></pre></td></tr></table></figure>
<p>设计函数输入参数的时候，我们会停顿一下，这里，用 <code>&amp;str</code> 好呢，还是 <code>String</code> 好呢？思考一番，从性能上考虑，有如下结论：</p>
<ol>
<li>如果使用 <code>String</code> 则外部在调用此函数的时候，<ol>
<li>如果外部的字符串是 <code>&amp;str</code>，那么，它需要做一次克隆，才能调用此函数；</li>
<li>如果外部的字符串是 <code>String</code>，那么，它不需要做克隆，就可以调用此函数。但是，一旦调用后，外部那个字符串的所有权就被 <code>move</code> 到此函数中了，外部的后续代码将无法再使用原字符串。</li>
</ol>
</li>
<li>如果使用 <code>&amp;str</code>，则不存在上述两个问题。但可能会遇到生命周期的问题，需要注意。</li>
</ol>
<p>继续分析上面的例子，我们发现，在函数体内，做了一次新字符串对象的生成和拷贝。</p>
<p>让我们来仔细分析一下业务需求。最坏的情况下，如果字符串中没有空白字符，那最好是直接原样返回。这种情况做这样一次对象的拷贝，完全就是浪费了。</p>
<p>于是我们心想改进这个算法。很快，又遇到了另一个问题，返回值是 <code>String</code> 的嘛，我不论怎样，要把 <code>&amp;str</code> 转换成 <code>String</code> 返回，始终都要经历一次复制。于是我们快要放弃了。</p>
<p>好吧，<code>Cow</code> 君这时出马了。写出了如下代码：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::borrow::Cow;<br><br><span class="hljs-keyword">fn</span> <span class="hljs-title function_">remove_spaces</span>&lt;<span class="hljs-symbol">&#x27;a</span>&gt;(input: &amp;<span class="hljs-symbol">&#x27;a</span> <span class="hljs-type">str</span>) <span class="hljs-punctuation">-&gt;</span> Cow&lt;<span class="hljs-symbol">&#x27;a</span>, <span class="hljs-type">str</span>&gt; &#123;<br>    <span class="hljs-keyword">if</span> input.<span class="hljs-title function_ invoke__">contains</span>(<span class="hljs-string">&#x27; &#x27;</span>) &#123;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">buf</span> = <span class="hljs-type">String</span>::<span class="hljs-title function_ invoke__">with_capacity</span>(input.<span class="hljs-title function_ invoke__">len</span>());<br><br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">c</span> <span class="hljs-keyword">in</span> input.<span class="hljs-title function_ invoke__">chars</span>() &#123;<br>            <span class="hljs-keyword">if</span> c != <span class="hljs-string">&#x27; &#x27;</span> &#123;<br>                buf.<span class="hljs-title function_ invoke__">push</span>(c);<br>            &#125;<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> Cow::<span class="hljs-title function_ invoke__">Owned</span>(buf);<br>    &#125;<br><br>    <span class="hljs-keyword">return</span> Cow::<span class="hljs-title function_ invoke__">Borrowed</span>(input);<br>&#125;<br></code></pre></td></tr></table></figure>
<p>完美解决了业务逻辑与返回值类型冲突的问题。本例可细细品味。</p>
<p>外部程序，拿到这个 <code>Cow</code> 返回值后，按照我们上文描述的 <code>Cow</code> 的特性使用就好了。</p>
<h2 id="Send-和-Sync"><a href="#Send-和-Sync" class="headerlink" title="Send 和 Sync"></a><code>Send</code> 和 <code>Sync</code></h2><p><code>std::marker</code> 模块中，有两个 trait：<code>Send</code> 和 <code>Sync</code>，它们与多线程安全相关。</p>
<p>标记为 <code>marker trait</code> 的 trait，它实际就是一种约定，没有方法的定义，也没有关联元素（associated items）。仅仅是一种约定，实现了它的类型必须满足这种约定。一种类型是否加上这种约定，要么是编译器的行为，要么是人工手动的行为。</p>
<p><code>Send</code> 和 <code>Sync</code> 在大部分情况下（针对 Rust 的基础类型和 std 中的大部分类型），会由编译器自动推导出来。对于不能由编译器自动推导出来的类型，要使它们具有 <code>Send</code> 或 <code>Sync</code> 的约定，可以由人手动实现。实现的时候，必须使用 <code>unsafe</code> 前缀，因为 Rust 默认不信任程序员，由程序员自己控制的东西，统统标记为 <code>unsafe</code>，出了问题（比如，把不是线程安全的对象加上 <code>Sync</code> 约定）由程序员自行负责。</p>
<p>它们的定义如下：</p>
<p>如果 <code>T: Send</code>，那么将 <code>T</code> 传到另一个线程中时（按值传送），不会导致数据竞争或其它不安全情况。</p>
<ol>
<li><code>Send</code> 是对象可以安全发送到另一个执行体中；</li>
<li><code>Send</code> 使被发送对象可以和产生它的线程解耦，防止原线程将此资源释放后，在目标线程中使用出错（use after free）。</li>
</ol>
<p>如果 <code>T: Sync</code>，那么将 <code>&amp;T</code> 传到另一个线程中时，不会导致数据竞争或其它不安全情况。</p>
<ol>
<li><code>Sync</code> 是可以被同时多个执行体访问而不出错；</li>
<li><code>Sync</code> 防止的是竞争；</li>
</ol>
<p>推论：</p>
<ol>
<li><code>T: Sync</code> 意味着 <code>&amp;T: Send</code>；</li>
<li><code>Sync + Copy = Send</code>；</li>
<li>当 <code>T: Send</code> 时，可推导出 <code>&amp;mut T: Send</code>；</li>
<li>当 <code>T: Sync</code> 时，可推导出 <code>&amp;mut T: Sync</code>；</li>
<li>当 <code>&amp;mut T: Send</code> 时，不能推导出 <code>T: Send</code>；</li>
</ol>
<p>（注：<code>T</code>, <code>&amp;T</code>, <code>&amp;mut T</code>，<code>Box&lt;T&gt;</code> 等都是不同的类型）</p>
<p>具体的类型：</p>
<ol>
<li>原始类型（比如： u8, f64），都是 <code>Sync</code>，都是 <code>Copy</code>，因此都是 <code>Send</code>；</li>
<li>只包含原始类型的复合类型，都是 <code>Sync</code>，都是 <code>Copy</code>，因此都是 <code>Send</code>；</li>
<li>当 <code>T: Sync</code>，<code>Box&lt;T&gt;</code>, <code>Vec&lt;T&gt;</code> 等集合类型是 <code>Sync</code>；</li>
<li>具有内部可变性的的指针，不是 <code>Sync</code> 的，比如 <code>Cell</code>, <code>RefCell</code>, <code>UnsafeCell</code>；</li>
<li><code>Rc</code> 不是 <code>Sync</code>。因为只要一做 <code>&amp;Rc&lt;T&gt;</code> 操作，就会克隆一个新引用，它会以非原子性的方式修改引用计数，所以是不安全的；</li>
<li>被 <code>Mutex</code> 和 <code>RWLock</code> 锁住的类型 <code>T: Send</code>，是 <code>Sync</code> 的；</li>
<li>原始指针（<code>*mut</code>, <code>*const</code>）既不是 <code>Send</code> 也不是 <code>Sync</code>；</li>
</ol>
<p>Rust 正是通过这两大武器：<code>所有权和生命周期</code> + <code>Send 和 Sync</code>（本质上为类型系统）来为并发编程提供了安全可靠的基础设施。使得程序员可以放心在其上构建稳健的并发模型。这也正是 Rust 的核心设计观的体现：内核只提供最基础的原语，真正的实现能分离出去就分离出去。并发也是如此。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li><a href="https://github.com/rustcc/RustPrimer">https://github.com/rustcc/RustPrimer</a></li>
</ol>
]]></content>
      <categories>
        <category>rust</category>
      </categories>
      <tags>
        <tag>rust</tag>
      </tags>
  </entry>
  <entry>
    <title>反社会的人笔记</title>
    <url>/uncategorized/%E5%8F%8D%E7%A4%BE%E4%BC%9A%E7%9A%84%E4%BA%BA/</url>
    <content><![CDATA[<h1 id="反社会的人"><a href="#反社会的人" class="headerlink" title="反社会的人"></a>反社会的人</h1><p>上层阶级与下层阶级是如何搞垮德国，而谁又在从中获利。</p>
<p>作者瓦尔特·伍伦韦伯在成为记者之前曾就读于海德堡大学政治和法律专业。《反社会的人》是其于2012年出版的首部著作。一经面世就成为当年德国最畅销的读物之一。作者在书中所呈现的德国社会不公正性虽然不为中国读者所熟悉，但这些现象在德国国内早已是热门话题。与大量将焦点放在贫富差距上的社会讨论不同的是，本书的作者首先定义了德国社会中业已形成的两个新兴阶级，即上层阶级和下层阶级。通过对这两个社会群体发展过程的详细描述，作者提出了自己看似惊人的观点。他认为新兴上层阶级和下层阶级的存在使德国社会面临着分崩离析的局面。这两个看似对立的阶级实质上具有近乎一致的发展轨迹和表现特征。它们不但在德国社会运作过程中极力地逃避着作为公民的义务和责任，而且其生存形态给德国的普通纳税人造成了极大负担。</p>
<p>瓦尔特·伍伦韦伯进而在本书的第二部分中将讨论的重点转向了促成这两个阶级形成的原因，也就是德国经济中两个最庞大的产业：金融和社会救助产业。借助大量的统计数据和对相关人员的采访，作者揭示了这两个产业依附于上层和下层阶级得到快速发展的事实。金融行业通过帮助上层阶级进行巨额财富的投机活动，不但使德国经济陷入巨大的危机，还让上层阶级这种纯粹依靠资本运作获益的生活方式成为可能。而社会救助产业则通过利用德国社会福利制度的漏洞，将大量的政府公共支出占为己有，从而发展成为拥有200万从业人员的巨型产业。本书同时尖锐地指出了德国历届联邦政府在这两个产业的畸形发展过程中所起到的推波助澜的作用。政府逐步解除对这两个行业的监管导致其在追求利益最大化的道路上越走越远。</p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>哲学家路德维希·维特根斯坦曾说过：“对于一无所知的事情，人们应当保持沉默。”正因如此，对于同时在德国社会结构的两极发生着的巨大变化，至今无人展开讨论。对探讨对象的一无所知往往导致这类讨论在尚未开始时就注定夭折。作为社会凝聚力瓦解的前兆，下层乃至上层阶级纷纷退缩到各自的平行世界。而人们却一直在回避这一威胁。其中一个最重要的原因就是：我们对于德国社会结构中的这两类人群知之甚少。</p>
<p>当然无知并不是唯一的障眼法，阻碍人们认识这个新兴阶级社会的还有习惯性思维。东部和西部的德国人早已不习惯以阶级来划分他们所处的社会。统一之前的东德人活在“统一的社会主义社会”的假象中。而西德人则相信当时的联邦德国正朝着“均质化的中产阶级社会”发展。“不管怎样大家都是中产阶级”这一观念在如今西德人的意识里依然根深蒂固。</p>
<p>对于其他主要欧美国家的人民来说，认识到社会阶级的存在是件理所当然的事情。而如今的德国人在这方面却显得缺乏经验。对于中产阶级社会狭隘的固着遮挡了他们的视线，使其难以察觉到德国社会正在经历的转变，因为这种转变首先就发生在德国下层和上层阶级中。</p>
<p>上层阶级是否仍然像大多数人认为的那样担当着社会生产力的骨干角色？<strong>事实是那些过去的企业奠基人及开拓者早已完成转型，变成了纯粹的投资者。</strong>他们现在更喜欢在高风险的金融投资领域用资本获取更多的资本。德国的财富精英们也绝非像他们宣称的那样被税收压得喘不过气来。恰恰相反，他们厚实的肩膀上仅仅承担着微不足道的税赋。维持国家日常运作的经济重任则转由辛勤的中产阶级纳税人来承担。在过去十年中，上亿欧元的财富被转移到了金字塔的顶端。但为何如此巨大的财富转移时至今日仍不为社会所察觉？这应当归咎于上层阶级在自身习性上的一个重大转变。受世人瞩目自古以来就是一种特权。不论是剧院里高高在上的包厢还是主席台的中间席位，显眼的位子总是属于位高权重的人。然而如今的上层阶级却更愿意生活在外人无从窥探的平行世界里。他们只会在互相之间展示其财富。在外界看来，德国的富人们就像根本不存在一样。</p>
<p>然而最近的金融危机却提醒了世人：少数极端富有者的贪欲已经威胁到了社会中大多数人的生活水准。因此，不论上层阶级如何百般地抵制，其价值观和行为都有必要成为公共探讨的主题。相对于上层阶级，下层阶级倒是活得不藏不掖。早在十年前，我成为了首批对这一新兴社会阶层进行报导的记者之一，并在之后的数年间对这个尚不为人所了解的社会阶层进行了大量的采访研究。我得出的结论是：人们对下层阶级的印象仍停留在那些老生常谈、空穴来风的故事或是个别利益集团别有用心的不实宣传上。人们不会想到，物质的匮乏如今和下层阶级的日常生活根本扯不上关系，游戏机、智能电话、电脑和电视娱乐节目才是他们生活中最重要的组成部分。德国的社会福利制度早已战胜了匮乏的物质生活。德国的穷人可以说有的是钱。尽管如此，下层阶级所遭受的不公正对待却是真实且残酷的，而这种不公正性的根源及广度绝非“贫穷”一词所能概括。<strong>下层阶级被剥夺的是一种宝贵的社会参与性。较之金钱的匮乏，受教育机会的缺失才是下层阶级的主要标志。匮乏的教育机会几乎可以说是一切相关问题的根源，包括失业、健康以及抚养后代时的力不从心。下层阶级和中产阶级之间的区别并非收入水平，文化上的差异才是横跨在两者之间的鸿沟。</strong></p>
<p>绩效观念对于德国社会有着不可替代的意义。它不光是富足生活和优越社会福利的保障，更是德国价值体系的支撑。然而上层阶级与下层阶级却发展出了各自独有的价值观和道德观，并与主流社会的认同渐行渐远，这使得对生产绩效的追求失去往日的意义。因为对于以上两个阶级来说，工作所得并非其典型的经济来源。<strong>上层阶级通过毫不费力的资本投资获取财富，而下层阶级则基本依靠社会救济拨款来维持生活。</strong>有别于中产阶级，生产绩效对这两个生活在平行社会中的阶级来说丝毫不具备同等的意义。</p>
<p>社会学家注意到，这两个社会阶级的成员几乎已无法意识到付出劳动与获得成功之间的因果关系。上层阶级视成功为理所当然的结果。下层阶级则缺乏通过付出努力而获取成功的经历。这就使得各种缺乏诚信的手段在两个阶层获得了极大的生存空间。蓬勃发展的税收规避行业正在帮助上层阶级最大限度地逃避纳税。而下层阶级也在社会福利法规制度的丛林中发挥着自身的创造性，尽其所能地不劳而获。可以说，<strong>善于寻找和利用法律漏洞正日渐成为这两个社会阶层所共有的特性</strong>。而德国政府却在经济上资助了这种社会离心运动的发展。中产阶级基本上是在独自维持社会的正常运作，其缴纳的高额税收既化解了财富阶层金融投机行为所带来的风险，同时又保障了下层阶级坐享各种社会福利。他们既为有钱人的财富提供了保护伞，又承担着人人受益的社会福利体系。可以说中产阶级在同时供养着上层和下层阶级。</p>
]]></content>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>Internlm-03-基于 InternLM 和 LangChain 搭建你的知识库</title>
    <url>/internlm/internlm-03/</url>
    <content><![CDATA[<h1 id="基于-InternLM-和-LangChain-搭建你的知识库"><a href="#基于-InternLM-和-LangChain-搭建你的知识库" class="headerlink" title="基于 InternLM 和 LangChain 搭建你的知识库"></a>基于 InternLM 和 LangChain 搭建你的知识库</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h3 id="InternLM-环境"><a href="#InternLM-环境" class="headerlink" title="InternLM 环境"></a>InternLM 环境</h3><p>开发环境除了 <code>pytorch</code> 等库以外，还需要安装以下库</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 升级pip</span><br>python -m pip install --upgrade pip<br><br>pip install modelscope==1.9.5<br>pip install transformers==4.35.2<br>pip install streamlit==1.24.0<br>pip install sentencepiece==0.1.99<br>pip install accelerate==0.24.1<br></code></pre></td></tr></table></figure>
<h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">import torch<br>from modelscope import snapshot_download, AutoModel, AutoTokenizer<br>import os<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;Shanghai_AI_Laboratory/internlm-chat-7b&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;/root/data/model&#x27;</span>, revision=<span class="hljs-string">&#x27;v1.0.3&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="配置-Langchain"><a href="#配置-Langchain" class="headerlink" title="配置 Langchain"></a>配置 Langchain</h3><p>除了配置大模型的运行环境以外，还需要配置 Langchain 运行环境。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install langchain==0.0.292<br>pip install gradio==4.4.0<br>pip install chromadb==0.4.15<br>pip install sentence-transformers==2.2.2<br>pip install unstructured==0.10.30<br>pip install markdown==3.3.7<br></code></pre></td></tr></table></figure>
<p><img  src="安装依赖.png"  ><span class="image-caption">安装依赖</span></p>
<h3 id="下载-Embedding-模型"><a href="#下载-Embedding-模型" class="headerlink" title="下载 Embedding 模型"></a>下载 Embedding 模型</h3><p>同时，我们需要使用到开源词向量模型 <a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2">Sentence Transformer - paraphrase-multilingual-MiniLM-L12-v2</a>:（我们也可以选用别的开源词向量模型来进行 Embedding，教程中选用这个模型是相对轻量、支持中文且效果较好的，我这里选择使用了更为好用的 bge 系列的 Embedding 模型 <a href="[BAAI/bge-large-zh-v1.5 · Hugging Face](https://huggingface.co/BAAI/bge-large-zh-v1.5">BAAI/bge-large-zh-v1.5</a>)）</p>
<p>首先需要使用 <code>huggingface</code> 官方提供的 <code>huggingface-cli</code> 命令行工具。安装依赖:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install -U huggingface_hub<br></code></pre></td></tr></table></figure>
<p>然后在和 <code>/root/data</code> 目录下新建python文件 <code>download_hf.py</code>，填入以下代码：</p>
<ul>
<li>resume-download：断点续下</li>
<li>local-dir：本地存储路径。（linux环境下需要填写绝对路径）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 下载模型</span><br>os.system(<span class="hljs-string">&#x27;huggingface-cli download --resume-download BAAI/bge-large-zh-v1.5 --local-dir /root/data/model/bge-large-zh-v1.5&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>但是，使用 huggingface 下载可能速度较慢，我们可以使用 huggingface 镜像下载。与使用hugginge face下载相同，只需要填入镜像地址即可。</p>
<p>将 <code>download_hf.py</code> 中的代码修改为以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 设置环境变量</span><br>os.environ[<span class="hljs-string">&#x27;HF_ENDPOINT&#x27;</span>] = <span class="hljs-string">&#x27;https://hf-mirror.com&#x27;</span><br><br><span class="hljs-comment"># 下载模型</span><br>os.system(<span class="hljs-string">&#x27;huggingface-cli download --resume-download BAAI/bge-large-zh-v1.5 --local-dir /root/data/model/bge-large-zh-v1.5&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>然后，在 <code>/root/data</code> 目录下执行该脚本即可自动开始下载：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python download_hf.py<br></code></pre></td></tr></table></figure>
<p><img  src="下载bge.png"  ><span class="image-caption">下载bge模型</span></p>
<h3 id="下载-NLTK-相关资源"><a href="#下载-NLTK-相关资源" class="headerlink" title="下载 NLTK 相关资源"></a>下载 NLTK 相关资源</h3><p>我们在使用开源词向量模型构建开源词向量的时候，需要用到第三方库 <code>nltk</code> 的一些资源。正常情况下，其会自动从互联网上下载，但可能由于网络原因会导致下载中断，此处我们可以从国内仓库镜像地址下载相关资源，保存到服务器上。</p>
<p>我们用以下命令下载 nltk 资源并解压到服务器上：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root<br>git <span class="hljs-built_in">clone</span> https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages<br><span class="hljs-built_in">cd</span> nltk_data<br><span class="hljs-built_in">mv</span> packages/*  ./<br><span class="hljs-built_in">cd</span> tokenizers<br>unzip punkt.zip<br><span class="hljs-built_in">cd</span> ../taggers<br>unzip averaged_perceptron_tagger.zip<br></code></pre></td></tr></table></figure>
<p>之后使用时服务器即会自动使用已有资源，无需再次下载。</p>
<h3 id="下载教程代码"><a href="#下载教程代码" class="headerlink" title="下载教程代码"></a>下载教程代码</h3><p>我们在仓库中同步提供了所有脚本，可以查看该教程文件的同级目录的 <code>demo</code> 文件夹。</p>
<p>建议通过以下目录将仓库 clone 到本地，可以直接在本地运行相关代码：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/data<br>git <span class="hljs-built_in">clone</span> https://github.com/InternLM/tutorial<br></code></pre></td></tr></table></figure>
<p>通过上述命令，可以将本仓库 clone 到本地 <code>root/data/tutorial</code> 目录下，在之后的过程中可以对照仓库中的脚本来完成自己的代码，也可以直接使用仓库中的脚本。</p>
<h2 id="知识库搭建"><a href="#知识库搭建" class="headerlink" title="知识库搭建"></a>知识库搭建</h2><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><p>教程选择了由上海人工智能实验室开源的一系列大模型工具开源仓库作为语料库来源，包括：</p>
<ul>
<li><a href="https://gitee.com/open-compass/opencompass">OpenCompass</a>：面向大模型评测的一站式平台</li>
<li><a href="https://gitee.com/InternLM/lmdeploy">IMDeploy</a>：涵盖了 LLM 任务的全套轻量化、部署和服务解决方案的高效推理工具箱</li>
<li><a href="https://gitee.com/InternLM/xtuner">XTuner</a>：轻量级微调大语言模型的工具库</li>
<li><a href="https://gitee.com/InternLM/InternLM-XComposer">InternLM-XComposer</a>：浦语·灵笔，基于书生·浦语大语言模型研发的视觉-语言大模型</li>
<li><a href="https://gitee.com/InternLM/lagent">Lagent</a>：一个轻量级、开源的基于大语言模型的智能体（agent）框架</li>
<li><a href="https://gitee.com/InternLM/InternLM">InternLM</a>：一个开源的轻量级训练框架，旨在支持大模型训练而无需大量的依赖</li>
</ul>
<p>首先我们需要将上述远程开源仓库 Clone 到本地，可以使用以下命令：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># 进入到数据库盘</span><br>cd <span class="hljs-regexp">/root/</span>data<br><span class="hljs-comment"># clone 上述开源仓库</span><br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/open-compass/</span>opencompass.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>lmdeploy.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>xtuner.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>InternLM-XComposer.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>lagent.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>InternLM.git<br></code></pre></td></tr></table></figure>
<p>接着，为语料处理方便，我们将选用上述仓库中所有的 markdown、txt 文件作为示例语料库。注意，也可以选用其中的代码文件加入到知识库中，但需要针对代码文件格式进行额外处理（因为代码文件对逻辑联系要求较高，且规范性较强，在分割时最好基于代码模块进行分割再加入向量数据库）。</p>
<p>我们首先将上述仓库中所有满足条件的文件路径找出来，我们定义一个函数，该函数将递归指定文件夹路径，返回其中所有满足条件（即后缀名为 .md 或者 .txt 的文件）的文件路径：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_files</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    file_list = []<br>    <span class="hljs-keyword">for</span> filepath, dirnames, filenames <span class="hljs-keyword">in</span> os.walk(dir_path):<br>        <span class="hljs-comment"># os.walk 函数将递归遍历指定文件夹</span><br>        <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> filenames:<br>            <span class="hljs-comment"># 通过后缀名判断文件类型是否满足要求</span><br>            <span class="hljs-keyword">if</span> filename.endswith(<span class="hljs-string">&quot;.md&quot;</span>):<br>                <span class="hljs-comment"># 如果满足要求，将其绝对路径加入到结果列表</span><br>                file_list.append(os.path.join(filepath, filename))<br>            <span class="hljs-keyword">elif</span> filename.endswith(<span class="hljs-string">&quot;.txt&quot;</span>):<br>                file_list.append(os.path.join(filepath, filename))<br>    <span class="hljs-keyword">return</span> file_list<br></code></pre></td></tr></table></figure>
<h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>得到所有目标文件路径之后，我们可以使用 LangChain 提供的 FileLoader 对象来加载目标文件，得到由目标文件解析出的纯文本内容。由于不同类型的文件需要对应不同的 FileLoader，我们判断目标文件类型，并针对性调用对应类型的 FileLoader，同时，调用 FileLoader 对象的 load 方法来得到加载之后的纯文本对象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredFileLoader<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredMarkdownLoader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    <span class="hljs-comment"># 首先调用上文定义的函数得到目标文件路径列表</span><br>    file_lst = get_files(dir_path)<br>    <span class="hljs-comment"># docs 存放加载之后的纯文本对象</span><br>    docs = []<br>    <span class="hljs-comment"># 遍历所有目标文件</span><br>    <span class="hljs-keyword">for</span> one_file <span class="hljs-keyword">in</span> tqdm(file_lst):<br>        file_type = one_file.split(<span class="hljs-string">&#x27;.&#x27;</span>)[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> file_type == <span class="hljs-string">&#x27;md&#x27;</span>:<br>            loader = UnstructuredMarkdownLoader(one_file)<br>        <span class="hljs-keyword">elif</span> file_type == <span class="hljs-string">&#x27;txt&#x27;</span>:<br>            loader = UnstructuredFileLoader(one_file)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 如果是不符合条件的文件，直接跳过</span><br>            <span class="hljs-keyword">continue</span><br>        docs.extend(loader.load())<br>    <span class="hljs-keyword">return</span> docs<br></code></pre></td></tr></table></figure>
<p>使用上文函数，我们得到的 <code>docs</code> 为一个纯文本对象对应的列表。</p>
<h3 id="构建向量数据库"><a href="#构建向量数据库" class="headerlink" title="构建向量数据库"></a>构建向量数据库</h3><p>得到该列表之后，我们就可以将它引入到 LangChain 框架中构建向量数据库。由纯文本对象构建向量数据库，我们需要先对文本进行分块，接着对文本块进行向量化。</p>
<p>LangChain 提供了多种文本分块工具，此处我们使用字符串递归分割器，并选择分块大小为 500，块重叠长度为 150（由于篇幅限制，此处没有展示切割效果，学习者可以自行尝试一下，想要深入学习 LangChain 文本分块可以参考教程 <a href="https://github.com/datawhalechina/prompt-engineering-for-developers/blob/9dbcb48416eb8af9ff9447388838521dc0f9acb0/content/LangChain Chat with Your Data/1.简介 Introduction.md">《LangChain - Chat With Your Data》</a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter<br><br>text_splitter = RecursiveCharacterTextSplitter(<br>    chunk_size=<span class="hljs-number">500</span>, chunk_overlap=<span class="hljs-number">150</span>)<br>split_docs = text_splitter.split_documents(docs)<br></code></pre></td></tr></table></figure>
<p>接着我们选用开源词向量模型 <a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2">Sentence Transformer</a> 来进行文本向量化。LangChain 提供了直接引入 HuggingFace 开源社区中的模型进行向量化的接口：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><br>embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>同时，考虑到 Chroma 是目前最常用的入门数据库，我们选择 Chroma 作为向量数据库，基于上文分块后的文档以及加载的开源向量化模型，将语料加载到指定路径下的向量数据库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><br><span class="hljs-comment"># 定义持久化路径</span><br>persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><span class="hljs-comment"># 加载数据库</span><br>vectordb = Chroma.from_documents(<br>    documents=split_docs,<br>    embedding=embeddings,<br>    persist_directory=persist_directory  <span class="hljs-comment"># 允许我们将persist_directory目录保存到磁盘上</span><br>)<br><span class="hljs-comment"># 将加载的向量数据库持久化到磁盘上</span><br>vectordb.persist()<br></code></pre></td></tr></table></figure>
<h3 id="整体脚本"><a href="#整体脚本" class="headerlink" title="整体脚本"></a>整体脚本</h3><p>将上述代码整合在一起为知识库搭建的脚本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 首先导入所需第三方库</span><br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredFileLoader<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredMarkdownLoader<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 获取文件路径函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_files</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    file_list = []<br>    <span class="hljs-keyword">for</span> filepath, dirnames, filenames <span class="hljs-keyword">in</span> os.walk(dir_path):<br>        <span class="hljs-comment"># os.walk 函数将递归遍历指定文件夹</span><br>        <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> filenames:<br>            <span class="hljs-comment"># 通过后缀名判断文件类型是否满足要求</span><br>            <span class="hljs-keyword">if</span> filename.endswith(<span class="hljs-string">&quot;.md&quot;</span>):<br>                <span class="hljs-comment"># 如果满足要求，将其绝对路径加入到结果列表</span><br>                file_list.append(os.path.join(filepath, filename))<br>            <span class="hljs-keyword">elif</span> filename.endswith(<span class="hljs-string">&quot;.txt&quot;</span>):<br>                file_list.append(os.path.join(filepath, filename))<br>    <span class="hljs-keyword">return</span> file_list<br><br><span class="hljs-comment"># 加载文件函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    <span class="hljs-comment"># 首先调用上文定义的函数得到目标文件路径列表</span><br>    file_lst = get_files(dir_path)<br>    <span class="hljs-comment"># docs 存放加载之后的纯文本对象</span><br>    docs = []<br>    <span class="hljs-comment"># 遍历所有目标文件</span><br>    <span class="hljs-keyword">for</span> one_file <span class="hljs-keyword">in</span> tqdm(file_lst):<br>        file_type = one_file.split(<span class="hljs-string">&#x27;.&#x27;</span>)[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> file_type == <span class="hljs-string">&#x27;md&#x27;</span>:<br>            loader = UnstructuredMarkdownLoader(one_file)<br>        <span class="hljs-keyword">elif</span> file_type == <span class="hljs-string">&#x27;txt&#x27;</span>:<br>            loader = UnstructuredFileLoader(one_file)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 如果是不符合条件的文件，直接跳过</span><br>            <span class="hljs-keyword">continue</span><br>        docs.extend(loader.load())<br>    <span class="hljs-keyword">return</span> docs<br><br><span class="hljs-comment"># 目标文件夹</span><br>tar_dir = [<br>    <span class="hljs-string">&quot;/root/data/InternLM&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/InternLM-XComposer&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/lagent&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/lmdeploy&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/opencompass&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/xtuner&quot;</span><br>]<br><br><span class="hljs-comment"># 加载目标文件</span><br>docs = []<br><span class="hljs-keyword">for</span> dir_path <span class="hljs-keyword">in</span> tar_dir:<br>    docs.extend(get_text(dir_path))<br><br><span class="hljs-comment"># 对文本进行分块</span><br>text_splitter = RecursiveCharacterTextSplitter(<br>    chunk_size=<span class="hljs-number">500</span>, chunk_overlap=<span class="hljs-number">150</span>)<br>split_docs = text_splitter.split_documents(docs)<br><br><span class="hljs-comment"># 加载开源词向量模型</span><br>embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br><br><span class="hljs-comment"># 构建向量数据库</span><br><span class="hljs-comment"># 定义持久化路径</span><br>persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><span class="hljs-comment"># 加载数据库</span><br>vectordb = Chroma.from_documents(<br>    documents=split_docs,<br>    embedding=embeddings,<br>    persist_directory=persist_directory  <span class="hljs-comment"># 允许我们将persist_directory目录保存到磁盘上</span><br>)<br><span class="hljs-comment"># 将加载的向量数据库持久化到磁盘上</span><br>vectordb.persist()<br></code></pre></td></tr></table></figure>
<p>可以在 <code>/root/data</code> 下新建一个 <code>demo</code>目录，将该脚本和后续脚本均放在该目录下运行。运行上述脚本，即可在本地构建已持久化的向量数据库，后续直接导入该数据库即可，无需重复构建。</p>
<h2 id="InternLM-接入-LangChain"><a href="#InternLM-接入-LangChain" class="headerlink" title="InternLM 接入 LangChain"></a>InternLM 接入 LangChain</h2><p>为便捷构建 LLM 应用，我们需要基于本地部署的 InternLM，继承 LangChain 的 LLM 类自定义一个 InternLM LLM 子类，从而实现将 InternLM 接入到 LangChain 框架中。完成 LangChain 的自定义 LLM 子类之后，可以以完全一致的方式调用 LangChain 的接口，而无需考虑底层模型调用的不一致。</p>
<p>基于本地部署的 InternLM 自定义 LLM 类并不复杂，我们只需从 LangChain.llms.base.LLM 类继承一个子类，并重写构造函数与 <code>_call</code> 函数即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.llms.base <span class="hljs-keyword">import</span> LLM<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span>, <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span><br><span class="hljs-keyword">from</span> langchain.callbacks.manager <span class="hljs-keyword">import</span> CallbackManagerForLLMRun<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InternLM_LLM</span>(<span class="hljs-title class_ inherited__">LLM</span>):<br>    <span class="hljs-comment"># 基于本地 InternLM 自定义 LLM 类</span><br>    tokenizer : AutoTokenizer = <span class="hljs-literal">None</span><br>    model: AutoModelForCausalLM = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path :<span class="hljs-built_in">str</span></span>):<br>        <span class="hljs-comment"># model_path: InternLM 模型路径</span><br>        <span class="hljs-comment"># 从本地初始化模型</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;正在从本地加载模型...&quot;</span>)<br>        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=<span class="hljs-literal">True</span>)<br>        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=<span class="hljs-literal">True</span>).to(torch.bfloat16).cuda()<br>        self.model = self.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;完成本地模型的加载&quot;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_call</span>(<span class="hljs-params">self, prompt : <span class="hljs-built_in">str</span>, stop: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                run_manager: <span class="hljs-type">Optional</span>[CallbackManagerForLLMRun] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                **kwargs: <span class="hljs-type">Any</span></span>):<br>        <span class="hljs-comment"># 重写调用函数</span><br>        system_prompt = <span class="hljs-string">&quot;&quot;&quot;You are an AI assistant whose name is InternLM (书生·浦语).</span><br><span class="hljs-string">        - InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span><br><span class="hljs-string">        - InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <br>        messages = [(system_prompt, <span class="hljs-string">&#x27;&#x27;</span>)]<br>        response, history = self.model.chat(self.tokenizer, prompt , history=messages)<br>        <span class="hljs-keyword">return</span> response<br>        <br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_llm_type</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;InternLM&quot;</span><br></code></pre></td></tr></table></figure>
<p>在上述类定义中，重写了构造函数和 <code>_call</code> 函数：对于构造函数，我们在对象实例化的一开始加载本地部署的 InternLM 模型，从而避免每一次调用都需要重新加载模型带来的时间过长；<code>_call</code> 函数是 LLM 类的核心函数，LangChain 会调用该函数来调用 LLM，在该函数中，我们调用已实例化模型的 chat 方法，从而实现对模型的调用并返回调用结果。</p>
<p>在整体项目中，我们将上述代码封装为 LLM.py，后续将直接从该文件中引入自定义的 LLM 类。</p>
<h2 id="构建检索问答链"><a href="#构建检索问答链" class="headerlink" title="构建检索问答链"></a>构建检索问答链</h2><p>LangChain 通过提供检索问答链对象来实现对于 RAG 全流程的封装。所谓检索问答链，即通过一个对象完成检索增强问答（即RAG）的全流程，针对 RAG 的更多概念，我们会在视频内容中讲解，也欢迎读者查阅该教程来进一步了解：<a href="https://github.com/datawhalechina/llm-universe/tree/main">《LLM Universe》</a>。我们可以调用一个 LangChain 提供的 <code>RetrievalQA</code> 对象，通过初始化时填入已构建的数据库和自定义 LLM 作为参数，来简便地完成检索增强问答的全流程，LangChain 会自动完成基于用户提问进行检索、获取相关文档、拼接为合适的 Prompt 并交给 LLM 问答的全部流程。</p>
<h3 id="加载向量数据库"><a href="#加载向量数据库" class="headerlink" title="加载向量数据库"></a>加载向量数据库</h3><p>首先我们需要将上文构建的向量数据库导入进来，我们可以直接通过 Chroma 以及上文定义的词向量模型来加载已构建的数据库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 定义 Embeddings</span><br>embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br><br><span class="hljs-comment"># 向量数据库持久化路径</span><br>persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><br><span class="hljs-comment"># 加载数据库</span><br>vectordb = Chroma(<br>    persist_directory=persist_directory, <br>    embedding_function=embeddings<br>)<br></code></pre></td></tr></table></figure>
<p>上述代码得到的 <code>vectordb</code> 对象即为我们已构建的向量数据库对象，该对象可以针对用户的 <code>query</code> 进行语义向量检索，得到与用户提问相关的知识片段。</p>
<h3 id="实例化自定义-LLM-与-Prompt-Template"><a href="#实例化自定义-LLM-与-Prompt-Template" class="headerlink" title="实例化自定义 LLM 与 Prompt Template"></a>实例化自定义 LLM 与 Prompt Template</h3><p>接着，我们实例化一个基于 InternLM 自定义的 LLM 对象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> LLM <span class="hljs-keyword">import</span> InternLM_LLM<br>llm = InternLM_LLM(model_path = <span class="hljs-string">&quot;/root/data/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span>)<br>llm.predict(<span class="hljs-string">&quot;你是谁&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>构建检索问答链，还需要构建一个 Prompt Template，该 Template 其实基于一个带变量的字符串，在检索之后，LangChain 会将检索到的相关文档片段填入到 Template 的变量中，从而实现带知识的 Prompt 构建。我们可以基于 LangChain 的 Template 基类来实例化这样一个 Template 对象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><br><span class="hljs-comment"># 我们所构造的 Prompt 模板</span><br>template = <span class="hljs-string">&quot;&quot;&quot;使用以下上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。</span><br><span class="hljs-string">问题: &#123;question&#125;</span><br><span class="hljs-string">可参考的上下文：</span><br><span class="hljs-string">···</span><br><span class="hljs-string">&#123;context&#125;</span><br><span class="hljs-string">···</span><br><span class="hljs-string">如果给定的上下文无法让你做出回答，请回答你不知道。</span><br><span class="hljs-string">有用的回答:&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 调用 LangChain 的方法来实例化一个 Template 对象，该对象包含了 context 和 question 两个变量，在实际调用时，这两个变量会被检索到的文档片段和用户提问填充</span><br>QA_CHAIN_PROMPT = PromptTemplate(input_variables=[<span class="hljs-string">&quot;context&quot;</span>,<span class="hljs-string">&quot;question&quot;</span>],template=template)<br></code></pre></td></tr></table></figure>
<h3 id="构建检索问答链-1"><a href="#构建检索问答链-1" class="headerlink" title="构建检索问答链"></a>构建检索问答链</h3><p>最后，可以调用 LangChain 提供的检索问答链构造函数，基于我们的自定义 LLM、Prompt Template 和向量知识库来构建一个基于 InternLM 的检索问答链：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br><br>qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectordb.as_retriever(),return_source_documents=<span class="hljs-literal">True</span>,chain_type_kwargs=&#123;<span class="hljs-string">&quot;prompt&quot;</span>:QA_CHAIN_PROMPT&#125;)<br></code></pre></td></tr></table></figure>
<p>得到的 <code>qa_chain</code> 对象即可以实现我们的核心功能，即基于 InternLM 模型的专业知识库助手。我们可以对比该检索问答链和纯 LLM 的问答效果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 检索问答链回答效果</span><br>question = <span class="hljs-string">&quot;什么是InternLM&quot;</span><br>result = qa_chain(&#123;<span class="hljs-string">&quot;query&quot;</span>: question&#125;)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;检索问答链回答 question 的结果：&quot;</span>)<br><span class="hljs-built_in">print</span>(result[<span class="hljs-string">&quot;result&quot;</span>])<br><br><span class="hljs-comment"># 仅 LLM 回答效果</span><br>result_2 = llm(question)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;大模型回答 question 的结果：&quot;</span>)<br><span class="hljs-built_in">print</span>(result_2)<br></code></pre></td></tr></table></figure>
<h2 id="部署一个-Web-Demo"><a href="#部署一个-Web-Demo" class="headerlink" title="部署一个 Web Demo"></a>部署一个 Web Demo</h2><p>之后我们可以基于 Gradio 框架将其部署到 Web 网页，从而搭建一个小型 Demo，便于测试与使用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入必要的库</span><br><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> LLM <span class="hljs-keyword">import</span> InternLM_LLM<br><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_chain</span>():<br>    <span class="hljs-comment"># 加载问答链</span><br>    <span class="hljs-comment"># 定义 Embeddings</span><br>    embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br><br>    <span class="hljs-comment"># 向量数据库持久化路径</span><br>    persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><br>    <span class="hljs-comment"># 加载数据库</span><br>    vectordb = Chroma(<br>        persist_directory=persist_directory,  <span class="hljs-comment"># 允许我们将persist_directory目录保存到磁盘上</span><br>        embedding_function=embeddings<br>    )<br><br>    llm = InternLM_LLM(model_path = <span class="hljs-string">&quot;/root/data/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span>)<br><br>    template = <span class="hljs-string">&quot;&quot;&quot;使用以下上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。</span><br><span class="hljs-string">    问题: &#123;question&#125;</span><br><span class="hljs-string">    可参考的上下文：</span><br><span class="hljs-string">    ···</span><br><span class="hljs-string">    &#123;context&#125;</span><br><span class="hljs-string">    ···</span><br><span class="hljs-string">    如果给定的上下文无法让你做出回答，请回答你不知道。</span><br><span class="hljs-string">    有用的回答:&quot;&quot;&quot;</span><br><br>    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[<span class="hljs-string">&quot;context&quot;</span>,<span class="hljs-string">&quot;question&quot;</span>],<br>                                    template=template)<br><br>    <span class="hljs-comment"># 运行 chain</span><br>    <span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br><br>    qa_chain = RetrievalQA.from_chain_type(llm,<br>                                        retriever=vectordb.as_retriever(),<br>                                        return_source_documents=<span class="hljs-literal">True</span>,<br>                                        chain_type_kwargs=&#123;<span class="hljs-string">&quot;prompt&quot;</span>:QA_CHAIN_PROMPT&#125;)<br>    <br>    <span class="hljs-keyword">return</span> qa_chain<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_center</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    存储问答 Chain 的对象 </span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.chain = load_chain()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">qa_chain_self_answer</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, chat_history: <span class="hljs-built_in">list</span> = []</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        调用不带历史记录的问答链进行回答</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> question == <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">len</span>(question) &lt; <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, chat_history<br>        <span class="hljs-keyword">try</span>:<br>            chat_history.append(<br>                (question, self.chain(&#123;<span class="hljs-string">&quot;query&quot;</span>: question&#125;)[<span class="hljs-string">&quot;result&quot;</span>]))<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, chat_history<br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            <span class="hljs-keyword">return</span> e, chat_history<br><br><br>model_center = Model_center()<br><br>block = gr.Blocks()<br><span class="hljs-keyword">with</span> block <span class="hljs-keyword">as</span> demo:<br>    <span class="hljs-keyword">with</span> gr.Row(equal_height=<span class="hljs-literal">True</span>):   <br>        <span class="hljs-keyword">with</span> gr.Column(scale=<span class="hljs-number">15</span>):<br>            gr.Markdown(<span class="hljs-string">&quot;&quot;&quot;&lt;h1&gt;&lt;center&gt;InternLM&lt;/center&gt;&lt;/h1&gt;</span><br><span class="hljs-string">                &lt;center&gt;书生浦语&lt;/center&gt;</span><br><span class="hljs-string">                &quot;&quot;&quot;</span>)<br>        <span class="hljs-comment"># gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)</span><br><br>    <span class="hljs-keyword">with</span> gr.Row():<br>        <span class="hljs-keyword">with</span> gr.Column(scale=<span class="hljs-number">4</span>):<br>            chatbot = gr.Chatbot(height=<span class="hljs-number">450</span>, show_copy_button=<span class="hljs-literal">True</span>)<br>            <span class="hljs-comment"># 创建一个文本框组件，用于输入 prompt。</span><br>            msg = gr.Textbox(label=<span class="hljs-string">&quot;Prompt/问题&quot;</span>)<br><br>            <span class="hljs-keyword">with</span> gr.Row():<br>                <span class="hljs-comment"># 创建提交按钮。</span><br>                db_wo_his_btn = gr.Button(<span class="hljs-string">&quot;Chat&quot;</span>)<br>            <span class="hljs-keyword">with</span> gr.Row():<br>                <span class="hljs-comment"># 创建一个清除按钮，用于清除聊天机器人组件的内容。</span><br>                clear = gr.ClearButton(<br>                    components=[chatbot], value=<span class="hljs-string">&quot;Clear console&quot;</span>)<br>                <br>        <span class="hljs-comment"># 设置按钮的点击事件。当点击时，调用上面定义的 qa_chain_self_answer 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。</span><br>        db_wo_his_btn.click(model_center.qa_chain_self_answer, inputs=[<br>                            msg, chatbot], outputs=[msg, chatbot])<br>        <br>    gr.Markdown(<span class="hljs-string">&quot;&quot;&quot;提醒：&lt;br&gt;</span><br><span class="hljs-string">    1. 初始化数据库时间可能较长，请耐心等待。</span><br><span class="hljs-string">    2. 使用中如果出现异常，将会在文本输入框进行展示，请不要惊慌。 &lt;br&gt;</span><br><span class="hljs-string">    &quot;&quot;&quot;</span>)<br><span class="hljs-comment"># threads to consume the request</span><br>gr.close_all()<br><span class="hljs-comment"># 启动新的 Gradio 应用，设置分享功能为 True，并使用环境变量 PORT1 指定服务器端口。</span><br><span class="hljs-comment"># demo.launch(share=True, server_port=int(os.environ[&#x27;PORT1&#x27;]))</span><br><span class="hljs-comment"># 直接启动</span><br>demo.launch()<br></code></pre></td></tr></table></figure>
<p>运行截图如下：</p>
<p><img  src="gradio.png"  ><span class="image-caption">运行gradio</span></p>
<p><img  src="Langchain+InternLM问答.png"  ><span class="image-caption">Langchain+InternLM问答</span></p>
<p>如图，能够正确地回答知识库中的知识。</p>
<h2 id="问题解决以及-Langchain-调试"><a href="#问题解决以及-Langchain-调试" class="headerlink" title="问题解决以及 Langchain 调试"></a>问题解决以及 Langchain 调试</h2><p>我们在遇到奇怪问题的时候，想要调试 Langchain，这个时候可以借助 Langchain 的全局设置设置调试模式，设置方式如下所示：</p>
<p><a href="https://python.langchain.com/docs/guides/debugging">Debugging | 🦜️🔗 Langchain</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.<span class="hljs-built_in">globals</span> <span class="hljs-keyword">import</span> set_verbose <span class="hljs-comment"># 我这里用的 langchain 版本为 0.1.0</span><br><br>set_verbose(<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<p><img  src="langchain-debug.png"  ><span class="image-caption">langchain 的调试输出</span></p>
<h2 id="将应用部署在-OpenXLab-上"><a href="#将应用部署在-OpenXLab-上" class="headerlink" title="将应用部署在 OpenXLab 上"></a>将应用部署在 OpenXLab 上</h2><p><a href="https://openxlab.org.cn/apps/detail/EnableAsync/network-bot">计算机网络问答机器人</a></p>
<h3 id="Sqlite-问题1"><a href="#Sqlite-问题1" class="headerlink" title="Sqlite 问题1"></a>Sqlite 问题<sup><a href="#fn_1" id="reffn_1">1</a></sup></h3><p>OpenXLab 上的 sqlite3 版本低于我们项目用的 Chroma 要求。可参考<a href="https://link.zhihu.com/?target=https%3A//docs.trychroma.com/troubleshooting%23sqlite"> Troubleshooting | Chroma (trychroma.com)</a>，在 <code>requirements.txt</code> 中添加 <code>pysqlite3-binary</code> ，之后加载 sqlite3 库来绕过这个问题。否则就要写脚本在运行时自己安装上更新版本的sqlite3了。下面是修改加载 sqlite3 库的 trick 命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">__import__</span>(<span class="hljs-string">&#x27;pysqlite3&#x27;</span>)<br><span class="hljs-keyword">import</span> sys<br>sys.modules[<span class="hljs-string">&#x27;sqlite3&#x27;</span>] = sys.modules.pop(<span class="hljs-string">&#x27;pysqlite3&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="运行截图"><a href="#运行截图" class="headerlink" title="运行截图"></a>运行截图</h3><p><img  src="openxlab-deploy.png"  ><span class="image-caption">openxlab-deploy</span></p>
<p><img  src="loading.png"  ><span class="image-caption">加载模型</span></p>
<p><img  src="部署.png"  ><span class="image-caption">部署</span></p>
<p><img  src="运行日志.png"  ><span class="image-caption">运行日志</span></p>
<h2 id="参考内容"><a href="#参考内容" class="headerlink" title="参考内容"></a>参考内容</h2><blockquote id="fn_1">
<sup>1</sup>. <a href="https://zhuanlan.zhihu.com/p/676719586">书生・浦语大模型实战营第三课作业(基础+进阶) - 知乎 (zhihu.com)</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>Internlm-04-XTuner 大模型单卡低成本微调实战</title>
    <url>/internlm/internlm-04/</url>
    <content><![CDATA[<h1 id="XTuner-大模型单卡低成本微调实战"><a href="#XTuner-大模型单卡低成本微调实战" class="headerlink" title="XTuner 大模型单卡低成本微调实战"></a>XTuner 大模型单卡低成本微调实战</h1><p>微调前<br><img  src="官方回答.png"  ><span class="image-caption">官方回答</span></p>
<p>微调后<br><img  src="微调后.png"  ><span class="image-caption">微调后.png</span></p>
<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h2><h3 id="1-1-XTuner"><a href="#1-1-XTuner" class="headerlink" title="1.1 XTuner"></a>1.1 XTuner</h3><p>一个大语言模型微调工具箱。由 MMRazor 和 MMDeploy 联合开发。</p>
<h3 id="1-2-支持的开源LLM-2023-11-01"><a href="#1-2-支持的开源LLM-2023-11-01" class="headerlink" title="1.2 支持的开源LLM (2023.11.01)"></a>1.2 支持的开源LLM (2023.11.01)</h3><ul>
<li><a href="https://huggingface.co/internlm/internlm-7b">InternLM</a></li>
<li><a href="https://huggingface.co/meta-llama">Llama，Llama2</a></li>
<li><a href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2</a>，<a href="https://huggingface.co/THUDM/chatglm3-6b-base">ChatGLM3</a></li>
<li><a href="https://huggingface.co/Qwen/Qwen-7B">Qwen</a></li>
<li><a href="https://huggingface.co/baichuan-inc/Baichuan-7B">Baichuan</a>，<a href="https://huggingface.co/baichuan-inc/Baichuan2-7B-Base">Baichuan2</a></li>
<li><a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta">Zephyr</a> </li>
</ul>
<h3 id="1-3-特色"><a href="#1-3-特色" class="headerlink" title="1.3 特色"></a>1.3 特色</h3><ul>
<li><strong>傻瓜化：</strong> 以 配置文件 的形式封装了大部分微调场景，<strong>0基础的非专业人员也能一键开始微调</strong>。</li>
<li><strong>轻量级：</strong> 对于 7B 参数量的LLM，<strong>微调所需的最小显存仅为 8GB</strong></li>
</ul>
<h3 id="1-4-微调原理"><a href="#1-4-微调原理" class="headerlink" title="1.4 微调原理"></a>1.4 微调原理</h3><blockquote>
<p>想象一下，你有一个超大的玩具，现在你想改造这个超大的玩具。但是，<strong>对整个玩具进行全面的改动会非常昂贵</strong>。</p>
</blockquote>
<p>※ 因此，你找到了一种叫 <strong>LoRA</strong> 的方法：<strong>只对玩具中的某些零件进行改动，而不是对整个玩具进行全面改动</strong>。</p>
<p>※ 而 <strong>QLoRA</strong> 是 LoRA 的一种改进</p>
<h2 id="2-快速上手"><a href="#2-快速上手" class="headerlink" title="2 快速上手"></a>2 快速上手</h2><h3 id="2-1-平台"><a href="#2-1-平台" class="headerlink" title="2.1 平台"></a>2.1 平台</h3><p>Ubuntu + Anaconda + CUDA/CUDNN + 8GB nvidia显卡</p>
<h3 id="2-2-安装"><a href="#2-2-安装" class="headerlink" title="2.2 安装"></a>2.2 安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 如果你是在 InternStudio 平台，则从本地 clone 一个已有 pytorch 2.0.1 的环境：</span><br>/root/share/install_conda_env_internlm_base.sh xtuner0.1.9<br><span class="hljs-comment"># 如果你是在其他平台：</span><br>conda create --name xtuner0.1.9 python=3.10 -y<br><br><span class="hljs-comment"># 激活环境</span><br>conda activate xtuner0.1.9<br><span class="hljs-comment"># 进入家目录 （~的意思是 “当前用户的home路径”）</span><br><span class="hljs-built_in">cd</span> ~<br><span class="hljs-comment"># 创建版本文件夹并进入，以跟随本教程</span><br><span class="hljs-built_in">mkdir</span> xtuner019 &amp;&amp; <span class="hljs-built_in">cd</span> xtuner019<br><br><br><span class="hljs-comment"># 拉取 0.1.9 的版本源码</span><br>git <span class="hljs-built_in">clone</span> -b v0.1.9  https://github.com/InternLM/xtuner<br><span class="hljs-comment"># 无法访问github的用户请从 gitee 拉取:</span><br><span class="hljs-comment"># git clone -b v0.1.9 https://gitee.com/Internlm/xtuner</span><br><br><span class="hljs-comment"># 进入源码目录</span><br><span class="hljs-built_in">cd</span> xtuner<br><br><span class="hljs-comment"># 从源码安装 XTuner</span><br>pip install -e <span class="hljs-string">&#x27;.[all]&#x27;</span><br></code></pre></td></tr></table></figure>
<p>安装完后，就开始搞搞准备工作了。（准备在 oasst1 数据集上微调 internlm-7b-chat）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建一个微调 oasst1 数据集的工作路径，进入</span><br><span class="hljs-built_in">mkdir</span> ~/ft-oasst1 &amp;&amp; <span class="hljs-built_in">cd</span> ~/ft-oasst1<br></code></pre></td></tr></table></figure>
<h3 id="2-3-微调"><a href="#2-3-微调" class="headerlink" title="2.3 微调"></a>2.3 微调</h3><h4 id="2-3-1-准备配置文件"><a href="#2-3-1-准备配置文件" class="headerlink" title="2.3.1 准备配置文件"></a>2.3.1 准备配置文件</h4><p>XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-comment"># 列出所有内置配置</span><br>xtuner list-cfg<br></code></pre></td></tr></table></figure>
<blockquote>
<p>假如显示bash: xtuner: command not found的话可以考虑在终端输入 export PATH=$PATH:’/root/.local/bin’</p>
</blockquote>
<p><img  src="cfg-list.png"  ><span class="image-caption">部分配置文件展示</span></p>
<p>拷贝一个配置文件到当前目录：<br><code># xtuner copy-cfg $&#123;CONFIG_NAME&#125; $&#123;SAVE_PATH&#125;</code></p>
<p>在本案例中即：（注意最后有个英文句号，代表复制到当前路径）<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-built_in">cd</span> ~/ft-oasst1<br>xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .<br></code></pre></td></tr></table></figure></p>
<p>配置文件名的解释：</p>
<blockquote>
<p>xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型名</th>
<th>internlm_chat_7b</th>
</tr>
</thead>
<tbody>
<tr>
<td>使用算法</td>
<td>qlora</td>
</tr>
<tr>
<td>数据集</td>
<td>oasst1</td>
</tr>
<tr>
<td>把数据集跑几次</td>
<td>跑3次：e3 (epoch 3 )</td>
</tr>
</tbody>
</table>
</div>
<p>*无 chat比如 <code>internlm-7b</code> 代表是基座(base)模型</p>
<h4 id="2-3-2-模型下载"><a href="#2-3-2-模型下载" class="headerlink" title="2.3.2 模型下载"></a>2.3.2 模型下载</h4><blockquote>
<p>由于下载模型很慢，用教学平台的同学可以直接复制模型。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-built_in">cp</span> -r /root/share/temp/model_repos/internlm-chat-7b ~/ft-oasst1/<br></code></pre></td></tr></table></figure>
<blockquote>
<p>以下是自己下载模型的步骤。</p>
</blockquote>
<p>不用 xtuner 默认的<code>从 huggingface 拉取模型</code>，而是提前从 ModelScope 下载模型到本地</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-comment"># 创建一个目录，放模型文件，防止散落一地</span><br><span class="hljs-built_in">mkdir</span> ~/ft-oasst1/internlm-chat-7b<br><br><span class="hljs-comment"># 装一下拉取模型文件要用的库</span><br>pip install modelscope<br><br><span class="hljs-comment"># 从 modelscope 下载下载模型文件</span><br><span class="hljs-built_in">cd</span> ~/ft-oasst1<br>apt install git git-lfs -y<br>git lfs install<br>git lfs <span class="hljs-built_in">clone</span> https://modelscope.cn/Shanghai_AI_Laboratory/internlm-chat-7b.git -b v1.0.3<br></code></pre></td></tr></table></figure>
<h4 id="2-3-3-数据集下载"><a href="#2-3-3-数据集下载" class="headerlink" title="2.3.3 数据集下载"></a>2.3.3 数据集下载</h4><blockquote>
<p><a href="https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/main">https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/main</a></p>
</blockquote>
<p>由于 huggingface 网络问题，咱们已经给大家提前下载好了，复制到正确位置即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~/ft-oasst1<br><span class="hljs-comment"># ...-guanaco 后面有个空格和英文句号啊</span><br><span class="hljs-built_in">cp</span> -r /root/share/temp/datasets/openassistant-guanaco .<br></code></pre></td></tr></table></figure>
<p>此时，当前路径的文件应该长这样：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">|-- internlm-chat-7b<br>|   |-- README.md<br>|   |-- config.json<br>|   |-- configuration.json<br>|   |-- configuration_internlm.py<br>|   |-- generation_config.json<br>|   |-- modeling_internlm.py<br>|   |-- pytorch_model-00001-of-00008.bin<br>|   |-- pytorch_model-00002-of-00008.bin<br>|   |-- pytorch_model-00003-of-00008.bin<br>|   |-- pytorch_model-00004-of-00008.bin<br>|   |-- pytorch_model-00005-of-00008.bin<br>|   |-- pytorch_model-00006-of-00008.bin<br>|   |-- pytorch_model-00007-of-00008.bin<br>|   |-- pytorch_model-00008-of-00008.bin<br>|   |-- pytorch_model.bin.index.json<br>|   |-- special_tokens_map.json<br>|   |-- tokenization_internlm.py<br>|   |-- tokenizer.model<br>|   `-- tokenizer_config.json<br>|-- internlm_chat_7b_qlora_oasst1_e3_copy.py<br>`-- openassistant-guanaco<br>    |-- openassistant_best_replies_eval.jsonl<br>    `-- openassistant_best_replies_train.jsonl<br></code></pre></td></tr></table></figure>
<h4 id="2-3-4-修改配置文件"><a href="#2-3-4-修改配置文件" class="headerlink" title="2.3.4 修改配置文件"></a>2.3.4 修改配置文件</h4><p>修改其中的模型和数据集为 本地路径</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~/ft-oasst1<br>vim internlm_chat_7b_qlora_oasst1_e3_copy.py<br></code></pre></td></tr></table></figure>
<blockquote>
<p>在vim界面完成修改后，请输入:wq退出。假如认为改错了可以用:q!退出且不保存。当然我们也可以考虑打开python文件直接修改，但注意修改完后需要按下Ctrl+S进行保存。</p>
</blockquote>
<p>减号代表要删除的行，加号代表要增加的行。<br><figure class="highlight diff"><table><tr><td class="code"><pre><code class="hljs diff"># 修改模型为本地路径<br><span class="hljs-deletion">- pretrained_model_name_or_path = &#x27;internlm/internlm-chat-7b&#x27;</span><br><span class="hljs-addition">+ pretrained_model_name_or_path = &#x27;./internlm-chat-7b&#x27;</span><br><br># 修改训练数据集为本地路径<br><span class="hljs-deletion">- data_path = &#x27;timdettmers/openassistant-guanaco&#x27;</span><br><span class="hljs-addition">+ data_path = &#x27;./openassistant-guanaco&#x27;</span><br></code></pre></td></tr></table></figure></p>
<p><strong>常用超参</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数名</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>data_path</strong></td>
<td>数据路径或 HuggingFace 仓库名</td>
</tr>
<tr>
<td>max_length</td>
<td>单条数据最大 Token 数，超过则截断</td>
</tr>
<tr>
<td>pack_to_max_length</td>
<td>是否将多条短数据拼接到 max_length，提高 GPU 利用率</td>
</tr>
<tr>
<td>accumulative_counts</td>
<td>梯度累积，每多少次 backward 更新一次参数</td>
</tr>
<tr>
<td>evaluation_inputs</td>
<td>训练过程中，会根据给定的问题进行推理，便于观测训练状态</td>
</tr>
<tr>
<td>evaluation_freq</td>
<td>Evaluation 的评测间隔 iter 数</td>
</tr>
<tr>
<td>……</td>
<td>……</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>如果想把显卡的现存吃满，充分利用显卡资源，可以将 <code>max_length</code> 和 <code>batch_size</code> 这两个参数调大。</p>
</blockquote>
<h4 id="2-3-5-开始微调"><a href="#2-3-5-开始微调" class="headerlink" title="2.3.5 开始微调"></a>2.3.5 开始微调</h4><p><strong>训练：</strong></p>
<p>xtuner train ${CONFIG_NAME_OR_PATH}</p>
<p><strong>也可以增加 deepspeed 进行训练加速：</strong></p>
<p>xtuner train ${CONFIG_NAME_OR_PATH} —deepspeed deepspeed_zero2</p>
<p>例如，我们可以利用 QLoRA 算法在 oasst1 数据集上微调 InternLM-7B：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-comment"># 单卡</span><br><span class="hljs-comment">## 用刚才改好的config文件训练</span><br>xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py<br><br><span class="hljs-comment"># 多卡</span><br>NPROC_PER_NODE=<span class="hljs-variable">$&#123;GPU_NUM&#125;</span> xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py<br><br><span class="hljs-comment"># 若要开启 deepspeed 加速，增加 --deepspeed deepspeed_zero2 即可</span><br></code></pre></td></tr></table></figure>
<blockquote>
<p>微调得到的 PTH 模型文件和其他杂七杂八的文件都默认在当前的 <code>./work_dirs</code> 中。</p>
</blockquote>
<p><img  src="train.png"  ><span class="image-caption">训练截图</span></p>
<p>跑完训练后，当前路径应该长这样：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash">|-- internlm-chat-7b<br>|-- internlm_chat_7b_qlora_oasst1_e3_copy.py<br>|-- openassistant-guanaco<br>|   |-- openassistant_best_replies_eval.jsonl<br>|   `-- openassistant_best_replies_train.jsonl<br>`-- work_dirs<br>    `-- internlm_chat_7b_qlora_oasst1_e3_copy<br>        |-- 20231101_152923<br>        |   |-- 20231101_152923.<span class="hljs-built_in">log</span><br>        |   `-- vis_data<br>        |       |-- 20231101_152923.json<br>        |       |-- config.py<br>        |       `-- scalars.json<br>        |-- epoch_1.pth<br>        |-- epoch_2.pth<br>        |-- epoch_3.pth<br>        |-- internlm_chat_7b_qlora_oasst1_e3_copy.py<br>        `-- last_checkpoint<br></code></pre></td></tr></table></figure>
<h4 id="2-3-6-将得到的-PTH-模型转换为-HuggingFace-模型，即：生成-Adapter-文件夹"><a href="#2-3-6-将得到的-PTH-模型转换为-HuggingFace-模型，即：生成-Adapter-文件夹" class="headerlink" title="2.3.6 将得到的 PTH 模型转换为 HuggingFace 模型，即：生成 Adapter 文件夹"></a>2.3.6 将得到的 PTH 模型转换为 HuggingFace 模型，<strong>即：生成 Adapter 文件夹</strong></h4><p><code>xtuner convert pth_to_hf $&#123;CONFIG_NAME_OR_PATH&#125; $&#123;PTH_file_dir&#125; $&#123;SAVE_PATH&#125;</code></p>
<p>在本示例中，为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> hf<br><span class="hljs-built_in">export</span> MKL_SERVICE_FORCE_INTEL=1<br><br>xtuner convert pth_to_hf ./internlm_chat_7b_qlora_oasst1_e3_copy.py ./work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_1.pth ./hf<br></code></pre></td></tr></table></figure><br>此时，路径中应该长这样：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash">|-- internlm-chat-7b<br>|-- internlm_chat_7b_qlora_oasst1_e3_copy.py<br>|-- openassistant-guanaco<br>|   |-- openassistant_best_replies_eval.jsonl<br>|   `-- openassistant_best_replies_train.jsonl<br>|-- hf<br>|   |-- README.md<br>|   |-- adapter_config.json<br>|   |-- adapter_model.bin<br>|   `-- xtuner_config.py<br>`-- work_dirs<br>    `-- internlm_chat_7b_qlora_oasst1_e3_copy<br>        |-- 20231101_152923<br>        |   |-- 20231101_152923.<span class="hljs-built_in">log</span><br>        |   `-- vis_data<br>        |       |-- 20231101_152923.json<br>        |       |-- config.py<br>        |       `-- scalars.json<br>        |-- epoch_1.pth<br>        |-- epoch_2.pth<br>        |-- epoch_3.pth<br>        |-- internlm_chat_7b_qlora_oasst1_e3_copy.py<br>        `-- last_checkpoint<br></code></pre></td></tr></table></figure>
<p><span style="color: red;"><strong>此时，hf 文件夹即为我们平时所理解的所谓 “LoRA 模型文件”</strong></span></p>
<blockquote>
<p>可以简单理解：LoRA 模型文件 = Adapter</p>
</blockquote>
<h3 id="2-4-部署与测试"><a href="#2-4-部署与测试" class="headerlink" title="2.4 部署与测试"></a>2.4 部署与测试</h3><h4 id="2-4-1-将-HuggingFace-adapter-合并到大语言模型："><a href="#2-4-1-将-HuggingFace-adapter-合并到大语言模型：" class="headerlink" title="2.4.1 将 HuggingFace adapter 合并到大语言模型："></a>2.4.1 将 HuggingFace adapter 合并到大语言模型：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash">xtuner convert merge ./internlm-chat-7b ./hf ./merged --max-shard-size 2GB<br><span class="hljs-comment"># xtuner convert merge \</span><br><span class="hljs-comment">#     $&#123;NAME_OR_PATH_TO_LLM&#125; \</span><br><span class="hljs-comment">#     $&#123;NAME_OR_PATH_TO_ADAPTER&#125; \</span><br><span class="hljs-comment">#     $&#123;SAVE_PATH&#125; \</span><br><span class="hljs-comment">#     --max-shard-size 2GB</span><br></code></pre></td></tr></table></figure>
<h4 id="2-4-2-与合并后的模型对话："><a href="#2-4-2-与合并后的模型对话：" class="headerlink" title="2.4.2 与合并后的模型对话："></a>2.4.2 与合并后的模型对话：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-comment"># 加载 Adapter 模型对话（Float 16）</span><br>xtuner chat ./merged --prompt-template internlm_chat<br><br><span class="hljs-comment"># 4 bit 量化加载</span><br><span class="hljs-comment"># xtuner chat ./merged --bits 4 --prompt-template internlm_chat</span><br></code></pre></td></tr></table></figure>
<h4 id="2-4-3-Demo"><a href="#2-4-3-Demo" class="headerlink" title="2.4.3 Demo"></a>2.4.3 Demo</h4><ul>
<li>修改 <code>cli_demo.py</code> 中的模型路径<figure class="highlight diff"><table><tr><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">- model_name_or_path = &quot;/root/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span><br><span class="hljs-addition">+ model_name_or_path = &quot;merged&quot;</span><br></code></pre></td></tr></table></figure></li>
<li>运行 <code>cli_demo.py</code> 以目测微调效果<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python ./cli_demo.py<br></code></pre></td></tr></table></figure>
</li>
</ul>
<p><strong><code>xtuner chat</code></strong> <strong>的启动参数</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>启动参数</th>
<th>干哈滴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>—prompt-template</strong></td>
<td>指定对话模板</td>
</tr>
<tr>
<td>—system</td>
<td>指定SYSTEM文本</td>
</tr>
<tr>
<td>—system-template</td>
<td>指定SYSTEM模板</td>
</tr>
<tr>
<td>-<strong>-bits</strong></td>
<td>LLM位数</td>
</tr>
<tr>
<td>—bot-name</td>
<td>bot名称</td>
</tr>
<tr>
<td>—with-plugins</td>
<td>指定要使用的插件</td>
</tr>
<tr>
<td><strong>—no-streamer</strong></td>
<td>是否启用流式传输</td>
</tr>
<tr>
<td><strong>—lagent</strong></td>
<td>是否使用lagent</td>
</tr>
<tr>
<td>—command-stop-word</td>
<td>命令停止词</td>
</tr>
<tr>
<td>—answer-stop-word</td>
<td>回答停止词</td>
</tr>
<tr>
<td>—offload-folder</td>
<td>存放模型权重的文件夹（或者已经卸载模型权重的文件夹）</td>
</tr>
<tr>
<td>—max-new-tokens</td>
<td>生成文本中允许的最大 <code>token</code> 数量</td>
</tr>
<tr>
<td><strong>—temperature</strong></td>
<td>温度值</td>
</tr>
<tr>
<td>—top-k</td>
<td>保留用于顶k筛选的最高概率词汇标记数</td>
</tr>
<tr>
<td>—top-p</td>
<td>如果设置为小于1的浮点数，仅保留概率相加高于 <code>top_p</code> 的最小一组最有可能的标记</td>
</tr>
<tr>
<td>—seed</td>
<td>用于可重现文本生成的随机种子</td>
</tr>
</tbody>
</table>
</div>
<h2 id="3-自定义微调"><a href="#3-自定义微调" class="headerlink" title="3 自定义微调"></a>3 自定义微调</h2><blockquote>
<p>以 <strong><a href="https://github.com/abachaa/Medication_QA_MedInfo2019">Medication QA</a></strong> <strong>数据集</strong>为例</p>
</blockquote>
<h3 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h3><h4 id="3-1-1-场景需求"><a href="#3-1-1-场景需求" class="headerlink" title="3.1.1 场景需求"></a>3.1.1 <strong>场景需求</strong></h4><p>   基于 InternLM-chat-7B 模型，用 MedQA 数据集进行微调，将其往<code>医学问答</code>领域对齐。</p>
<h4 id="3-1-2-真实数据预览"><a href="#3-1-2-真实数据预览" class="headerlink" title="3.1.2 真实数据预览"></a>3.1.2 <strong>真实数据预览</strong></h4><div class="table-container">
<table>
<thead>
<tr>
<th>问题</th>
<th>答案</th>
</tr>
</thead>
<tbody>
<tr>
<td>What are ketorolac eye drops?（什么是酮咯酸滴眼液？）</td>
<td>Ophthalmic   ketorolac is used to treat itchy eyes caused by allergies. It also is used to   treat swelling and redness (inflammation) that can occur after cataract   surgery. Ketorolac is in a class of medications called nonsteroidal   anti-inflammatory drugs (NSAIDs). It works by stopping the release of   substances that cause allergy symptoms and inflammation.</td>
</tr>
<tr>
<td>What medicines raise blood sugar? （什么药物会升高血糖？）</td>
<td>Some   medicines for conditions other than diabetes can raise your blood sugar   level. This is a concern when you have diabetes. Make sure every doctor you   see knows about all of the medicines, vitamins, or herbal supplements you   take. This means anything you take with or without a prescription. Examples include:     Barbiturates.     Thiazide diuretics.     Corticosteroids.     Birth control pills (oral contraceptives) and progesterone.     Catecholamines.     Decongestants that contain beta-adrenergic agents, such as pseudoephedrine.     The B vitamin niacin. The risk of high blood sugar from niacin lowers after you have taken it for a few months. The antipsychotic medicine olanzapine (Zyprexa).</td>
</tr>
</tbody>
</table>
</div>
<h3 id="3-2-数据准备"><a href="#3-2-数据准备" class="headerlink" title="3.2 数据准备"></a>3.2 数据准备</h3><blockquote>
<p><strong>以</strong> <strong><a href="https://github.com/abachaa/Medication_QA_MedInfo2019">Medication QA</a></strong> <strong>数据集为例</strong></p>
</blockquote>
<p><strong>原格式：(.xlsx)</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>问题</strong></th>
<th>药物类型</th>
<th>问题类型</th>
<th><strong>回答</strong></th>
<th>主题</th>
<th>URL</th>
</tr>
</thead>
<tbody>
<tr>
<td>aaa</td>
<td>bbb</td>
<td>ccc</td>
<td>ddd</td>
<td>eee</td>
<td>fff</td>
</tr>
</tbody>
</table>
</div>
<h4 id="3-2-1-将数据转为-XTuner-的数据格式"><a href="#3-2-1-将数据转为-XTuner-的数据格式" class="headerlink" title="3.2.1 将数据转为 XTuner 的数据格式"></a>3.2.1 将数据转为 XTuner 的数据格式</h4><p><strong>目标格式：(.jsonL)</strong></p>
<figure class="highlight json"><table><tr><td class="code"><pre><code class="hljs JSON"><span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;conversation&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;system&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;input&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;output&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;conversation&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;system&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;input&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;output&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure>
<p>通过 pytho n脚本：将 <code>.xlsx</code> 中的 问题 和 回答 两列 提取出来，再放入 <code>.jsonL</code> 文件的每个 conversation 的 input 和 output 中。</p>
<blockquote>
<p>这一步的 python 脚本可以请 ChatGPT 来完成。</p>
</blockquote>
<figure class="highlight text"><table><tr><td class="code"><pre><code class="hljs text">Write a python file for me. using openpyxl. input file name is MedQA2019.xlsx<br>Step1: The input file is .xlsx. Exact the column A and column D in the sheet named &quot;DrugQA&quot; .<br>Step2: Put each value in column A into each &quot;input&quot; of each &quot;conversation&quot;. Put each value in column D into each &quot;output&quot; of each &quot;conversation&quot;.<br>Step3: The output file is .jsonL. It looks like:<br>[&#123;<br>    &quot;conversation&quot;:[<br>        &#123;<br>            &quot;system&quot;: &quot;xxx&quot;,<br>            &quot;input&quot;: &quot;xxx&quot;,<br>            &quot;output&quot;: &quot;xxx&quot;<br>        &#125;<br>    ]<br>&#125;,<br>&#123;<br>    &quot;conversation&quot;:[<br>        &#123;<br>            &quot;system&quot;: &quot;xxx&quot;,<br>            &quot;input&quot;: &quot;xxx&quot;,<br>            &quot;output&quot;: &quot;xxx&quot;<br>        &#125;<br>    ]<br>&#125;]<br>Step4: All &quot;system&quot; value changes to &quot;You are a professional, highly experienced doctor professor. You always provide accurate, comprehensive, and detailed answers based on the patients&#x27; questions.&quot;<br></code></pre></td></tr></table></figure>
<blockquote>
<p>ChatGPT 生成的 python 代码见本仓库的 <a href="./xlsx2jsonl.py">xlsx2jsonl.py</a></p>
</blockquote>
<p>执行 python 脚本，获得格式化后的数据集：<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python xlsx2jsonl.py<br></code></pre></td></tr></table></figure></p>
<p>此时，当然也可以对数据进行训练集和测试集的分割，同样可以让 ChatGPT 写 python 代码。当然如果你没有严格的科研需求、不在乎“训练集泄露”的问题，也可以不做训练集与测试集的分割。</p>
<h4 id="3-2-2-划分训练集和测试集"><a href="#3-2-2-划分训练集和测试集" class="headerlink" title="3.2.2 划分训练集和测试集"></a>3.2.2 划分训练集和测试集</h4><figure class="highlight text"><table><tr><td class="code"><pre><code class="hljs text">my .jsonL file looks like:<br>[&#123;<br>    &quot;conversation&quot;:[<br>        &#123;<br>            &quot;system&quot;: &quot;xxx&quot;,<br>            &quot;input&quot;: &quot;xxx&quot;,<br>            &quot;output&quot;: &quot;xxx&quot;<br>        &#125;<br>    ]<br>&#125;,<br>&#123;<br>    &quot;conversation&quot;:[<br>        &#123;<br>            &quot;system&quot;: &quot;xxx&quot;,<br>            &quot;input&quot;: &quot;xxx&quot;,<br>            &quot;output&quot;: &quot;xxx&quot;<br>        &#125;<br>    ]<br>&#125;]<br>Step1, read the .jsonL file.<br>Step2, count the amount of the &quot;conversation&quot; elements.<br>Step3, randomly split all &quot;conversation&quot; elements by 7:3. Targeted structure is same as the input.<br>Step4, save the 7/10 part as train.jsonl. save the 3/10 part as test.jsonl<br></code></pre></td></tr></table></figure>
<p>生成的python代码见 <a href="./split2train_and_test.py">split2train_and_test.py</a></p>
<h3 id="3-3-开始自定义微调"><a href="#3-3-开始自定义微调" class="headerlink" title="3.3 开始自定义微调"></a>3.3 开始自定义微调</h3><p>此时，我们重新建一个文件夹来玩“微调自定义数据集”<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> ~/ft-medqa &amp;&amp; <span class="hljs-built_in">cd</span> ~/ft-medqa<br></code></pre></td></tr></table></figure></p>
<p>把前面下载好的internlm-chat-7b模型文件夹拷贝过来。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> -r ~/ft-oasst1/internlm-chat-7b .<br></code></pre></td></tr></table></figure>
<p>别忘了把自定义数据集，即几个 <code>.jsonL</code>，也传到服务器上。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/InternLM/tutorial<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> ~/tutorial/xtuner/MedQA2019-structured-train.jsonl .<br></code></pre></td></tr></table></figure>
<h4 id="3-3-1-准备配置文件"><a href="#3-3-1-准备配置文件" class="headerlink" title="3.3.1 准备配置文件"></a>3.3.1 准备配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 复制配置文件到当前目录</span><br>xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .<br><span class="hljs-comment"># 改个文件名</span><br><span class="hljs-built_in">mv</span> internlm_chat_7b_qlora_oasst1_e3_copy.py internlm_chat_7b_qlora_medqa2019_e3.py<br><br><span class="hljs-comment"># 修改配置文件内容</span><br>vim internlm_chat_7b_qlora_medqa2019_e3.py<br></code></pre></td></tr></table></figure>
<p>减号代表要删除的行，加号代表要增加的行。<br><figure class="highlight diff"><table><tr><td class="code"><pre><code class="hljs diff"># 修改import部分<br><span class="hljs-deletion">- from xtuner.dataset.map_fns import oasst1_map_fn, template_map_fn_factory</span><br><span class="hljs-addition">+ from xtuner.dataset.map_fns import template_map_fn_factory</span><br><br># 修改模型为本地路径<br><span class="hljs-deletion">- pretrained_model_name_or_path = &#x27;internlm/internlm-chat-7b&#x27;</span><br><span class="hljs-addition">+ pretrained_model_name_or_path = &#x27;./internlm-chat-7b&#x27;</span><br><br># 修改训练数据为 MedQA2019-structured-train.jsonl 路径<br><span class="hljs-deletion">- data_path = &#x27;timdettmers/openassistant-guanaco&#x27;</span><br><span class="hljs-addition">+ data_path = &#x27;MedQA2019-structured-train.jsonl&#x27;</span><br><br># 修改 train_dataset 对象<br>train_dataset = dict(<br>    type=process_hf_dataset,<br><span class="hljs-deletion">-   dataset=dict(type=load_dataset, path=data_path),</span><br><span class="hljs-addition">+   dataset=dict(type=load_dataset, path=&#x27;json&#x27;, data_files=dict(train=data_path)),</span><br>    tokenizer=tokenizer,<br>    max_length=max_length,<br><span class="hljs-deletion">-   dataset_map_fn=alpaca_map_fn,</span><br><span class="hljs-addition">+   dataset_map_fn=None,</span><br>    template_map_fn=dict(<br>        type=template_map_fn_factory, template=prompt_template),<br>    remove_unused_columns=True,<br>    shuffle_before_pack=True,<br>    pack_to_max_length=pack_to_max_length)<br></code></pre></td></tr></table></figure></p>
<h4 id="3-3-2-XTuner！启动！"><a href="#3-3-2-XTuner！启动！" class="headerlink" title="3.3.2 XTuner！启动！"></a>3.3.2 <strong>XTuner！启动！</strong></h4><p><img  src="imgs/ysqd.png"  ><span class="image-caption">tH8udZzECYl5are.png</span></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">xtuner train internlm_chat_7b_qlora_medqa2019_e3.py --deepspeed deepspeed_zero2<br></code></pre></td></tr></table></figure>
<h4 id="3-3-3-pth-转-huggingface"><a href="#3-3-3-pth-转-huggingface" class="headerlink" title="3.3.3 pth 转 huggingface"></a>3.3.3 pth 转 huggingface</h4><p>同前述，这里不赘述了。<a href="#236-将得到的-pth-模型转换为-huggingface-模型即生成adapter文件夹">将得到的-pth-模型转换为-huggingface-模型即生成adapter文件夹</a>  </p>
<h4 id="3-3-4-部署与测试"><a href="#3-3-4-部署与测试" class="headerlink" title="3.3.4 部署与测试"></a>3.3.4 部署与测试</h4><p>同前述。<a href="#24-部署与测试">部署与测试</a></p>
<h2 id="4-用-MS-Agent-数据集-赋予-LLM-以-Agent-能力"><a href="#4-用-MS-Agent-数据集-赋予-LLM-以-Agent-能力" class="headerlink" title="4 用 MS-Agent 数据集 赋予 LLM 以 Agent 能力"></a>4 用 MS-Agent 数据集 赋予 LLM 以 Agent 能力</h2><h3 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h3><p>MSAgent 数据集每条样本包含一个对话列表（conversations），其里面包含了 system、user、assistant 三种字段。其中：</p>
<ul>
<li><p>system: 表示给模型前置的人设输入，其中有告诉模型如何调用插件以及生成请求</p>
</li>
<li><p>user: 表示用户的输入 prompt，分为两种，通用生成的prompt和调用插件需求的 prompt</p>
</li>
<li><p>assistant: 为模型的回复。其中会包括插件调用代码和执行代码，调用代码是要 LLM 生成的，而执行代码是调用服务来生成结果的</p>
</li>
</ul>
<p>一条调用网页搜索插件查询“上海明天天气”的数据样本示例如下图所示：<br><img  src="imgs/msagent_data.png"  ><span class="image-caption">BlgfEqpiRFO5G6L.png</span></p>
<h3 id="4-2-微调步骤"><a href="#4-2-微调步骤" class="headerlink" title="4.2 微调步骤"></a>4.2 微调步骤</h3><h4 id="4-2-1-准备工作"><a href="#4-2-1-准备工作" class="headerlink" title="4.2.1 准备工作"></a>4.2.1 准备工作</h4><blockquote>
<p>xtuner 是从国内的 ModelScope 平台下载 MS-Agent 数据集，因此不用提前手动下载数据集文件。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 准备工作</span><br><span class="hljs-built_in">mkdir</span> ~/ft-msagent &amp;&amp; <span class="hljs-built_in">cd</span> ~/ft-msagent<br><span class="hljs-built_in">cp</span> -r ~/ft-oasst1/internlm-chat-7b .<br><br><span class="hljs-comment"># 查看配置文件</span><br>xtuner list-cfg | grep msagent<br><br><span class="hljs-comment"># 复制配置文件到当前目录</span><br>xtuner copy-cfg internlm_7b_qlora_msagent_react_e3_gpu8 .<br><br><span class="hljs-comment"># 修改配置文件中的模型为本地路径</span><br>vim ./internlm_7b_qlora_msagent_react_e3_gpu8_copy.py <br></code></pre></td></tr></table></figure>
<figure class="highlight diff"><table><tr><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">- pretrained_model_name_or_path = &#x27;internlm/internlm-chat-7b&#x27;</span><br><span class="hljs-addition">+ pretrained_model_name_or_path = &#x27;./internlm-chat-7b&#x27;</span><br></code></pre></td></tr></table></figure>
<h4 id="4-2-2-开始微调"><a href="#4-2-2-开始微调" class="headerlink" title="4.2.2 开始微调"></a>4.2.2 开始微调</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash">xtuner train ./internlm_7b_qlora_msagent_react_e3_gpu8_copy.py --deepspeed deepspeed_zero2<br></code></pre></td></tr></table></figure>
<h3 id="4-3-直接使用"><a href="#4-3-直接使用" class="headerlink" title="4.3 直接使用"></a>4.3 直接使用</h3><blockquote>
<p>由于 msagent 的训练非常费时，大家如果想尽快把这个教程跟完，可以直接从 modelScope 拉取咱们已经微调好了的 Adapter。如下演示。</p>
</blockquote>
<h4 id="4-3-1-下载-Adapter"><a href="#4-3-1-下载-Adapter" class="headerlink" title="4.3.1 下载 Adapter"></a>4.3.1 下载 Adapter</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-built_in">cd</span> ~/ft-msagent<br>apt install git git-lfs<br>git lfs install<br>git lfs <span class="hljs-built_in">clone</span> https://www.modelscope.cn/xtuner/internlm-7b-qlora-msagent-react.git<br></code></pre></td></tr></table></figure>
<p>OK，现在目录应该长这样：</p>
<ul>
<li>internlm_7b_qlora_msagent_react_e3_gpu8_copy.py</li>
<li>internlm-7b-qlora-msagent-react</li>
<li>internlm-chat-7b</li>
<li>work_dir（可有可无）</li>
</ul>
<p>有了这个在 msagent 上训练得到的Adapter，模型现在已经有 agent 能力了！就可以加 —lagent 以调用来自 lagent 的代理功能了！</p>
<h4 id="4-3-2-添加-serper-环境变量"><a href="#4-3-2-添加-serper-环境变量" class="headerlink" title="4.3.2 添加 serper 环境变量"></a>4.3.2 添加 serper 环境变量</h4><blockquote>
<p><strong>开始 chat 之前，还要加个 serper 的环境变量：</strong></p>
<p>去 serper.dev 免费注册一个账号，生成自己的 api key。这个东西是用来给 lagent 去获取 google 搜索的结果的。等于是 serper.dev 帮你去访问 google，而不是从你自己本地去访问 google 了。</p>
</blockquote>
<p><img  src="imgs/serper.png"  ><span class="image-caption">kDSdpQrhHfTWYsc.png</span></p>
<p>添加 serper api key 到环境变量：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SERPER_API_KEY=abcdefg<br></code></pre></td></tr></table></figure>
<h4 id="4-3-3-xtuner-agent，启动！"><a href="#4-3-3-xtuner-agent，启动！" class="headerlink" title="4.3.3 xtuner + agent，启动！"></a>4.3.3 xtuner + agent，启动！</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">xtuner chat ./internlm-chat-7b --adapter internlm-7b-qlora-msagent-react --lagent<br></code></pre></td></tr></table></figure>
<h2 id="5-注意事项"><a href="#5-注意事项" class="headerlink" title="5 注意事项"></a>5 注意事项</h2><p>本教程使用 xtuner 0.1.9 版本<br>若需要跟着本教程一步一步完成，建议严格遵循本教程的步骤！</p>
<p>若出现莫名其妙报错，请尝试更换为以下包的版本：（如果有报错再检查，没报错不用看）<br><figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">torch</span>                         <span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">transformers</span>                  <span class="hljs-number">4</span>.<span class="hljs-number">34</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">transformers</span>-stream-generator <span class="hljs-number">0</span>.<span class="hljs-number">0</span>.<span class="hljs-number">4</span><br></code></pre></td></tr></table></figure><br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install torch==2.1.1<br>pip install transformers==4.34.0<br>pip install transformers-stream-generator=0.0.4<br></code></pre></td></tr></table></figure><br>CUDA 相关：（如果有报错再检查，没报错不用看）<br><figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">NVIDIA</span>-SMI <span class="hljs-number">535</span>.<span class="hljs-number">54</span>.<span class="hljs-number">03</span>              <br><span class="hljs-attribute">Driver</span> Version: <span class="hljs-number">535</span>.<span class="hljs-number">54</span>.<span class="hljs-number">03</span>    <br><span class="hljs-attribute">CUDA</span> Version: <span class="hljs-number">12</span>.<span class="hljs-number">2</span><br><br><span class="hljs-attribute">nvidia</span>-cuda-cupti-cu12        <span class="hljs-number">12</span>.<span class="hljs-number">1</span>.<span class="hljs-number">105</span><br><span class="hljs-attribute">nvidia</span>-cuda-nvrtc-cu12        <span class="hljs-number">12</span>.<span class="hljs-number">1</span>.<span class="hljs-number">105</span><br><span class="hljs-attribute">nvidia</span>-cuda-runtime-cu12      <span class="hljs-number">12</span>.<span class="hljs-number">1</span>.<span class="hljs-number">105</span><br></code></pre></td></tr></table></figure></p>
<h2 id="6-作业"><a href="#6-作业" class="headerlink" title="6 作业"></a>6 作业</h2><h3 id="1-概述-1"><a href="#1-概述-1" class="headerlink" title="1 概述"></a>1 概述</h3><p>目标：通过微调，让模型成为我们的小助手</p>
<p>方式：使用 XTuner 进行微调</p>
<p><strong>微调前</strong><br><img  src="官方回答.png"  ><span class="image-caption">官方回答</span></p>
<p><strong>微调后</strong><br><img  src="微调后.png"  ><span class="image-caption">微调后.png</span></p>
<h3 id="2-实操"><a href="#2-实操" class="headerlink" title="2 实操"></a>2 实操</h3><h4 id="微调环境准备"><a href="#微调环境准备" class="headerlink" title="微调环境准备"></a>微调环境准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># InternStudio 平台中，从本地 clone 一个已有 pytorch 2.0.1 的环境（后续均在该环境执行，若为其他环境可作为参考）</span><br><span class="hljs-comment"># 进入环境后首先 bash</span><br><span class="hljs-comment"># 进入环境后首先 bash</span><br><span class="hljs-comment"># 进入环境后首先 bash</span><br>bash<br>conda create --name personal_assistant --<span class="hljs-built_in">clone</span>=/root/share/conda_envs/internlm-base<br><span class="hljs-comment"># 如果在其他平台：</span><br><span class="hljs-comment"># conda create --name personal_assistant python=3.10 -y</span><br><br><span class="hljs-comment"># 激活环境</span><br>conda activate personal_assistant<br><span class="hljs-comment"># 进入家目录 （~的意思是 “当前用户的home路径”）</span><br><span class="hljs-built_in">cd</span> ~<br><span class="hljs-comment"># 创建版本文件夹并进入，以跟随本教程</span><br><span class="hljs-comment"># personal_assistant用于存放本教程所使用的东西</span><br><span class="hljs-built_in">mkdir</span> /root/personal_assistant &amp;&amp; <span class="hljs-built_in">cd</span> /root/personal_assistant<br><span class="hljs-built_in">mkdir</span> /root/personal_assistant/xtuner019 &amp;&amp; <span class="hljs-built_in">cd</span> /root/personal_assistant/xtuner019<br><br><span class="hljs-comment"># 拉取 0.1.9 的版本源码</span><br>git <span class="hljs-built_in">clone</span> -b v0.1.9  https://github.com/InternLM/xtuner<br><span class="hljs-comment"># 无法访问github的用户请从 gitee 拉取:</span><br><span class="hljs-comment"># git clone -b v0.1.9 https://gitee.com/Internlm/xtuner</span><br><br><span class="hljs-comment"># 进入源码目录</span><br><span class="hljs-built_in">cd</span> xtuner<br><br><span class="hljs-comment"># 从源码安装 XTuner</span><br>pip install -e <span class="hljs-string">&#x27;.[all]&#x27;</span><br></code></pre></td></tr></table></figure>
<h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>创建<code>data</code>文件夹用于存放用于训练的数据集</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p /root/personal_assistant/data &amp;&amp; <span class="hljs-built_in">cd</span> /root/personal_assistant/data<br></code></pre></td></tr></table></figure>
<p>在<code>data</code>目录下创建一个json文件<code>personal_assistant.json</code>作为本次微调所使用的数据集。json中内容可参考下方(复制粘贴n次做数据增广，数据量小无法有效微调，下面仅用于展示格式，下面也有生成脚本)</p>
<p>其中<code>conversation</code>表示一次对话的内容，<code>input</code>为输入，即用户会问的问题，<code>output</code>为输出，即想要模型回答的答案。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;conversation&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>            <span class="hljs-punctuation">&#123;</span><br>                <span class="hljs-attr">&quot;input&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;请介绍一下你自己&quot;</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-attr">&quot;output&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;我是不要葱姜蒜大佬的小助手，内在是上海AI实验室书生·浦语的7B大模型哦&quot;</span><br>            <span class="hljs-punctuation">&#125;</span><br>        <span class="hljs-punctuation">]</span><br>    <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;conversation&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>            <span class="hljs-punctuation">&#123;</span><br>                <span class="hljs-attr">&quot;input&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;请做一下自我介绍&quot;</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-attr">&quot;output&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;我是不要葱姜蒜大佬的小助手，内在是上海AI实验室书生·浦语的7B大模型哦&quot;</span><br>            <span class="hljs-punctuation">&#125;</span><br>        <span class="hljs-punctuation">]</span><br>    <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure>
<p>以下是一个python脚本，用于生成数据集。在<code>data</code>目录下新建一个generate_data.py文件，将以下代码复制进去，然后运行该脚本即可生成数据集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><br><span class="hljs-comment"># 输入你的名字</span><br>name = <span class="hljs-string">&#x27;Shengshenlan&#x27;</span><br><span class="hljs-comment"># 重复次数</span><br>n = <span class="hljs-number">10000</span><br><br>data = [<br>    &#123;<br>        <span class="hljs-string">&quot;conversation&quot;</span>: [<br>            &#123;<br>                <span class="hljs-string">&quot;input&quot;</span>: <span class="hljs-string">&quot;请做一下自我介绍&quot;</span>,<br>                <span class="hljs-string">&quot;output&quot;</span>: <span class="hljs-string">&quot;我是&#123;&#125;的小助手，内在是上海AI实验室书生·浦语的7B大模型哦&quot;</span>.<span class="hljs-built_in">format</span>(name)<br>            &#125;<br>        ]<br>    &#125;<br>]<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>    data.append(data[<span class="hljs-number">0</span>])<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;personal_assistant.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    json.dump(data, f, ensure_ascii=<span class="hljs-literal">False</span>, indent=<span class="hljs-number">4</span>)<br><br></code></pre></td></tr></table></figure>
<h4 id="配置准备"><a href="#配置准备" class="headerlink" title="配置准备"></a>配置准备</h4><p>下载模型<code>InternLM-chat-7B</code></p>
<p><a href="https://studio.intern-ai.org.cn/">InternStudio</a> 平台的 <code>share</code> 目录下已经为我们准备了全系列的 <code>InternLM</code> 模型，可以使用如下命令复制<code>internlm-chat-7b</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p /root/personal_assistant/model/Shanghai_AI_Laboratory<br><span class="hljs-built_in">cp</span> -r /root/share/temp/model_repos/internlm-chat-7b /root/personal_assistant/model/Shanghai_AI_Laboratory<br></code></pre></td></tr></table></figure>
<p>XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 列出所有内置配置</span><br>xtuner list-cfg<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#创建用于存放配置的文件夹config并进入</span><br><span class="hljs-built_in">mkdir</span> /root/personal_assistant/config &amp;&amp; <span class="hljs-built_in">cd</span> /root/personal_assistant/config<br></code></pre></td></tr></table></figure>
<p>拷贝一个配置文件到当前目录：<code>xtuner copy-cfg $&#123;CONFIG_NAME&#125; $&#123;SAVE_PATH&#125;</code><br>在本例中：（注意最后有个英文句号，代表复制到当前路径）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .<br></code></pre></td></tr></table></figure>
<p>修改拷贝后的文件internlm_chat_7b_qlora_oasst1_e3_copy.py，修改下述位置：<br>(这是一份修改好的文件<a href="./internlm_chat_7b_qlora_oasst1_e3_copy.py">internlm_chat_7b_qlora_oasst1_e3_copy.py</a>)<br><img  src="修改配置.png"  ><span class="image-caption">修改配置</span></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># PART 1 中</span><br><span class="hljs-comment"># 预训练模型存放的位置</span><br>pretrained_model_name_or_path = <span class="hljs-string">&#x27;/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b&#x27;</span><br><br><span class="hljs-comment"># 微调数据存放的位置</span><br>data_path = <span class="hljs-string">&#x27;/root/personal_assistant/data/personal_assistant.json&#x27;</span><br><br><span class="hljs-comment"># 训练中最大的文本长度</span><br>max_length = 512<br><br><span class="hljs-comment"># 每一批训练样本的大小</span><br>batch_size = 2<br><br><span class="hljs-comment"># 最大训练轮数</span><br>max_epochs = 3<br><br><span class="hljs-comment"># 验证的频率</span><br>evaluation_freq = 90<br><br><span class="hljs-comment"># 用于评估输出内容的问题（用于评估的问题尽量与数据集的question保持一致）</span><br>evaluation_inputs = [ <span class="hljs-string">&#x27;请介绍一下你自己&#x27;</span>, <span class="hljs-string">&#x27;请做一下自我介绍&#x27;</span> ]<br><br><br><span class="hljs-comment"># PART 3 中</span><br>dataset=dict(<span class="hljs-built_in">type</span>=load_dataset, path=<span class="hljs-string">&#x27;json&#x27;</span>, data_files=dict(train=data_path))<br>dataset_map_fn=None<br></code></pre></td></tr></table></figure>
<h4 id="微调启动"><a href="#微调启动" class="headerlink" title="微调启动"></a>微调启动</h4><p>用<code>xtuner train</code>命令启动训练、</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">xtuner train /root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py<br></code></pre></td></tr></table></figure>
<p><img  src="训练过程.png"  ><span class="image-caption">训练数据样例</span></p>
<blockquote>
<p>会在训练完成后，输出用于验证的Sample output</p>
<h4 id="微调后参数转换-合并"><a href="#微调后参数转换-合并" class="headerlink" title="微调后参数转换/合并"></a>微调后参数转换/合并</h4></blockquote>
<p>训练后的pth格式参数转Hugging Face格式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建用于存放Hugging Face格式参数的hf文件夹</span><br><span class="hljs-built_in">mkdir</span> /root/personal_assistant/config/work_dirs/hf<br><br><span class="hljs-built_in">export</span> MKL_SERVICE_FORCE_INTEL=1<br><br><span class="hljs-comment"># 配置文件存放的位置</span><br><span class="hljs-built_in">export</span> CONFIG_NAME_OR_PATH=/root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py<br><br><span class="hljs-comment"># 模型训练后得到的pth格式参数存放的位置</span><br><span class="hljs-built_in">export</span> PTH=/root/personal_assistant/config/work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_3.pth<br><br><span class="hljs-comment"># pth文件转换为Hugging Face格式后参数存放的位置</span><br><span class="hljs-built_in">export</span> SAVE_PATH=/root/personal_assistant/config/work_dirs/hf<br><br><span class="hljs-comment"># 执行参数转换</span><br>xtuner convert pth_to_hf <span class="hljs-variable">$CONFIG_NAME_OR_PATH</span> <span class="hljs-variable">$PTH</span> <span class="hljs-variable">$SAVE_PATH</span><br></code></pre></td></tr></table></figure>
<p>Merge模型参数<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> MKL_SERVICE_FORCE_INTEL=1<br><span class="hljs-built_in">export</span> MKL_THREADING_LAYER=<span class="hljs-string">&#x27;GNU&#x27;</span><br><br><span class="hljs-comment"># 原始模型参数存放的位置</span><br><span class="hljs-built_in">export</span> NAME_OR_PATH_TO_LLM=/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b<br><br><span class="hljs-comment"># Hugging Face格式参数存放的位置</span><br><span class="hljs-built_in">export</span> NAME_OR_PATH_TO_ADAPTER=/root/personal_assistant/config/work_dirs/hf<br><br><span class="hljs-comment"># 最终Merge后的参数存放的位置</span><br><span class="hljs-built_in">mkdir</span> /root/personal_assistant/config/work_dirs/hf_merge<br><span class="hljs-built_in">export</span> SAVE_PATH=/root/personal_assistant/config/work_dirs/hf_merge<br><br><span class="hljs-comment"># 执行参数Merge</span><br>xtuner convert merge \<br>    <span class="hljs-variable">$NAME_OR_PATH_TO_LLM</span> \<br>    <span class="hljs-variable">$NAME_OR_PATH_TO_ADAPTER</span> \<br>    <span class="hljs-variable">$SAVE_PATH</span> \<br>    --max-shard-size 2GB<br></code></pre></td></tr></table></figure></p>
<h4 id="网页DEMO"><a href="#网页DEMO" class="headerlink" title="网页DEMO"></a>网页DEMO</h4><p>安装网页Demo所需依赖</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install streamlit==1.24.0<br></code></pre></td></tr></table></figure>
<p>下载 InternLM 项目代码</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建code文件夹用于存放InternLM项目代码</span><br><span class="hljs-built_in">mkdir</span> /root/personal_assistant/code &amp;&amp; <span class="hljs-built_in">cd</span> /root/personal_assistant/code<br>git <span class="hljs-built_in">clone</span> https://github.com/InternLM/InternLM.git<br></code></pre></td></tr></table></figure>
<p>将 <code>/root/code/InternLM/web_demo.py</code> 中 29 行和 33 行的模型路径更换为Merge后存放参数的路径 <code>/root/personal_assistant/config/work_dirs/hf_merge</code><br>运行 <code>/root/personal_assistant/code/InternLM</code> 目录下的 <code>web_demo.py</code> 文件，之后将端口映射到本地。在本地浏览器输入 <code>http://127.0.0.1:6006</code> 即可。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">streamlit run /root/personal_assistant/code/InternLM/web_demo<span class="hljs-selector-class">.py</span> <span class="hljs-attr">--server</span><span class="hljs-selector-class">.address</span> <span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span> <span class="hljs-attr">--server</span><span class="hljs-selector-class">.port</span> <span class="hljs-number">6006</span><br></code></pre></td></tr></table></figure>
<p>注意：要在浏览器打开 <code>http://127.0.0.1:6006</code> 页面后，模型才会加载。<br>在加载完模型之后，就可以与微调后的 InternLM-Chat-7B 进行对话了</p>
<h3 id="3-效果"><a href="#3-效果" class="headerlink" title="3 效果"></a>3 效果</h3><p>微调前<br><img  src="官方回答.png"  ><span class="image-caption">官方回答</span></p>
<p>微调后<br><img  src="微调后.png"  ><span class="image-caption">微调后.png</span></p>
<h2 id="7-进阶作业"><a href="#7-进阶作业" class="headerlink" title="7 进阶作业"></a>7 进阶作业</h2><h3 id="1-模型上传"><a href="#1-模型上传" class="headerlink" title="1 模型上传"></a>1 模型上传</h3><p><img  src="model-upload.png"  ><span class="image-caption">model-upload.png</span></p>
<h3 id="2-修改启动文件"><a href="#2-修改启动文件" class="headerlink" title="2 修改启动文件"></a>2 修改启动文件</h3><p>接下来需要修改启动文件以下载模型以及合并 lora 层，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> openxlab.model <span class="hljs-keyword">import</span> download<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Download</span>():<br>    download(model_repo=<span class="hljs-string">&#x27;OpenLMLab/InternLM-chat-7b&#x27;</span>,output=<span class="hljs-string">&#x27;/home/xlab-app-center/InternLM-chat-7b&#x27;</span>)<br>    download(model_repo=<span class="hljs-string">&#x27;EnableAsync/openxlab-assistant&#x27;</span>,output=<span class="hljs-string">&quot;/home/xlab-app-center/hf&quot;</span>)<br><br>Download()<br>os.system(<span class="hljs-string">&#x27;echo $PWD&#x27;</span>)<br>os.system(<span class="hljs-string">&#x27;ls&#x27;</span>)<br><br>os.system(<span class="hljs-string">&#x27;xtuner convert merge /home/xlab-app-center/InternLM-chat-7b /home/xlab-app-center/hf /home/xlab-app-center/hf-merge --max-shard-size 2GB&#x27;</span>)<br>os.system(<span class="hljs-string">&#x27;streamlit run /home/xlab-app-center/InternLM/web_demo.py --server.address=0.0.0.0 --server.port 7860&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="3-构建并运行"><a href="#3-构建并运行" class="headerlink" title="3 构建并运行"></a>3 构建并运行</h3><p><img  src="./build.png"  ><span class="image-caption">构建及运行</span></p>
<p>Github 地址如下：</p>
<p><a href="https://github.com/EnableAsync/openxlab-assistant">EnableAsync/openxlab-assistant (github.com)</a></p>
<p>运行地址如下：</p>
<p><a href="https://openxlab.org.cn/apps/detail/EnableAsync/openxlab-assistant">应用中心-OpenXLab-小卡的助手</a></p>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>Internlm-06-使用 OpenCompass 对大模型进行评测</title>
    <url>/internlm/internlm-06/</url>
    <content><![CDATA[<h1 id="使用-OpenCompass-对大模型进行评测"><a href="#使用-OpenCompass-对大模型进行评测" class="headerlink" title="使用 OpenCompass 对大模型进行评测"></a>使用 OpenCompass 对大模型进行评测</h1><h2 id="大模型评测概要"><a href="#大模型评测概要" class="headerlink" title="大模型评测概要"></a>大模型评测概要</h2><h3 id="人工智能技术的发展和主要模型的演变"><a href="#人工智能技术的发展和主要模型的演变" class="headerlink" title="人工智能技术的发展和主要模型的演变"></a>人工智能技术的发展和主要模型的演变</h3><ul>
<li><strong>OpenAI GPT系列：</strong><ul>
<li>2018年：发布第一代GPT模型，开启自然语言模型生成式预训练。</li>
<li>随后：发布GPT-2和GPT-3模型。</li>
</ul>
</li>
<li><strong>谷歌的预训练模型：</strong><ul>
<li>探索不同的大规模预训练模型，如T5, Flan等。</li>
</ul>
</li>
<li><strong>OpenAI的ChatGPT和GPT-4：</strong><ul>
<li>2022年11月：发布ChatGPT，展示问答、逻辑推理和内容创作能力。</li>
<li>2023年4月：发布GPT-4，引入多模态能力，拓展语言模型能力。</li>
</ul>
</li>
</ul>
<h3 id="大模型的国际竞争和应用"><a href="#大模型的国际竞争和应用" class="headerlink" title="大模型的国际竞争和应用"></a>大模型的国际竞争和应用</h3><ul>
<li><strong>OpenAI和微软的集成：</strong><ul>
<li>将ChatGPT和GPT-4集成进搜索引擎和Office办公套件，推出New Bing和Office Copilot。</li>
</ul>
</li>
<li><strong>谷歌的Bard：</strong><ul>
<li>基于PaLM和PaLM-2模型，与OpenAI和微软竞争。</li>
</ul>
</li>
<li><strong>中国企业和高校的发展：</strong><ul>
<li>百度、阿里、华为、商汤、讯飞等发布国产大模型。</li>
<li>清华、复旦等高校发布GLM, MOSS等模型。</li>
</ul>
</li>
</ul>
<h3 id="大模型评测的国际和国内进展"><a href="#大模型评测的国际和国内进展" class="headerlink" title="大模型评测的国际和国内进展"></a>大模型评测的国际和国内进展</h3><ul>
<li><strong>国际评测框架和数据集：</strong><ul>
<li>斯坦福大学的HELM评测框架。</li>
<li>纽约大学与谷歌、Meta的SuperGLUE评测集。</li>
<li>加州大学伯克利分校的MMLU测试集。</li>
<li>谷歌的Big-Bench评测集。</li>
</ul>
</li>
<li><strong>中国的评测数据集：</strong><ul>
<li>如CLUE, CUGE等，评测中文语言模型能力。</li>
</ul>
</li>
</ul>
<h3 id="面临的挑战和OpenCompass的提议"><a href="#面临的挑战和OpenCompass的提议" class="headerlink" title="面临的挑战和OpenCompass的提议"></a>面临的挑战和OpenCompass的提议</h3><ul>
<li><strong>当前挑战：</strong><ul>
<li>大模型应用场景广泛，但评测方案往往缺乏系统化。</li>
</ul>
</li>
<li><strong>OpenCompass的提议：</strong><ul>
<li>设计全面、高效、可拓展的评测方案。</li>
<li>提供分布式自动化评测系统，支持全面系统的能力评估。</li>
</ul>
</li>
</ul>
<h1 id="OpenCompass介绍"><a href="#OpenCompass介绍" class="headerlink" title="OpenCompass介绍"></a>OpenCompass介绍</h1><h2 id="评测对象"><a href="#评测对象" class="headerlink" title="评测对象"></a>评测对象</h2><p>本算法库的主要评测对象为语言大模型与多模态大模型。我们以语言大模型为例介绍评测的具体模型类型。</p>
<ul>
<li><p><strong>基座模型</strong>：一般是经过海量的文本数据以自监督学习的方式进行训练获得的模型（如OpenAI的GPT-3，Meta的LLaMA），往往具有强大的文字续写能力。</p>
</li>
<li><p><strong>对话模型</strong>：一般是在的基座模型的基础上，经过指令微调或人类偏好对齐获得的模型（如OpenAI的ChatGPT、上海人工智能实验室的书生·浦语），能理解人类指令，具有较强的对话能力。</p>
</li>
</ul>
<h2 id="工具架构"><a href="#工具架构" class="headerlink" title="工具架构"></a>工具架构</h2><p><img  src="工具架构.png"  ><span class="image-caption">工具架构</span></p>
<h3 id="大模型评测的层级结构"><a href="#大模型评测的层级结构" class="headerlink" title="大模型评测的层级结构"></a>大模型评测的层级结构</h3><ul>
<li><p>模型层</p>
<ul>
<li>重点评测对象：<ul>
<li>基座模型</li>
<li>对话模型</li>
</ul>
</li>
</ul>
</li>
<li><p>能力层</p>
<ul>
<li><p>通用能力：</p>
<ul>
<li>语言</li>
<li>知识</li>
<li>理解</li>
<li>推理</li>
<li>安全</li>
</ul>
</li>
<li><p>特色能力：</p>
<ul>
<li>长文本处理</li>
<li>编码能力</li>
<li>工具使用</li>
<li>知识增强</li>
</ul>
</li>
</ul>
</li>
<li><p>方法层</p>
<ul>
<li><p>客观评测：</p>
<ul>
<li>评估模型在确定答案任务（如选择题、填空、封闭式问答）上的能力。</li>
</ul>
</li>
<li><p>主观评测：</p>
<ul>
<li>评估用户对模型回复的真实满意度。</li>
<li>方法包括：<ul>
<li>基于模型辅助的主观评测</li>
<li>基于人类反馈的主观评测</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>工具层</p>
<ul>
<li>自动化评测支持：<ul>
<li>分布式评测技术</li>
<li>提示词工程</li>
<li>对接评测数据库</li>
<li>评测榜单发布</li>
<li>评测报告生成</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="评测方法"><a href="#评测方法" class="headerlink" title="评测方法"></a>评测方法</h2><h3 id="客观评测"><a href="#客观评测" class="headerlink" title="客观评测"></a>客观评测</h3><h4 id="评测客观问题的方法"><a href="#评测客观问题的方法" class="headerlink" title="评测客观问题的方法"></a>评测客观问题的方法</h4><ul>
<li><strong>定量比较：</strong><ul>
<li>使用定量指标比较模型输出与标准答案的差异。</li>
<li>根据差异结果衡量模型性能。</li>
</ul>
</li>
<li><p><strong>输入输出规范：</strong></p>
<ul>
<li>在评测阶段规范模型的输入和输出。</li>
<li>尽量减少噪声输出，以便更客观地评价模型能力。</li>
</ul>
<h4 id="模型能力的激发与引导"><a href="#模型能力的激发与引导" class="headerlink" title="模型能力的激发与引导"></a>模型能力的激发与引导</h4></li>
<li><p>提示词工程（Prompt Engineering）：</p>
<ul>
<li>使用特定提示词引导模型输出。</li>
</ul>
</li>
<li><p>语境学习（In-Context Learning）：</p>
<ul>
<li>利用上下文环境提升模型的输出质量。</li>
</ul>
<h4 id="客观评测的具体实践"><a href="#客观评测的具体实践" class="headerlink" title="客观评测的具体实践"></a>客观评测的具体实践</h4></li>
<li><p><strong>判别式评测：</strong></p>
<ul>
<li>结合问题和候选答案。</li>
<li>计算困惑度（perplexity），选择困惑度最小的答案。</li>
</ul>
</li>
<li><strong>生成式评测：</strong><ul>
<li>用于生成类任务（如语言翻译、程序生成、逻辑分析）。</li>
<li>使用问题作为输入，留白答案区域由模型补全。</li>
<li>对模型输出进行后处理，确保满足数据集要求。</li>
</ul>
</li>
</ul>
<h3 id="主观评测"><a href="#主观评测" class="headerlink" title="主观评测"></a>主观评测</h3><h4 id="主观评测的重要性"><a href="#主观评测的重要性" class="headerlink" title="主观评测的重要性"></a>主观评测的重要性</h4><ul>
<li>场景和能力多样性：<ul>
<li>语言表达丰富多变，很多场景和能力难以通过客观指标评测。</li>
</ul>
</li>
<li>模型安全和语言能力：<ul>
<li>需要依赖人的主观感受进行评测，以更真实地反映模型能力。</li>
</ul>
</li>
</ul>
<h4 id="OpenCompass的主观评测方案"><a href="#OpenCompass的主观评测方案" class="headerlink" title="OpenCompass的主观评测方案"></a>OpenCompass的主观评测方案</h4><ul>
<li>评测实施：<ul>
<li>使用受试者的主观判断对大语言模型进行评测。</li>
<li>构建主观测试问题集，对比不同模型的回复。</li>
</ul>
</li>
<li>成本与效率：<ul>
<li>高成本的人类主观评测。</li>
<li>结合使用性能优异的大语言模型进行主观打分。</li>
</ul>
</li>
</ul>
<h4 id="主观评测的具体实践"><a href="#主观评测的具体实践" class="headerlink" title="主观评测的具体实践"></a>主观评测的具体实践</h4><ul>
<li>单模型回复满意度统计：<ul>
<li>对单一模型的回复进行满意度评分。</li>
</ul>
</li>
<li>多模型满意度比较：<ul>
<li>比较不同模型回复的满意度。</li>
</ul>
</li>
</ul>
<h1 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h1><p><img  src="opencompass流程.png"  ><span class="image-caption">opencompass 评判流程</span></p>
<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>在 OpenCompass 中评估一个模型通常包括以下几个阶段：<strong>配置</strong> -&gt; <strong>推理</strong> -&gt; <strong>评估</strong> -&gt; <strong>可视化</strong>。</p>
<p><strong>配置</strong>：这是整个工作流的起点。您需要配置整个评估过程，选择要评估的模型和数据集。此外，还可以选择评估策略、计算后端等，并定义显示结果的方式。</p>
<p><strong>推理与评估</strong>：在这个阶段，OpenCompass 将会开始对模型和数据集进行并行推理和评估。<strong>推理</strong>阶段主要是让模型从数据集产生输出，而<strong>评估</strong>阶段则是衡量这些输出与标准答案的匹配程度。这两个过程会被拆分为多个同时运行的“任务”以提高效率，但请注意，如果计算资源有限，这种策略可能会使评测变得更慢。</p>
<p><strong>可视化</strong>：评估完成后，OpenCompass 将结果整理成易读的表格，并将其保存为 CSV 和 TXT 文件。你也可以激活飞书状态上报功能，此后可以在飞书客户端中及时获得评测状态报告。</p>
<p>接下来，我们将展示 OpenCompass 的基础用法，展示书生浦语在 <a href="https://cevalbenchmark.com/index.html#home">C-Eval</a> 基准任务上的评估。它们的配置文件可以在 <a href="https://github.com/open-compass/opencompass/blob/main/configs/eval_demo.py">configs/eval_demo.py</a> 中找到。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="面向GPU的环境安装"><a href="#面向GPU的环境安装" class="headerlink" title="面向GPU的环境安装"></a>面向GPU的环境安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">conda create --name opencompass --<span class="hljs-built_in">clone</span>=/root/share/conda_envs/internlm-base<br>conda activate opencompass<br>git <span class="hljs-built_in">clone</span> https://github.com/open-compass/opencompass<br><span class="hljs-built_in">cd</span> opencompass<br>pip install -e .<br></code></pre></td></tr></table></figure>
<p>有部分第三方功能,如代码能力基准测试 Humaneval 以及 Llama格式的模型评测,可能需要额外步骤才能正常运行，如需评测，详细步骤请参考<a href="https://opencompass.readthedocs.io/zh_CN/latest/get_started/installation.html">安装指南</a>。</p>
<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 解压评测数据集到 data/ 处</span><br><span class="hljs-built_in">cp</span> /share/temp/datasets/OpenCompassData-core-20231110.zip /root/opencompass/<br>unzip OpenCompassData-core-20231110.zip<br><br><span class="hljs-comment"># 将会在opencompass下看到data文件夹</span><br></code></pre></td></tr></table></figure>
<h3 id="查看支持的数据集和模型"><a href="#查看支持的数据集和模型" class="headerlink" title="查看支持的数据集和模型"></a>查看支持的数据集和模型</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 列出所有跟 internlm 及 ceval 相关的配置</span><br>python tools/list_configs.py internlm ceval<br></code></pre></td></tr></table></figure>
<p>将会看到</p>
<figure class="highlight text"><table><tr><td class="code"><pre><code class="hljs text">+--------------------------+--------------------------------------------------------+<br>| Model                    | Config Path                                            |<br>|--------------------------+--------------------------------------------------------|<br>| hf_internlm_20b          | configs/models/hf_internlm/hf_internlm_20b.py          |<br>| hf_internlm_7b           | configs/models/hf_internlm/hf_internlm_7b.py           |<br>| hf_internlm_chat_20b     | configs/models/hf_internlm/hf_internlm_chat_20b.py     |<br>| hf_internlm_chat_7b      | configs/models/hf_internlm/hf_internlm_chat_7b.py      |<br>| hf_internlm_chat_7b_8k   | configs/models/hf_internlm/hf_internlm_chat_7b_8k.py   |<br>| hf_internlm_chat_7b_v1_1 | configs/models/hf_internlm/hf_internlm_chat_7b_v1_1.py |<br>| internlm_7b              | configs/models/internlm/internlm_7b.py                 |<br>| ms_internlm_chat_7b_8k   | configs/models/ms_internlm/ms_internlm_chat_7b_8k.py   |<br>+--------------------------+--------------------------------------------------------+<br>+----------------------------+------------------------------------------------------+<br>| Dataset                    | Config Path                                          |<br>|----------------------------+------------------------------------------------------|<br>| ceval_clean_ppl            | configs/datasets/ceval/ceval_clean_ppl.py            |<br>| ceval_gen                  | configs/datasets/ceval/ceval_gen.py                  |<br>| ceval_gen_2daf24           | configs/datasets/ceval/ceval_gen_2daf24.py           |<br>| ceval_gen_5f30c7           | configs/datasets/ceval/ceval_gen_5f30c7.py           |<br>| ceval_ppl                  | configs/datasets/ceval/ceval_ppl.py                  |<br>| ceval_ppl_578f8d           | configs/datasets/ceval/ceval_ppl_578f8d.py           |<br>| ceval_ppl_93e5ce           | configs/datasets/ceval/ceval_ppl_93e5ce.py           |<br>| ceval_zero_shot_gen_bd40ef | configs/datasets/ceval/ceval_zero_shot_gen_bd40ef.py |<br>+----------------------------+------------------------------------------------------+<br></code></pre></td></tr></table></figure>
<h3 id="启动评测"><a href="#启动评测" class="headerlink" title="启动评测"></a>启动评测</h3><p>确保按照上述步骤正确安装 OpenCompass 并准备好数据集后，可以通过以下命令评测 InternLM-Chat-7B 模型在 C-Eval 数据集上的性能。由于 OpenCompass 默认并行启动评估过程，我们可以在第一次运行时以 <code>--debug</code> 模式启动评估，并检查是否存在问题。在 <code>--debug</code> 模式下，任务将按顺序执行，并实时打印输出。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py --datasets ceval_gen --hf-path /share/temp/model_repos/internlm-chat-7b/ --tokenizer-path /share/temp/model_repos/internlm-chat-7b/ --tokenizer-kwargs padding_side=<span class="hljs-string">&#x27;left&#x27;</span> truncation=<span class="hljs-string">&#x27;left&#x27;</span> trust_remote_code=True --model-kwargs trust_remote_code=True device_map=<span class="hljs-string">&#x27;auto&#x27;</span> --max-seq-len 2048 --max-out-len 16 --batch-size 4 --num-gpus 1 --debug<br></code></pre></td></tr></table></figure>
<p>命令解析<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">--datasets ceval_gen \<br>--hf-path /share/temp/model_repos/internlm-chat-7b/ \  <span class="hljs-comment"># HuggingFace 模型路径</span><br>--tokenizer-path /share/temp/model_repos/internlm-chat-7b/ \  <span class="hljs-comment"># HuggingFace tokenizer 路径（如果与模型路径相同，可以省略）</span><br>--tokenizer-kwargs padding_side=<span class="hljs-string">&#x27;left&#x27;</span> truncation=<span class="hljs-string">&#x27;left&#x27;</span> trust_remote_code=True \  <span class="hljs-comment"># 构建 tokenizer 的参数</span><br>--model-kwargs device_map=<span class="hljs-string">&#x27;auto&#x27;</span> trust_remote_code=True \  <span class="hljs-comment"># 构建模型的参数</span><br>--max-seq-len 2048 \  <span class="hljs-comment"># 模型可以接受的最大序列长度</span><br>--max-out-len 16 \  <span class="hljs-comment"># 生成的最大 token 数</span><br>--batch-size 2  \  <span class="hljs-comment"># 批量大小</span><br>--num-gpus 1  <span class="hljs-comment"># 运行模型所需的 GPU 数量</span><br>--debug<br></code></pre></td></tr></table></figure></p>
<p>如果一切正常，您应该看到屏幕上显示 “Starting inference process”：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[2024-01-12 18:23:55,076] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [INFO] Starting inference process...<br></code></pre></td></tr></table></figure>
<p>评测完成后，将会看到：<br><figure class="highlight maxima"><table><tr><td class="code"><pre><code class="hljs maxima"><br>dataset                                         version    metric         mode      opencompass.models.huggingface.HuggingFace_model_repos_internlm-chat-7b<br>----------------------------------------------  ---------  -------------  ------  -------------------------------------------------------------------------<br>ceval-computer_network                          db9ce2     accuracy       gen                                                                         <span class="hljs-number">31.58</span><br>ceval-operating_system                          1c2571     accuracy       gen                                                                         <span class="hljs-number">36.84</span><br>ceval-computer_architecture                     a74dad     accuracy       gen                                                                         <span class="hljs-number">28.57</span><br>ceval-college_programming                       4ca32a     accuracy       gen                                                                         <span class="hljs-number">32.43</span><br>ceval-college_physics                           963fa8     accuracy       gen                                                                         <span class="hljs-number">26.32</span><br>ceval-college_chemistry                         e78857     accuracy       gen                                                                         <span class="hljs-number">16.67</span><br>ceval-advanced_mathematics                      ce03e2     accuracy       gen                                                                         <span class="hljs-number">21.05</span><br>ceval-probability_and_statistics                <span class="hljs-number">65e812</span>     accuracy       gen                                                                         <span class="hljs-number">38.89</span><br>ceval-discrete_mathematics                      e894ae     accuracy       gen                                                                         <span class="hljs-number">18.75</span><br>ceval-electrical_engineer                       ae42b9     accuracy       gen                                                                         <span class="hljs-number">35.14</span><br>ceval-metrology_engineer                        ee34ea     accuracy       gen                                                                         <span class="hljs-number">50</span><br>ceval-high_school_mathematics                   1dc5bf     accuracy       gen                                                                         <span class="hljs-number">22.22</span><br>ceval-high_school_physics                       adf25f     accuracy       gen                                                                         <span class="hljs-number">31.58</span><br>ceval-high_school_chemistry                     2ed27f     accuracy       gen                                                                         <span class="hljs-number">15.79</span><br>ceval-high_school_biology                       8e2b9a     accuracy       gen                                                                         <span class="hljs-number">36.84</span><br>ceval-middle_school_mathematics                 bee8d5     accuracy       gen                                                                         <span class="hljs-number">26.32</span><br>ceval-middle_school_biology                     86817c     accuracy       gen                                                                         <span class="hljs-number">61.9</span><br>ceval-middle_school_physics                     8accf6     accuracy       gen                                                                         <span class="hljs-number">63.16</span><br>ceval-middle_school_chemistry                   167a15     accuracy       gen                                                                         <span class="hljs-number">60</span><br>ceval-veterinary_medicine                       b4e08d     accuracy       gen                                                                         <span class="hljs-number">47.83</span><br>ceval-college_economics                         f3f4e6     accuracy       gen                                                                         <span class="hljs-number">41.82</span><br>ceval-business_administration                   c1614e     accuracy       gen                                                                         <span class="hljs-number">33.33</span><br>ceval-marxism                                   cf874c     accuracy       gen                                                                         <span class="hljs-number">68.42</span><br>ceval-mao_zedong_thought                        51c7a4     accuracy       gen                                                                         <span class="hljs-number">70.83</span><br>ceval-education_science                         591fee     accuracy       gen                                                                         <span class="hljs-number">58.62</span><br>ceval-teacher_qualification                     4e4ced     accuracy       gen                                                                         <span class="hljs-number">70.45</span><br>ceval-high_school_politics                      5c0de2     accuracy       gen                                                                         <span class="hljs-number">26.32</span><br>ceval-high_school_geography                     <span class="hljs-number">865461</span>     accuracy       gen                                                                         <span class="hljs-number">47.37</span><br>ceval-middle_school_politics                    5be3e7     accuracy       gen                                                                         <span class="hljs-number">52.38</span><br>ceval-middle_school_geography                   8a63be     accuracy       gen                                                                         <span class="hljs-number">58.33</span><br>ceval-modern_chinese_history                    fc01af     accuracy       gen                                                                         <span class="hljs-number">73.91</span><br>ceval-ideological_and_moral_cultivation         a2aa4a     accuracy       gen                                                                         <span class="hljs-number">63.16</span><br>ceval-logic                                     f5b022     accuracy       gen                                                                         <span class="hljs-number">31.82</span><br>ceval-law                                       a110a1     accuracy       gen                                                                         <span class="hljs-number">25</span><br>ceval-chinese_language_and_literature           <span class="hljs-number">0f8b68</span>     accuracy       gen                                                                         <span class="hljs-number">30.43</span><br>ceval-art_studies                               2a1300     accuracy       gen                                                                         <span class="hljs-number">60.61</span><br>ceval-professional_tour_guide                   4e673e     accuracy       gen                                                                         <span class="hljs-number">62.07</span><br>ceval-legal_professional                        ce8787     accuracy       gen                                                                         <span class="hljs-number">39.13</span><br>ceval-high_school_chinese                       <span class="hljs-number">315705</span>     accuracy       gen                                                                         <span class="hljs-number">63.16</span><br>ceval-high_school_history                       7eb30a     accuracy       gen                                                                         <span class="hljs-number">70</span><br>ceval-middle_school_history                     48ab4a     accuracy       gen                                                                         <span class="hljs-number">59.09</span><br>ceval-civil_servant                             87d061     accuracy       gen                                                                         <span class="hljs-number">53.19</span><br>ceval-sports_science                            70f27b     accuracy       gen                                                                         <span class="hljs-number">52.63</span><br>ceval-plant_protection                          8941f9     accuracy       gen                                                                         <span class="hljs-number">59.09</span><br>ceval-basic_medicine                            c409d6     accuracy       gen                                                                         <span class="hljs-number">47.37</span><br>ceval-clinical_medicine                         49e82d     accuracy       gen                                                                         <span class="hljs-number">40.91</span><br>ceval-urban_and_rural_planner                   <span class="hljs-number">95b885</span>     accuracy       gen                                                                         <span class="hljs-number">45.65</span><br>ceval-accountant                                <span class="hljs-number">002837</span>     accuracy       gen                                                                         <span class="hljs-number">26.53</span><br>ceval-fire_engineer                             bc23f5     accuracy       gen                                                                         <span class="hljs-number">22.58</span><br>ceval-environmental_impact_assessment_engineer  c64e2d     accuracy       gen                                                                         <span class="hljs-number">64.52</span><br>ceval-tax_accountant                            3a5e3c     accuracy       gen                                                                         <span class="hljs-number">34.69</span><br>ceval-physician                                 6e277d     accuracy       gen                                                                         <span class="hljs-number">40.82</span><br>ceval-stem                                      -          naive_average  gen                                                                         <span class="hljs-number">35.09</span><br>ceval-social-science                            -          naive_average  gen                                                                         <span class="hljs-number">52.79</span><br>ceval-humanities                                -          naive_average  gen                                                                         <span class="hljs-number">52.58</span><br>ceval-other                                     -          naive_average  gen                                                                         <span class="hljs-number">44.36</span><br>ceval-hard                                      -          naive_average  gen                                                                         <span class="hljs-number">23.91</span><br>ceval                                           -          naive_average  gen                                                                         <span class="hljs-number">44.16</span><br></code></pre></td></tr></table></figure></p>
<p>有关 <code>run.py</code> 支持的所有与 HuggingFace 相关的参数，请阅读 <a href="https://opencompass.readthedocs.io/zh-cn/latest/user_guides/experimentation.html#id2">评测任务发起</a></p>
<p>除了通过命令行配置实验外，OpenCompass 还允许用户在配置文件中编写实验的完整配置，并通过 <code>run.py</code> 直接运行它。配置文件是以 Python 格式组织的，并且必须包括 <code>datasets</code> 和 <code>models</code> 字段。</p>
<p>示例测试配置在 <a href="https://github.com/open-compass/opencompass/blob/main/configs/eval_demo.py">configs/eval_demo.py</a> 中。此配置通过 <a href="../user_guides/config.md#继承机制">继承机制</a> 引入所需的数据集和模型配置，并以所需格式组合 <code>datasets</code> 和 <code>models</code> 字段。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mmengine.config <span class="hljs-keyword">import</span> read_base<br><br><span class="hljs-keyword">with</span> read_base():<br>    <span class="hljs-keyword">from</span> .datasets.siqa.siqa_gen <span class="hljs-keyword">import</span> siqa_datasets<br>    <span class="hljs-keyword">from</span> .datasets.winograd.winograd_ppl <span class="hljs-keyword">import</span> winograd_datasets<br>    <span class="hljs-keyword">from</span> .models.opt.hf_opt_125m <span class="hljs-keyword">import</span> opt125m<br>    <span class="hljs-keyword">from</span> .models.opt.hf_opt_350m <span class="hljs-keyword">import</span> opt350m<br><br>datasets = [*siqa_datasets, *winograd_datasets]<br>models = [opt125m, opt350m]<br></code></pre></td></tr></table></figure>
<p>运行任务时，我们只需将配置文件的路径传递给 <code>run.py</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py configs/eval_demo.py<br></code></pre></td></tr></table></figure>
<p>OpenCompass 提供了一系列预定义的模型配置，位于 <code>configs/models</code> 下。以下是与 <a href="https://github.com/open-compass/opencompass/blob/main/configs/models/opt/hf_opt_350m.py">opt-350m</a>（<code>configs/models/opt/hf_opt_350m.py</code>）相关的配置片段：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用 `HuggingFaceCausalLM` 评估由 HuggingFace 的 `AutoModelForCausalLM` 支持的模型</span><br><span class="hljs-keyword">from</span> opencompass.models <span class="hljs-keyword">import</span> HuggingFaceCausalLM<br><br><span class="hljs-comment"># OPT-350M</span><br>opt350m = <span class="hljs-built_in">dict</span>(<br>       <span class="hljs-built_in">type</span>=HuggingFaceCausalLM,<br>       <span class="hljs-comment"># `HuggingFaceCausalLM` 的初始化参数</span><br>       path=<span class="hljs-string">&#x27;facebook/opt-350m&#x27;</span>,<br>       tokenizer_path=<span class="hljs-string">&#x27;facebook/opt-350m&#x27;</span>,<br>       tokenizer_kwargs=<span class="hljs-built_in">dict</span>(<br>           padding_side=<span class="hljs-string">&#x27;left&#x27;</span>,<br>           truncation_side=<span class="hljs-string">&#x27;left&#x27;</span>,<br>           proxies=<span class="hljs-literal">None</span>,<br>           trust_remote_code=<span class="hljs-literal">True</span>),<br>       model_kwargs=<span class="hljs-built_in">dict</span>(device_map=<span class="hljs-string">&#x27;auto&#x27;</span>),<br>       <span class="hljs-comment"># 下面是所有模型的共同参数，不特定于 HuggingFaceCausalLM</span><br>       abbr=<span class="hljs-string">&#x27;opt350m&#x27;</span>,               <span class="hljs-comment"># 结果显示的模型缩写</span><br>       max_seq_len=<span class="hljs-number">2048</span>,             <span class="hljs-comment"># 整个序列的最大长度</span><br>       max_out_len=<span class="hljs-number">100</span>,              <span class="hljs-comment"># 生成的最大 token 数</span><br>       batch_size=<span class="hljs-number">64</span>,                <span class="hljs-comment"># 批量大小</span><br>       run_cfg=<span class="hljs-built_in">dict</span>(num_gpus=<span class="hljs-number">1</span>),     <span class="hljs-comment"># 该模型所需的 GPU 数量</span><br>    )<br></code></pre></td></tr></table></figure>
<p>使用配置时，我们可以通过命令行参数 <code>--models</code> 指定相关文件，或使用继承机制将模型配置导入到配置文件中的 <code>models</code> 列表中。</p>
<p>与模型类似，数据集的配置文件也提供在 <code>configs/datasets</code> 下。用户可以在命令行中使用 <code>--datasets</code>，或通过继承在配置文件中导入相关配置</p>
<p>下面是来自 <code>configs/eval_demo.py</code> 的与数据集相关的配置片段：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mmengine.config <span class="hljs-keyword">import</span> read_base  <span class="hljs-comment"># 使用 mmengine.read_base() 读取基本配置</span><br><br><span class="hljs-keyword">with</span> read_base():<br>    <span class="hljs-comment"># 直接从预设的数据集配置中读取所需的数据集配置</span><br>    <span class="hljs-keyword">from</span> .datasets.winograd.winograd_ppl <span class="hljs-keyword">import</span> winograd_datasets  <span class="hljs-comment"># 读取 Winograd 配置，基于 PPL（困惑度）进行评估</span><br>    <span class="hljs-keyword">from</span> .datasets.siqa.siqa_gen <span class="hljs-keyword">import</span> siqa_datasets  <span class="hljs-comment"># 读取 SIQA 配置，基于生成进行评估</span><br><br>datasets = [*siqa_datasets, *winograd_datasets]       <span class="hljs-comment"># 最终的配置需要包含所需的评估数据集列表 &#x27;datasets&#x27;</span><br></code></pre></td></tr></table></figure>
<p>数据集配置通常有两种类型：’ppl’ 和 ‘gen’，分别指示使用的评估方法。其中 <code>ppl</code> 表示辨别性评估，<code>gen</code> 表示生成性评估。</p>
<p>此外，<a href="https://github.com/open-compass/opencompass/blob/main/configs/datasets/collections">configs/datasets/collections</a> 收录了各种数据集集合，方便进行综合评估。OpenCompass 通常使用 <a href="https://github.com/open-compass/opencompass/blob/main/configs/datasets/collections/base_medium.py"><code>base_medium.py</code></a> 进行全面的模型测试。要复制结果，只需导入该文件，例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py --models hf_llama_7b --datasets base_medium<br></code></pre></td></tr></table></figure>
<p>OpenCompass 通常假定运行环境网络是可用的。如果您遇到网络问题或希望在离线环境中运行 OpenCompass，请参阅 <a href="https://opencompass.readthedocs.io/zh-cn/latest/get_started/faq.html">FAQ - 网络 - Q1</a> 寻求解决方案。</p>
<h2 id="可视化评估结果"><a href="#可视化评估结果" class="headerlink" title="可视化评估结果"></a>可视化评估结果</h2><p>评估完成后，评估结果表格将打印如下：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><code class="hljs text">dataset    version    metric    mode      opt350m    opt125m<br>---------  ---------  --------  ------  ---------  ---------<br>siqa       e78df3     accuracy  gen         21.55      12.44<br>winograd   b6c7ed     accuracy  ppl         51.23      49.82<br></code></pre></td></tr></table></figure>
<p>所有运行输出将定向到 <code>outputs/demo/</code> 目录，结构如下：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><code class="hljs text">outputs/default/<br>├── 20200220_120000<br>├── 20230220_183030     # 每个实验一个文件夹<br>│   ├── configs         # 用于记录的已转储的配置文件。如果在同一个实验文件夹中重新运行了不同的实验，可能会保留多个配置<br>│   ├── logs            # 推理和评估阶段的日志文件<br>│   │   ├── eval<br>│   │   └── infer<br>│   ├── predictions   # 每个任务的推理结果<br>│   ├── results       # 每个任务的评估结果<br>│   └── summary       # 单个实验的汇总评估结果<br>├── ...<br></code></pre></td></tr></table></figure>
<p>打印评测结果的过程可被进一步定制化，用于输出一些数据集的平均分 (例如 MMLU, C-Eval 等)。</p>
<p>关于评测结果输出的更多介绍可阅读 <a href="../user_guides/summarizer.md">结果展示</a>。</p>
<h2 id="更多教程"><a href="#更多教程" class="headerlink" title="更多教程"></a>更多教程</h2><p>想要更多了解 OpenCompass, 可以点击下列链接学习。</p>
<ul>
<li><a href="https://opencompass.readthedocs.io/zh-cn/latest/">https://opencompass.readthedocs.io/zh-cn/latest/</a></li>
</ul>
<h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><h3 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建环境</span><br>conda create --name opencompass --<span class="hljs-built_in">clone</span>=/root/share/conda_envs/internlm-base<br>conda activate opencompass<br><span class="hljs-comment"># 使用镜像 clone</span><br>git <span class="hljs-built_in">clone</span> https://mirror.ghproxy.com/https://github.com/open-compass/opencompass<br><span class="hljs-built_in">cd</span> opencompass<br>pip install -e .<br></code></pre></td></tr></table></figure>
<h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> /share/temp/datasets/OpenCompassData-core-20231110.zip /root/opencompass/<br><span class="hljs-built_in">cd</span> /root/opencompass/<br>unzip OpenCompassData-core-20231110.zip<br></code></pre></td></tr></table></figure>
<p><img  src="unzip.png"  ><span class="image-caption">解压数据</span></p>
<h3 id="查看支持的数据集和模型-1"><a href="#查看支持的数据集和模型-1" class="headerlink" title="查看支持的数据集和模型"></a>查看支持的数据集和模型</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python tools/list_configs.py internlm ceval<br></code></pre></td></tr></table></figure>
<p><img  src="list.png"  ><span class="image-caption">列出所有跟 internlm 及 ceval 相关的配置</span></p>
<h3 id="启动评测-1"><a href="#启动评测-1" class="headerlink" title="启动评测"></a>启动评测</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py \<br>--datasets ceval_gen \<br>--hf-path /share/model_repos/internlm2-chat-7b/ \  <span class="hljs-comment"># HuggingFace 模型路径</span><br>--tokenizer-path /share/model_repos/internlm2-chat-7b/ \  <span class="hljs-comment"># 注意这里是 internlm2</span><br>--tokenizer-kwargs padding_side=<span class="hljs-string">&#x27;left&#x27;</span> truncation=<span class="hljs-string">&#x27;left&#x27;</span> trust_remote_code=True \  <span class="hljs-comment"># 构建 tokenizer 的参数</span><br>--model-kwargs device_map=<span class="hljs-string">&#x27;auto&#x27;</span> trust_remote_code=True \  <span class="hljs-comment"># 构建模型的参数</span><br>--max-seq-len 2048 \  <span class="hljs-comment"># 模型可以接受的最大序列长度</span><br>--max-out-len 16 \  <span class="hljs-comment"># 生成的最大 token 数</span><br>--batch-size 2  \  <span class="hljs-comment"># 批量大小</span><br>--num-gpus 1 \ <span class="hljs-comment"># 运行模型所需的 GPU 数量</span><br>--debug<br></code></pre></td></tr></table></figure>
<p>便于复制版：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py \<br>--datasets ceval_gen \<br>--hf-path /share/model_repos/internlm2-chat-7b/ \<br>--tokenizer-path /share/model_repos/internlm2-chat-7b/ \<br>--tokenizer-kwargs padding_side=<span class="hljs-string">&#x27;left&#x27;</span> truncation=<span class="hljs-string">&#x27;left&#x27;</span> trust_remote_code=True \<br>--model-kwargs device_map=<span class="hljs-string">&#x27;auto&#x27;</span> trust_remote_code=True \<br>--max-seq-len 2048 \<br>--max-out-len 16 \<br>--batch-size 2  \<br>--num-gpus 1 \<br>--debug<br></code></pre></td></tr></table></figure>
<p>发现显存不够用，尝试改小 batch size 为 1。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py \<br>--datasets ceval_gen \<br>--hf-path /share/model_repos/internlm2-chat-7b/ \<br>--tokenizer-path /share/model_repos/internlm2-chat-7b/ \<br>--tokenizer-kwargs padding_side=<span class="hljs-string">&#x27;left&#x27;</span> truncation=<span class="hljs-string">&#x27;left&#x27;</span> trust_remote_code=True \<br>--model-kwargs device_map=<span class="hljs-string">&#x27;auto&#x27;</span> trust_remote_code=True \<br>--max-seq-len 2048 \<br>--max-out-len 16 \<br>--batch-size 1  \<br>--num-gpus 1 \<br>--debug<br></code></pre></td></tr></table></figure>
<p><img  src="run.png"  ><span class="image-caption">运行截图</span></p>
<p><img  src="result.png"  ><span class="image-caption">评测结果</span></p>
<h2 id="进阶作业"><a href="#进阶作业" class="headerlink" title="进阶作业"></a>进阶作业</h2><p>安装 lmdeploy，这一步是必须的，否则无法加载 TurboMind 模型</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install lmdeploy==0.2.0<br></code></pre></td></tr></table></figure>
<p>编写 config 文件如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mmengine.config <span class="hljs-keyword">import</span> read_base<br><span class="hljs-keyword">from</span> opencompass.models.turbomind <span class="hljs-keyword">import</span> TurboMindModel<br><br><span class="hljs-keyword">with</span> read_base():<br>    <span class="hljs-comment"># choose a list of datasets</span><br>    <span class="hljs-keyword">from</span> .datasets.ceval.ceval_gen_5f30c7 <span class="hljs-keyword">import</span> ceval_datasets<br><br><br>datasets = [*ceval_datasets]<br><br>internlm_meta_template = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">round</span>=[<br>    <span class="hljs-built_in">dict</span>(role=<span class="hljs-string">&#x27;HUMAN&#x27;</span>, begin=<span class="hljs-string">&#x27;&lt;|User|&gt;:&#x27;</span>, end=<span class="hljs-string">&#x27;\n&#x27;</span>),<br>    <span class="hljs-built_in">dict</span>(role=<span class="hljs-string">&#x27;BOT&#x27;</span>, begin=<span class="hljs-string">&#x27;&lt;|Bot|&gt;:&#x27;</span>, end=<span class="hljs-string">&#x27;&lt;eoa&gt;\n&#x27;</span>, generate=<span class="hljs-literal">True</span>),<br>],<br>                              eos_token_id=<span class="hljs-number">103028</span>)<br><br><span class="hljs-comment"># config for internlm-chat-7b</span><br>internlm_chat_7b = <span class="hljs-built_in">dict</span>(<br>    <span class="hljs-built_in">type</span>=TurboMindModel,<br>    abbr=<span class="hljs-string">&#x27;internlm-chat-7b&#x27;</span>,<br>    path=<span class="hljs-string">&#x27;/root/workspace_quant_awq4&#x27;</span>, <span class="hljs-comment"># 这里的 path 是上一节课中的 awq 模型</span><br>    engine_config=<span class="hljs-built_in">dict</span>(session_len=<span class="hljs-number">2048</span>,<br>                       max_batch_size=<span class="hljs-number">32</span>,<br>                       rope_scaling_factor=<span class="hljs-number">1.0</span>),<br>    gen_config=<span class="hljs-built_in">dict</span>(top_k=<span class="hljs-number">1</span>,<br>                    top_p=<span class="hljs-number">0.8</span>,<br>                    temperature=<span class="hljs-number">1.0</span>,<br>                    max_new_tokens=<span class="hljs-number">100</span>),<br>    max_out_len=<span class="hljs-number">100</span>,<br>    max_seq_len=<span class="hljs-number">1024</span>,<br>    batch_size=<span class="hljs-number">2</span>,<br>    concurrency=<span class="hljs-number">32</span>,<br>    meta_template=internlm_meta_template,<br>    run_cfg=<span class="hljs-built_in">dict</span>(num_gpus=<span class="hljs-number">1</span>, num_procs=<span class="hljs-number">1</span>),<br>)<br><br>models = [internlm_chat_7b]<br></code></pre></td></tr></table></figure>
<p>运行评测：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py configs/eval_internlm_my_deploy.py --debug<br></code></pre></td></tr></table></figure>
<p><img  src="awq.png"  ><span class="image-caption">加载量化后的模型</span></p>
<p><img  src="result-internlm-awq.png"  ><span class="image-caption">评判 internlm-awq</span></p>
<p>可见 internlm-AWQ 在 ceval 上的得分并不如 internlm2。</p>
<h3 id="使用-lmdeploy-0-2-0-转换-internlm2-为-awq-模型并进行评测"><a href="#使用-lmdeploy-0-2-0-转换-internlm2-为-awq-模型并进行评测" class="headerlink" title="使用 lmdeploy 0.2.0 转换 internlm2 为 awq 模型并进行评测"></a>使用 lmdeploy 0.2.0 转换 internlm2 为 awq 模型并进行评测</h3><p>使用 lmdeploy 0.2 的时候与 0.1 版本进行 AWQ 量化的方式略有不同，同时要从 huggingface 上下载测试数据集，所以国内可以使用镜像：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> HF_ENDPOINT=https://hf-mirror.com<br>lmdeploy lite auto_awq /root/share/model_repos/internlm2-chat-7b  --work-dir internlm2-chat-7b-4bit<br></code></pre></td></tr></table></figure>
<p>之后对模型进行转化：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy convert  internlm2-chat-7b ./internlm2-chat-7b-4bit/ --model-format awq --group-size 128  --dst-path  ./workspace_awq_internlm2<br></code></pre></td></tr></table></figure>
<p><img  src="convert.png"  ><span class="image-caption">转换模型</span></p>
<p>之后编写新的 config.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mmengine.config <span class="hljs-keyword">import</span> read_base<br><span class="hljs-keyword">from</span> opencompass.models.turbomind <span class="hljs-keyword">import</span> TurboMindModel<br><br><span class="hljs-keyword">with</span> read_base():<br>    <span class="hljs-comment"># choose a list of datasets</span><br>    <span class="hljs-keyword">from</span> .datasets.ceval.ceval_gen_5f30c7 <span class="hljs-keyword">import</span> ceval_datasets<br><br><br>datasets = [*ceval_datasets]<br><br>internlm_meta_template = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">round</span>=[<br>    <span class="hljs-built_in">dict</span>(role=<span class="hljs-string">&#x27;HUMAN&#x27;</span>, begin=<span class="hljs-string">&#x27;&lt;|User|&gt;:&#x27;</span>, end=<span class="hljs-string">&#x27;\n&#x27;</span>),<br>    <span class="hljs-built_in">dict</span>(role=<span class="hljs-string">&#x27;BOT&#x27;</span>, begin=<span class="hljs-string">&#x27;&lt;|Bot|&gt;:&#x27;</span>, end=<span class="hljs-string">&#x27;&lt;eoa&gt;\n&#x27;</span>, generate=<span class="hljs-literal">True</span>),<br>],<br>                              eos_token_id=<span class="hljs-number">103028</span>)<br><br><span class="hljs-comment"># config for internlm2-chat-7b-awq</span><br>internlm2_chat_7b = <span class="hljs-built_in">dict</span>(<br>    <span class="hljs-built_in">type</span>=TurboMindModel,<br>    abbr=<span class="hljs-string">&#x27;internlm-chat-7b&#x27;</span>,<br>    path=<span class="hljs-string">&#x27;/root/workspace_awq_internlm2&#x27;</span>,<br>    engine_config=<span class="hljs-built_in">dict</span>(session_len=<span class="hljs-number">2048</span>,<br>                       max_batch_size=<span class="hljs-number">32</span>,<br>                       rope_scaling_factor=<span class="hljs-number">1.0</span>),<br>    gen_config=<span class="hljs-built_in">dict</span>(top_k=<span class="hljs-number">1</span>,<br>                    top_p=<span class="hljs-number">0.8</span>,<br>                    temperature=<span class="hljs-number">1.0</span>,<br>                    max_new_tokens=<span class="hljs-number">100</span>),<br>    max_out_len=<span class="hljs-number">100</span>,<br>    max_seq_len=<span class="hljs-number">1024</span>,<br>    batch_size=<span class="hljs-number">2</span>,<br>    concurrency=<span class="hljs-number">32</span>,<br>    meta_template=internlm_meta_template,<br>    run_cfg=<span class="hljs-built_in">dict</span>(num_gpus=<span class="hljs-number">1</span>, num_procs=<span class="hljs-number">1</span>),<br>)<br><br>models = [internlm2_chat_7b]<br></code></pre></td></tr></table></figure>
<p>进行评测：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py configs/eval_internlm2_my_deploy.py --debug<br></code></pre></td></tr></table></figure>
<p><img  src="awq-result.png"  ><span class="image-caption">AWQ 量化评测</span></p>
<p>能够发现 AWQ 量化后的模型在 ceval 数据集上的得分比原模型要好。精度不仅没有明显下降，相反在不少任务上还有一定的提升。可能得原因是，量化会导致一定的误差，有时候这种误差可能会减少模型对训练数据的拟合，从而提高泛化性能。量化可以被视为引入轻微噪声的正则化方法。或者，也有可能量化后的模型正好对某些数据集具有更好的性能。</p>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>算法整理</title>
    <url>/uncategorized/leetcode/</url>
    <content><![CDATA[<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><h2 id="rust-中-dbg-超时"><a href="#rust-中-dbg-超时" class="headerlink" title="rust 中 dbg! 超时"></a><code>rust</code> 中 <code>dbg!</code> 超时</h2><p>在 <code>rust</code> 中使用 <code>dbg!</code> 的时候，在题目判定时，可能会因为 <code>dbg!</code> 超时，提交代码的时候要去掉 <code>dbg!</code></p>
<h1 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h1><h2 id="双指针"><a href="#双指针" class="headerlink" title="双指针"></a>双指针</h2><p>第 27、977 题就是经典的双指针题目。</p>
<h2 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h2><p>注意，使用滑动窗口的时候，for 循环代表滑动窗口的结尾，否则又会陷入两个 for 的困境。</p>
<h1 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h1><p>链表对于有插入、交换或者删除的操作的时候，一般加一个虚拟头节点更好处理。</p>
<h2 id="双指针-1"><a href="#双指针-1" class="headerlink" title="双指针"></a>双指针</h2><p>经典的一个 pre 指针，一个 cur 指针：可以解决反转链表、交换节点等问题。<br>还有一个 fast 指针，一个 slow 指针：可以解决删除第 n 个元素的问题。</p>
<h1 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h1><h2 id="最长公共子串"><a href="#最长公共子串" class="headerlink" title="最长公共子串"></a>最长公共子串</h2><p>状态转移方程如下：</p>
<script type="math/tex; mode=display">
d p[i][j]=\left\{\begin{array}{l}
d p[i-1][j-1]+1, \text { 当且仅当 } x[i]=y[j] \\
0, \text { 当 } x[i] \ne y[j]
\end{array}\right.</script><p>按照上面方程实现的算法时间复杂度为 $O(n^2)$，空间复杂度为 $O(n^2)$。</p>
<p><img  src="../leetcode/d6f0b0e17ed6e13f5c042d172b1ddca782cb6aba589f5fcfea8944831614502f-image.png"  ><span class="image-caption">image.png</span></p>
<p>注意到，更新 $dp[i][j]$ 只需要上一列，即 $dp[i-1]$ 列，所以可以将空间复杂度降低为 $O(n)$，但是需要注意因为使用的是相同的数组列，所以字符串不相等时需要设置 $dp[j] = 0$，同时要注意从后向前更新数组，因为如果从前向后更新，那么当前的 $dp[j]$ 使用的是当前列刚刚更新过的数据，而我们需要的是上一列的数据，所以可以从后向前更新数据避免这个问题。</p>
<p>rust 代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">dp</span> = <span class="hljs-built_in">vec!</span>[<span class="hljs-number">0</span>; s2.<span class="hljs-title function_ invoke__">len</span>()];<br><span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..s1.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>    <span class="hljs-comment">// 逆序迭代是因为更新a[i][j]需要a[i-1][j-1]</span><br>    <span class="hljs-comment">// 现在是一个数组，所以 a[j] 是原来的 a[i][j]，而我们需要的是 a[i-1][j]</span><br>    <span class="hljs-comment">// 所以从后向前迭代，a[j] 是原来的 a[i-1][j]</span><br>    <span class="hljs-keyword">for</span> <span class="hljs-variable">j</span> <span class="hljs-keyword">in</span> (<span class="hljs-number">0</span>..s2.<span class="hljs-title function_ invoke__">len</span>()).<span class="hljs-title function_ invoke__">s2</span>() &#123;<br>        <span class="hljs-keyword">if</span> s[i] == s2[j] &#123;<br>            <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> || j == <span class="hljs-number">0</span> &#123;<br>                dp[j] = <span class="hljs-number">1</span>;<br>            &#125; <span class="hljs-keyword">else</span> &#123;<br>                dp[j] = dp[j - <span class="hljs-number">1</span>] + <span class="hljs-number">1</span>;<br>            &#125;<br>            <span class="hljs-keyword">if</span> dp[j] &gt; max_len &#123;<br>                <span class="hljs-keyword">let</span> <span class="hljs-variable">before_s2</span> = s2.<span class="hljs-title function_ invoke__">len</span>() - <span class="hljs-number">1</span> - j;<br>                <span class="hljs-keyword">if</span> before_s2 + dp[j] - <span class="hljs-number">1</span> == i &#123;<br>                    max_len = dp[j];<br>                    max_end = i;<br>                &#125;<br>            &#125;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">// 与之前不同，之前使用的是不同的列，所以不需要置0</span><br>            dp[j] = <span class="hljs-number">0</span>;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h2 id="最长回文子串"><a href="#最长回文子串" class="headerlink" title="最长回文子串"></a>最长回文子串</h2><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>将字符串倒置之后求最长公共子串（状态转移方程与最长公共子串相同），并判断是否为回文子串，这里回文子串「由倒置字符串推出的原字符串末尾下标」与「i」应该相等。</p>
<p>代码中 <code>longest_palindrome1</code> 的求最长公共子串空间复杂度为 $O(n^2)$，<code>longest_palindrome2</code> 的求最长公共子串空间复杂度为 $O(n)$。</p>
<p>代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Solution</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">longest_palindrome1</span>(s: <span class="hljs-type">String</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">String</span> &#123;<br>        <span class="hljs-keyword">if</span> s.<span class="hljs-title function_ invoke__">len</span>() &lt;= <span class="hljs-number">1</span> &#123;<br>            <span class="hljs-keyword">return</span> s;<br>        &#125;<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">rev</span>: <span class="hljs-type">String</span> = s.<span class="hljs-title function_ invoke__">chars</span>().<span class="hljs-title function_ invoke__">rev</span>().<span class="hljs-title function_ invoke__">collect</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">rev</span> = rev.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = s.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">dp</span> = <span class="hljs-built_in">vec!</span>[<span class="hljs-built_in">vec!</span>[<span class="hljs-number">0</span>; rev.<span class="hljs-title function_ invoke__">len</span>()]; s.<span class="hljs-title function_ invoke__">len</span>()];<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">max_len</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">max_end</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..s.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>            <span class="hljs-keyword">for</span> <span class="hljs-variable">j</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..rev.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>                <span class="hljs-keyword">if</span> s[i] == rev[j] &#123;<br>                    <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> || j == <span class="hljs-number">0</span> &#123;<br>                        dp[i][j] = <span class="hljs-number">1</span>;<br>                    &#125; <span class="hljs-keyword">else</span> &#123;<br>                        dp[i][j] = dp[i - <span class="hljs-number">1</span>][j - <span class="hljs-number">1</span>] + <span class="hljs-number">1</span>;<br>                    &#125;<br>                &#125;<br>                <span class="hljs-keyword">if</span> dp[i][j] &gt; max_len &#123;<br>                    <span class="hljs-comment">// 如果是回文串，那么「由倒置字符串推出的原字符串末尾下标」与「i」应该相等</span><br>                    <span class="hljs-comment">// 其中，倒置字符串的 rev.len() - 1 - j，也就是倒置之前的开始下标，减一是因为长度比下标多一</span><br>                    <span class="hljs-comment">// 再加上 dp[i][j] - 1，就是原字符串的末尾下标。abc，a的下标为0，长度为3，0+3为3，但是最大下标为2，所以需要减一</span><br>                    <span class="hljs-keyword">let</span> <span class="hljs-variable">before_rev</span> = rev.<span class="hljs-title function_ invoke__">len</span>() - <span class="hljs-number">1</span> - j;<br>                    <span class="hljs-keyword">if</span> before_rev + dp[i][j] - <span class="hljs-number">1</span> == i &#123;<br>                        max_len = dp[i][j];<br>                        max_end = i;<br>                    &#125;<br>                &#125;<br>            &#125;<br>        &#125;<br>        std::<span class="hljs-type">str</span>::<span class="hljs-title function_ invoke__">from_utf8</span>(&amp;s[max_end + <span class="hljs-number">1</span> - max_len..max_end + <span class="hljs-number">1</span>])<br>            .<span class="hljs-title function_ invoke__">unwrap</span>()<br>            .<span class="hljs-title function_ invoke__">to_string</span>()<br>    &#125;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">longest_palindrome2</span>(s: <span class="hljs-type">String</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">String</span> &#123;<br>        <span class="hljs-keyword">if</span> s.<span class="hljs-title function_ invoke__">len</span>() &lt; <span class="hljs-number">1</span> &#123;<br>            <span class="hljs-keyword">return</span> s;<br>        &#125;<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">rev</span>: <span class="hljs-type">String</span> = s.<span class="hljs-title function_ invoke__">chars</span>().<span class="hljs-title function_ invoke__">rev</span>().<span class="hljs-title function_ invoke__">collect</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = s.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">rev</span> = rev.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">max_len</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">max_end</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">dp</span> = <span class="hljs-built_in">vec!</span>[<span class="hljs-number">0</span>; rev.<span class="hljs-title function_ invoke__">len</span>()];<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..s.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>            <span class="hljs-comment">// 逆序迭代是因为更新a[i][j]需要a[i-1][j-1]</span><br>            <span class="hljs-comment">// 现在是一个数组，所以 a[j] 是原来的 a[i][j]，而我们需要的是 a[i-1][j]</span><br>            <span class="hljs-comment">// 所以从后向前迭代，a[j] 是原来的 a[i-1][j]</span><br>            <span class="hljs-keyword">for</span> <span class="hljs-variable">j</span> <span class="hljs-keyword">in</span> (<span class="hljs-number">0</span>..rev.<span class="hljs-title function_ invoke__">len</span>()).<span class="hljs-title function_ invoke__">rev</span>() &#123;<br>                <span class="hljs-keyword">if</span> s[i] == rev[j] &#123;<br>                    <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> || j == <span class="hljs-number">0</span> &#123;<br>                        dp[j] = <span class="hljs-number">1</span>;<br>                    &#125; <span class="hljs-keyword">else</span> &#123;<br>                        dp[j] = dp[j - <span class="hljs-number">1</span>] + <span class="hljs-number">1</span>;<br>                    &#125;<br>                    <span class="hljs-keyword">if</span> dp[j] &gt; max_len &#123;<br>                        <span class="hljs-keyword">let</span> <span class="hljs-variable">before_rev</span> = rev.<span class="hljs-title function_ invoke__">len</span>() - <span class="hljs-number">1</span> - j;<br>                        <span class="hljs-keyword">if</span> before_rev + dp[j] - <span class="hljs-number">1</span> == i &#123;<br>                            max_len = dp[j];<br>                            max_end = i;<br>                        &#125;<br>                    &#125;<br>                &#125; <span class="hljs-keyword">else</span> &#123;<br>                    <span class="hljs-comment">// 与之前不同，之前使用的是不同的列，所以不需要置0</span><br>                    dp[j] = <span class="hljs-number">0</span>;<br>                &#125;<br>            &#125;<br>        &#125;<br>        std::<span class="hljs-type">str</span>::<span class="hljs-title function_ invoke__">from_utf8</span>(&amp;s[max_end + <span class="hljs-number">1</span> - max_len..max_end + <span class="hljs-number">1</span>])<br>            .<span class="hljs-title function_ invoke__">unwrap</span>()<br>            .<span class="hljs-title function_ invoke__">to_string</span>()<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h3 id="中心拓展算法"><a href="#中心拓展算法" class="headerlink" title="中心拓展算法"></a>中心拓展算法</h3><p>为了避免在之后的叙述中出现歧义，这里我们指出什么是“朴素算法”。</p>
<p>该算法通过下述方式工作：对每个中心位置 $i$ 在比较一对对应字符后，只要可能，该算法便尝试将答案加 $1$。</p>
<p>该算法是比较慢的：它只能在 $O(n^2)$ 的时间内计算答案。</p>
<p>该算法的实现如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// C++ Version</span><br><span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">d1</span><span class="hljs-params">(n)</span>, <span class="hljs-title">d2</span><span class="hljs-params">(n)</span></span>;<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) &#123;<br>  d1[i] = <span class="hljs-number">1</span>;<br>  <span class="hljs-keyword">while</span> (<span class="hljs-number">0</span> &lt;= i - d1[i] &amp;&amp; i + d1[i] &lt; n &amp;&amp; s[i - d1[i]] == s[i + d1[i]]) &#123;<br>    d1[i]++;<br>  &#125;<br><br>  d2[i] = <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">while</span> (<span class="hljs-number">0</span> &lt;= i - d2[i] - <span class="hljs-number">1</span> &amp;&amp; i + d2[i] &lt; n &amp;&amp;<br>         s[i - d2[i] - <span class="hljs-number">1</span>] == s[i + d2[i]]) &#123;<br>    d2[i]++;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Python Version</span><br>d1 = [<span class="hljs-number">0</span>] * n<br>d2 = [<span class="hljs-number">0</span>] * n<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n):<br>    d1[i] = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-number">0</span> &lt;= i - d1[i] <span class="hljs-keyword">and</span> i + d1[i] &lt; n <span class="hljs-keyword">and</span> s[i - d1[i]] == s[i + d1[i]]:<br>        d1[i] += <span class="hljs-number">1</span><br><br>    d2[i] = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-number">0</span> &lt;= i - d2[i] - <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> i + d2[i] &lt; n <span class="hljs-keyword">and</span> s[i - d2[i] - <span class="hljs-number">1</span>] == s[i + d2[i]]:<br>        d2[i] += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure>
<h3 id="Manacher-算法12"><a href="#Manacher-算法12" class="headerlink" title="Manacher 算法12"></a>Manacher 算法<sup><a href="#fn_1" id="reffn_1">1</a></sup><sup><a href="#fn_2" id="reffn_2">2</a></sup></h3><p>Manacher 算法是对中心拓展算法的优化，为了快速计算，我们维护已找到的最靠右的子回文串的 <strong>边界 $(l, r)$</strong>（即具有最大 $r$ 值的回文串，其中 $l$ 和 $r$ 分别为该回文串左右边界的位置）。初始时，我们置 $l = 0$ 和 $r = -1$（<em>-1</em>需区别于倒序索引位置，这里可为任意负数，仅为了循环初始时方便）。</p>
<p>现在假设我们要对下一个 $i$ 计算 $P[i]$，而之前所有 $P[]$ 中的值已计算完毕。我们将通过下列方式计算：</p>
<ul>
<li><p>如果 $i$ 位于当前子回文串之外，即 $i &gt; r$，那么我们调用朴素算法。</p>
<p>因此我们将连续地增加 $d_1[i]$，同时在每一步中检查当前的子串 $[i - P[i] \dots i +  P[i]]$（$P[i]$ 表示半径长度，下同）是否为一个回文串。如果我们找到了第一处对应字符不同，又或者碰到了 $s$  的边界，则算法停止。在两种情况下我们均已计算完 $P[i]$。此后，仍需记得更新 $(l, r)$。</p>
</li>
<li><p>现在考虑 $i \le r$ 的情况。我们将尝试从已计算过的 $P[]$ 的值中获取一些信息。首先在子回文串  $(l, r)$ 中反转位置 $i$，即我们得到 $j = l + (r - i)$。现在来考察值 $P[j]$。因为位置 $j$ 同位置  $i$ 对称，我们 <strong>几乎总是</strong> 可以置 $P[i] = P[j]$。</p>
<p>存在 <strong>棘手的情况</strong>，主要有以下：</p>
<ul>
<li><p>超出了 $r$</p>
<p><img  src="../leetcode/b0d52a5f30747e55ef09b3c7b7cfc23026e37040edc41f387263e8f8a0ba8f49-image.png"  ><span class="image-caption">图转自 LeetCode</span></p>
<p>当我们要求 $P [ i ]$ 的时候，$P [mirror] = 7$，而此时 $P [ i ]$ 并不等于 $7$，为什么呢，因为我们从 $i$ 开始往后数 $7$ 个，等于 $22$，已经超过了最右的 $r$，此时不能利用对称性了，但我们一定可以扩展到 $r$ 的，所以 $P [ i ]$ 至少等于 $r - i = 20 - 15 = 5$，会不会更大呢，我们只需要比较 $T [ r+1 ]$ 和 $T [ r+1 ]$ 关于 $i$ 的对称点就行了，就像中心扩展法一样一个个扩展。</p>
</li>
<li><p>$P[i]$ 遇到了原字符串的左边界</p>
<p><img  src="../leetcode/714e6f768e67304fb7162ecac3ae85fcf23ad82a21456e8ca55ac2c8cfd2609e-image.png"  ><span class="image-caption">image.png</span></p>
<p>此时$P [ i_{mirror} ] = 1$，但是 $P [ i ]$ 赋值成 1 是不正确的，出现这种情况的原因是 $P [ i_{mirror} ]$ 在扩展的时候首先是 “#” == “#”，之后遇到了 “^” 和另一个字符比较，也就是到了边界，才终止循环的。而 $P [ i ]$ 并没有遇到边界，所以我们可以继续通过中心扩展法一步一步向两边扩展就行了。</p>
</li>
<li><p>$i = r$</p>
<p>此时我们先把 P [ i ] 赋值为 0，然后通过中心扩展法一步一步扩展就行了。</p>
</li>
</ul>
<p>考虑 $r$ 的更新</p>
<p>就这样一步一步的求出每个 $P [ i ]$，当求出的 $P [ i ]$ 的右边界大于当前的 $r$ 时，我们就需要更新 $r$ 为当前的回文串了。</p>
</li>
</ul>
<h2 id="最长公共子序列（LCS）"><a href="#最长公共子序列（LCS）" class="headerlink" title="最长公共子序列（LCS）"></a>最长公共子序列（LCS）</h2><h3 id="动态规划-1"><a href="#动态规划-1" class="headerlink" title="动态规划"></a>动态规划</h3><p>状态转移方程如下：</p>
<script type="math/tex; mode=display">
d p[i][j]=\left\{\begin{array}{ll}
d p[i-1][j-1]+1, & t e x t_{1}[i-1]=t e x t_{2}[j-1] \\
\max (d p[i-1][j], d p[i][j-1]), & t e x t_{1}[i-1] \neq t e x t_{2}[j-1]
\end{array}\right.</script><p>LCS 对应的状态转移方程与最长公共子串不同之处在于：</p>
<ul>
<li>最长公共子串要求字符串连续，所以下一个状态只能由上一个对应的字符串得到。</li>
<li>LCS 不要求字符串连续，所以可以前后移动，就有了第二个式子。</li>
</ul>
<p>知道状态定义之后，我们开始写状态转移方程。</p>
<ul>
<li><p>当 $text_1[i - 1] = text_2[j - 1]$ 时，说明两个子字符串的最后一位相等，所以最长公共子序列又增加了 1，所以 $dp[i][j] = dp[i - 1][j - 1] + 1$；举个例子，比如对于 <code>ac</code> 和 <code>bc</code> 而言，他们的最长公共子序列的长度等于 <code>a</code> 和 <code>b</code> 的最长公共子序列长度 $0 + 1 = 1$。</p>
</li>
<li><p>当 $text_1[i - 1] \ne text_2[j - 1]$ 时，说明两个子字符串的最后一位不相等，那么此时的状态 $dp[i][j]$ 应该是 $dp[i - 1][j]$ 和 $dp[i][j - 1]$ 的最大值。举个例子，比如对于 <code>ace</code> 和 <code>bc</code> 而言，他们的最长公共子序列的长度等于</p>
<p> ① <code>ace</code> 和 <code>b</code> 的最长公共子序列长度 <code>0</code> 与</p>
<p>② <code>ac</code> 和 <code>bc</code> 的最长公共子序列长度 <code>1</code> 的最大值，即 <code>1</code>。</p>
</li>
</ul>
<p>代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Solution</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">longest_common_subsequence</span>(text1: <span class="hljs-type">String</span>, text2: <span class="hljs-type">String</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">i32</span> &#123;<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">text1</span> = text1.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">text2</span> = text2.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">m</span> = text1.<span class="hljs-title function_ invoke__">len</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">n</span> = text2.<span class="hljs-title function_ invoke__">len</span>();<br>        <span class="hljs-comment">// dp[i][j] 代表 text1[0..i] 与 text2[0..j] 的最大子序列，注意不包括第 i 和第 j 个字符</span><br>        <span class="hljs-comment">// 同理，dp 数组要循环到 m 与 n 才结束</span><br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">dp</span> = <span class="hljs-built_in">vec!</span>[<span class="hljs-built_in">vec!</span>[<span class="hljs-number">0</span>; n + <span class="hljs-number">1</span>]; m + <span class="hljs-number">1</span>];<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>..=m &#123;<br>            <span class="hljs-keyword">for</span> <span class="hljs-variable">j</span> <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>..=n &#123;<br>                <span class="hljs-comment">// 这里要注意，比较的是第 i-1 与第 j-1 个字符</span><br>                <span class="hljs-keyword">if</span> text1[i - <span class="hljs-number">1</span>] == text2[j - <span class="hljs-number">1</span>] &#123;<br>                    dp[i][j] = dp[i - <span class="hljs-number">1</span>][j - <span class="hljs-number">1</span>] + <span class="hljs-number">1</span>;<br>                &#125; <span class="hljs-keyword">else</span> &#123;<br>                    dp[i][j] = std::cmp::<span class="hljs-title function_ invoke__">max</span>(dp[i][j - <span class="hljs-number">1</span>], dp[i - <span class="hljs-number">1</span>][j]);<br>                &#125;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> dp[m][n];<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h1 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h1><h2 id="寻找两个正序数组的中位数3"><a href="#寻找两个正序数组的中位数3" class="headerlink" title="寻找两个正序数组的中位数3"></a>寻找两个正序数组的中位数<sup><a href="#fn_3" id="reffn_3">3</a></sup></h2><p>中位数定义：将一个集合划分为两个长度相等的子集，其中一个子集中的元素总是大于另一个子集中的元素。</p>
<pre><code>      left_part          |         right_part
A[0], A[1], ..., A[i-1]  |  A[i], A[i+1], ..., A[m-1]
B[0], B[1], ..., B[j-1]  |  B[j], B[j+1], ..., B[n-1]
</code></pre><p>根据中位数的定义，我们需要找到以上的划分（设两个数组总长度为偶数）使得</p>
<ul>
<li>$\text{len}(left_part) = \text{len}(right_part)$</li>
<li>$\max(left_part)=\max(right_part)$</li>
</ul>
<p>此时的中位数为：</p>
<script type="math/tex; mode=display">\text{median} = \frac{\max(left\_part)+\min(right\_part)}{2}</script><p>所以现在的问题关键在于寻找这样一个划分。要寻找这样一个划分需要根据这个划分满足的两个条件：</p>
<ul>
<li>左边元素共有 $i + j$ 个，右边元素共有 $(m-i)+(n-j)$ 个，所以由第一个式子可以得到 $i+j=(m-i)+(n-j)$。变形得到 $i+j=\frac{m+n}{2}$。假设 $m &lt; n$，即 B 数组长于 A 数组，则 $i\in[0,m]$，有 $j = \frac{m+n}{2}-i$ 且 $j \in [0,n]$，所以只要知道 $i$ 的值，那么 $j$ 的值也是确定的。</li>
<li>在 $(0, m)$ 中找到 $i$，满足 $A[i-1] \le B[j]$ 且 $A[i] \ge B[j-1]$ 。</li>
</ul>
<p>注意到第一个条件中，当 $i$ 增大的时候，$j$ 会减小以此来保证左右两部分的元素个数相同。同时 A、B 数组都是单调不递减的，所以一定存在一个最大的 $i$ 满足 $A[i-1] \le B[j]$。（当 $i$ 取 $i+1$ 时 $A[i] &gt; B[j-1]$）</p>
<p>所以问题转化为：找一个最大的 $i$ 使得 $A[i-1] \le B[j]$。</p>
<p>对于这个问题，我们容易枚举 $i$，同时 A、B 都是单调递增的，所以我们还能知道枚举出的 $i$ 是不是满足条件（$A[i-1] \le B[j]$），并从中找出满足条件的最大 $i$ 值即可。</p>
<p>对于两个数组总长度为奇数的情况，可以使得 $j = \lfloor \frac{m+n+1}{2}-i \rfloor$。</p>
<p>代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-meta">#[warn(dead_code)]</span><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Solution</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">find_median_sorted_arrays</span>(nums1: <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">i32</span>&gt;, nums2: <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">i32</span>&gt;) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">f64</span> &#123;<br>        <span class="hljs-keyword">if</span> nums1.<span class="hljs-title function_ invoke__">len</span>() &gt; nums2.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>            <span class="hljs-keyword">return</span> Solution::<span class="hljs-title function_ invoke__">find_median_sorted_arrays</span>(nums2, nums1);<br>        &#125;<br>        <span class="hljs-comment">// m &lt; n</span><br>        <span class="hljs-keyword">let</span> (m, n) = (nums1.<span class="hljs-title function_ invoke__">len</span>(), nums2.<span class="hljs-title function_ invoke__">len</span>());<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">left</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">right</span> = m;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">pos</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">median1</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">median2</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">while</span> left &lt;= right &#123;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">i</span> = (left + right) / <span class="hljs-number">2</span>;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">j</span> = (m + n + <span class="hljs-number">1</span>) / <span class="hljs-number">2</span> - i;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">nums_im1</span> = <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> &#123; -<span class="hljs-number">0x3f3f3f3f</span> &#125; <span class="hljs-keyword">else</span> &#123; nums1[i - <span class="hljs-number">1</span>] &#125;;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">nums_i</span> = <span class="hljs-keyword">if</span> i == m &#123; <span class="hljs-number">0x3f3f3f3f</span> &#125; <span class="hljs-keyword">else</span> &#123; nums1[i] &#125;;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">nums_jm1</span> = <span class="hljs-keyword">if</span> j == <span class="hljs-number">0</span> &#123; -<span class="hljs-number">0x3f3f3f3f</span> &#125; <span class="hljs-keyword">else</span> &#123; nums2[j - <span class="hljs-number">1</span>] &#125;;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">nums_j</span> = <span class="hljs-keyword">if</span> j == n &#123; <span class="hljs-number">0x3f3f3f3f</span> &#125; <span class="hljs-keyword">else</span> &#123; nums2[j] &#125;;<br>            <span class="hljs-keyword">if</span> nums_im1 &lt;= nums_j &#123;<br>                median1 = std::cmp::<span class="hljs-title function_ invoke__">max</span>(nums_im1, nums_jm1);<br>                median2 = std::cmp::<span class="hljs-title function_ invoke__">min</span>(nums_i, nums_j);<br>                left = i + <span class="hljs-number">1</span>;<br>            &#125; <span class="hljs-keyword">else</span> &#123;<br>                right = i - <span class="hljs-number">1</span>;<br>            &#125;<br>        &#125;<br>        <span class="hljs-title function_ invoke__">if</span> (m + n) &amp; <span class="hljs-number">1</span> == <span class="hljs-number">0</span> &#123;<br>            (median1 + median2) <span class="hljs-keyword">as</span> <span class="hljs-type">f64</span> / <span class="hljs-number">2.0</span><br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            median1 <span class="hljs-keyword">as</span> <span class="hljs-type">f64</span><br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h2 id="三数之和"><a href="#三数之和" class="headerlink" title="三数之和"></a>三数之和</h2><h3 id="朴素算法"><a href="#朴素算法" class="headerlink" title="朴素算法"></a>朴素算法</h3><p>排序之后三重循环，判断三个数之和是否为 $0$，时间复杂度 $O(n^3)$。</p>
<p>排序的目的是为了容易地去除重复数字，因为排序之后只需要判断当前和前一个元素是否相等就可以知道是否是重复数字。</p>
<h3 id="排序后双指针"><a href="#排序后双指针" class="headerlink" title="排序后双指针"></a>排序后双指针</h3><p>注意到排序之后整个数组是单调非递减的，我们需要 $a+b+c=0$，当固定了 $a$ 和 $b$ 的时候，$c$ 从大到小地判断是否有 $a+b+c=0$ 即可。看似是最外层对应 $a$ 的循环嵌套对应 $b$ 的循环，并在其中加上了 $c$ 递减的循环，但是实际上注意到当 $b$ 与 $c$ 是同一个元素时，如果仍然不满足 $a+b+c=0$，那么 $c$ 继续向左减小就与之前的数字重复了，所以对于每一次 $b$ 中的循环，最多运行 $n$ 次，外边再嵌套 $a$ 的循环，时间复杂度为 $O(n^2)$。</p>
<p>代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-meta">#[warn(dead_code)]</span><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Solution</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">three_sum</span>(<span class="hljs-keyword">mut</span> nums: <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">i32</span>&gt;) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">Vec</span>&lt;<span class="hljs-type">i32</span>&gt;&gt; &#123;<br>        nums.<span class="hljs-title function_ invoke__">sort</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">len</span> = nums.<span class="hljs-title function_ invoke__">len</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">ans</span> = <span class="hljs-type">Vec</span>::<span class="hljs-title function_ invoke__">new</span>();<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..len &#123;<br>            <span class="hljs-comment">// 防止取到相同的数字</span><br>            <span class="hljs-keyword">if</span> i &gt; <span class="hljs-number">0</span> &amp;&amp; nums[i - <span class="hljs-number">1</span>] == nums[i] &#123;<br>                <span class="hljs-keyword">continue</span>;<br>            &#125;<br>            <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">third</span> = len - <span class="hljs-number">1</span>;<br>            <span class="hljs-comment">// 注意这里开始位置是 i+1，目的是为了不与 a 取重</span><br>            <span class="hljs-keyword">for</span> <span class="hljs-variable">j</span> <span class="hljs-keyword">in</span> i + <span class="hljs-number">1</span>..len &#123;<br>                <span class="hljs-comment">// 注意这里判定条件是 j &gt; i+1 否则会取不到与 a 相同的数字</span><br>                <span class="hljs-keyword">if</span> j &gt; i + <span class="hljs-number">1</span> &amp;&amp; nums[j - <span class="hljs-number">1</span>] == nums[j] &#123;<br>                    <span class="hljs-keyword">continue</span>;<br>                &#125;<br>                <span class="hljs-keyword">while</span> j &lt; third &amp;&amp; nums[i] + nums[j] + nums[third] &gt; <span class="hljs-number">0</span> &#123;<br>                    third = third - <span class="hljs-number">1</span>;<br>                &#125;<br>                <span class="hljs-keyword">if</span> j == third &#123;<br>                    <span class="hljs-keyword">break</span>;<br>                &#125;<br>                <span class="hljs-keyword">if</span> nums[i] + nums[j] + nums[third] == <span class="hljs-number">0</span> &#123;<br>                    ans.<span class="hljs-title function_ invoke__">push</span>(<span class="hljs-built_in">vec!</span>[nums[i], nums[j], nums[third]]);<br>                &#125;<br>            &#125;<br>        &#125;<br>        ans<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h2 id="盛最多水的容器4"><a href="#盛最多水的容器4" class="headerlink" title="盛最多水的容器4"></a>盛最多水的容器<sup><a href="#fn_4" id="reffn_4">4</a></sup></h2><script type="math/tex; mode=display">
area = (right - left) * \min (height[left], height[right])</script><p>由上面的公式可以知道，面积由两部分共同决定：</p>
<ul>
<li>宽度</li>
<li>高度</li>
</ul>
<p>所以考虑尽可能地增加宽度和高度。假设左指针指向的数为 $x$，右指针指向的数为 $y$，假设 $x &lt; y$，距离为 $t$，接下来进行具体分析：</p>
<ol>
<li>水量 $ area = \min(x, y) <em> t = x </em> t $，当左指针不变的时候，右指针无论在哪都不会影响容器的水量了，水量是固定的 $x*t$。</li>
<li>所以考虑左指针向右移动，这样才有可能取到更大的水量。</li>
<li>同理左指针指向的数大于右指针指向的数的时候，左移右指针才有可能取到更大的水量。</li>
<li>重复以上步骤就可以得到最大水量。</li>
</ol>
<p>总时间复杂度为 $O(n)$。</p>
<p>注解：</p>
<ul>
<li>对于双指针问题，两个指针的初始位置不一定都在最左或者最右，要灵活地设置指针位置。</li>
</ul>
<h2 id="最接近三数之和"><a href="#最接近三数之和" class="headerlink" title="最接近三数之和"></a>最接近三数之和</h2><p>与「盛最多水的容器」和「三数之和」类似，代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-meta">#[warn(dead_code)]</span><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Solution</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">three_sum_closest</span>(<span class="hljs-keyword">mut</span> nums: <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">i32</span>&gt;, target: <span class="hljs-type">i32</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">i32</span> &#123;<br>        nums.<span class="hljs-title function_ invoke__">sort</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">len</span> = nums.<span class="hljs-title function_ invoke__">len</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">ans</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">diff</span> = <span class="hljs-number">0x3f3f3f3f</span>;<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..len &#123;<br>            <span class="hljs-keyword">if</span> i &gt; <span class="hljs-number">0</span> &amp;&amp; nums[i] == nums[i - <span class="hljs-number">1</span>] &#123;<br>                <span class="hljs-keyword">continue</span>;<br>            &#125;<br>            <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">j</span> = i + <span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">k</span> = len - <span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">while</span> j &lt; k &#123;<br>                <span class="hljs-comment">//dbg!((i, j , k));</span><br>                <span class="hljs-keyword">let</span> <span class="hljs-variable">sum</span> = nums[i] + nums[j] + nums[k];<br>                <span class="hljs-keyword">if</span> sum == target &#123;<br>                    <span class="hljs-keyword">return</span> sum;<br>                &#125;<br>                <span class="hljs-keyword">let</span> <span class="hljs-variable">tmp</span> = (sum - target).<span class="hljs-title function_ invoke__">abs</span>();<br>                <span class="hljs-keyword">if</span> tmp &lt; diff &#123;<br>                    diff = tmp;<br>                    ans = sum;<br>                &#125;<br>                <span class="hljs-keyword">if</span> sum &gt; target &#123;<br>                    <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">k0</span> = k - <span class="hljs-number">1</span>;<br>                    <span class="hljs-keyword">while</span> j &lt; k0 &amp;&amp; nums[k0] == nums[k] &#123;<br>                        k0 = k0 - <span class="hljs-number">1</span>;<br>                    &#125;<br>                    k = k0;<br>                &#125; <span class="hljs-keyword">else</span> &#123;<br>                    <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">j0</span> = j + <span class="hljs-number">1</span>;<br>                    <span class="hljs-keyword">while</span> j0 &lt; k &amp;&amp; nums[j0] == nums[j] &#123;<br>                        j0 = j0 + <span class="hljs-number">1</span>;<br>                    &#125;<br>                    j = j0;<br>                &#125;<br>            &#125;<br>        &#125;<br>        ans<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="合并K个升序链表"><a href="#合并K个升序链表" class="headerlink" title="合并K个升序链表"></a>合并K个升序链表</h2><p>使用优先队列即可。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::&#123;cmp::Reverse, collections::BinaryHeap&#125;;<br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">merge_k_lists</span>(lists: <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">Option</span>&lt;<span class="hljs-type">Box</span>&lt;ListNode&gt;&gt;&gt;) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">Option</span>&lt;<span class="hljs-type">Box</span>&lt;ListNode&gt;&gt; &#123;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">priority_queue</span> = BinaryHeap::<span class="hljs-title function_ invoke__">new</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">ret</span> = <span class="hljs-type">Box</span>::<span class="hljs-title function_ invoke__">new</span>(ListNode::<span class="hljs-title function_ invoke__">new</span>(<span class="hljs-number">0</span>));<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">ptr</span> = &amp;<span class="hljs-keyword">mut</span> ret;<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">list</span> <span class="hljs-keyword">in</span> lists &#123;<br>            <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">plist</span> = &amp;list;<br>            <span class="hljs-keyword">while</span> <span class="hljs-keyword">let</span> <span class="hljs-variable">Some</span>(node) = plist &#123;<br>                priority_queue.<span class="hljs-title function_ invoke__">push</span>(<span class="hljs-title function_ invoke__">Reverse</span>(node.val));<br>                plist = &amp;node.next;<br>            &#125;<br>        &#125;<br><br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">let</span> <span class="hljs-variable">Some</span>(<span class="hljs-title function_ invoke__">Reverse</span>(node)) = priority_queue.<span class="hljs-title function_ invoke__">pop</span>() &#123;<br>            ptr.next = <span class="hljs-title function_ invoke__">Some</span>(<span class="hljs-type">Box</span>::<span class="hljs-title function_ invoke__">new</span>(ListNode::<span class="hljs-title function_ invoke__">new</span>(node)));<br>            ptr = ptr.next.<span class="hljs-title function_ invoke__">as_mut</span>().<span class="hljs-title function_ invoke__">unwrap</span>();<br>        &#125;<br>        ret.next<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote id="fn_1">
<sup>1</sup>. <a href="https://leetcode-cn.com/problems/longest-palindromic-substring/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-bao-gu">https://leetcode-cn.com/problems/longest-palindromic-substring/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-bao-gu</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. <a href="https://oi-wiki.org/string/manacher/">https://oi-wiki.org/string/manacher/</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. <a href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/solution/xun-zhao-liang-ge-you-xu-shu-zu-de-zhong-wei-s-114/">https://leetcode-cn.com/problems/median-of-two-sorted-arrays/solution/xun-zhao-liang-ge-you-xu-shu-zu-de-zhong-wei-s-114/</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. <a href="https://leetcode-cn.com/problems/container-with-most-water/solution/sheng-zui-duo-shui-de-rong-qi-by-leetcode-solution/">https://leetcode-cn.com/problems/container-with-most-water/solution/sheng-zui-duo-shui-de-rong-qi-by-leetcode-solution/</a><a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a>
</blockquote>
]]></content>
      <tags>
        <tag>data structure, algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Internlm-05-LMDeploy 的量化和部署</title>
    <url>/internlm/internlm-05/</url>
    <content><![CDATA[<h1 id="LMDeploy-的量化和部署"><a href="#LMDeploy-的量化和部署" class="headerlink" title="LMDeploy 的量化和部署"></a>LMDeploy 的量化和部署</h1><h2 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1 环境配置"></a>1 环境配置</h2><p>这里 <code>/share/conda_envs</code> 目录下的环境是官方未大家准备好的基础环境，因为该目录是共享只读的，而我们后面需要在此基础上安装新的软件包，所以需要复制到我们自己的 conda 环境（该环境下我们是可写的）。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">conda create -n lmdeploy --<span class="hljs-built_in">clone</span> /share/conda_envs/internlm-base<br></code></pre></td></tr></table></figure>
<ul>
<li>如果clone操作过慢，可采用如下操作:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">/root/share/install_conda_env_internlm_base.sh lmdeploy<br></code></pre></td></tr></table></figure>
<p>我们取 <code>CONDA_ENV_NAME</code> 为 <code>lmdeploy</code>，复制完成后，可以在本地查看环境。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">conda <span class="hljs-built_in">env</span> list<br></code></pre></td></tr></table></figure>
<p>结果如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># conda environments:</span><br><span class="hljs-comment">#</span><br>base                  *  /root/.conda<br>lmdeploy                 /root/.conda/envs/lmdeploy<br></code></pre></td></tr></table></figure>
<p>然后激活环境。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">conda activate lmdeploy<br></code></pre></td></tr></table></figure>
<p>如果是在 InternStudio 开发环境，需要先运行下面的命令，否则会报错。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 解决 ModuleNotFoundError: No module named &#x27;packaging&#x27; 问题</span><br>pip install packaging<br><span class="hljs-comment"># 使用 flash_attn 的预编译包解决安装过慢问题</span><br>pip install /root/share/wheels/flash_attn-2.4.2+cu118torch2.0cxx11abiTRUE-cp310-cp310-linux_x86_64.whl<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install <span class="hljs-string">&#x27;lmdeploy[all]==v0.1.0&#x27;</span><br></code></pre></td></tr></table></figure>
<p>由于默认安装的是 runtime 依赖包，但是我们这里还需要部署和量化，所以，这里选择 <code>[all]</code>。然后可以再检查一下 lmdeploy 包，如下图所示。</p>
<p><img  src="env.png"  ><span class="image-caption">安装 lmdeploy 成功</span></p>
<p>基础环境到这里就配置好了。</p>
<h2 id="2-服务部署"><a href="#2-服务部署" class="headerlink" title="2 服务部署"></a>2 服务部署</h2><p>这一部分主要涉及本地推理和部署。我们先看一张图。</p>
<p><img  src="lmdeploy.drawio.png"  ><span class="image-caption">服务架构图</span></p>
<p>lmdeploy 从架构上把整个服务流程分成下面几个模块。</p>
<ul>
<li>模型推理/服务。主要提供模型本身的推理，一般来说可以和具体业务解耦，专注模型推理本身性能的优化。可以以模块、API等多种方式提供。</li>
<li>Client。可以理解为前端，与用户交互的地方。</li>
<li>API Server。一般作为前端的后端，提供与产品和服务相关的数据和功能支持。</li>
</ul>
<p>值得说明的是，以上的划分是一个相对完整的模型，但在实际中这并不是绝对的。比如可以把“模型推理”和“API Server”合并，有的甚至是三个流程打包在一起提供服务。</p>
<p>接下来，我们看一下 lmdeploy 提供的部署功能。</p>
<h3 id="2-1-模型转换"><a href="#2-1-模型转换" class="headerlink" title="2.1 模型转换"></a>2.1 模型转换</h3><p>使用 TurboMind 推理模型需要先将模型转化为 TurboMind 的格式，目前支持在线转换和离线转换两种形式。在线转换可以直接加载 Huggingface 模型，离线转换需需要先保存模型再加载。</p>
<p>TurboMind 是一款关于 LLM 推理的高效推理引擎，基于英伟达的 <a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a> 研发而成。它的主要功能包括：LLaMa 结构模型的支持，persistent batch 推理模式和可扩展的 KV 缓存管理器。</p>
<h4 id="2-1-1-在线转换"><a href="#2-1-1-在线转换" class="headerlink" title="2.1.1 在线转换"></a>2.1.1 在线转换</h4><p>lmdeploy 支持直接读取 Huggingface 模型权重，目前共支持三种类型：</p>
<ul>
<li>在 huggingface.co 上面通过 lmdeploy 量化的模型，如 <a href="https://huggingface.co/lmdeploy/llama2-chat-70b-4bit">llama2-70b-4bit</a>, <a href="https://huggingface.co/internlm/internlm-chat-20b-4bit">internlm-chat-20b-4bit</a></li>
<li>huggingface.co 上面其他 LM 模型，如 Qwen/Qwen-7B-Chat</li>
</ul>
<p>示例如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 需要能访问 Huggingface 的网络环境</span><br>lmdeploy chat turbomind internlm/internlm-chat-20b-4bit --model-name internlm-chat-20b<br>lmdeploy chat turbomind Qwen/Qwen-7B-Chat --model-name qwen-7b<br></code></pre></td></tr></table></figure>
<p>上面两行命令分别展示了如何直接加载 Huggingface 的模型，第一条命令是加载使用 lmdeploy 量化的版本，第二条命令是加载其他 LLM 模型。</p>
<p>我们也可以直接启动本地的 Huggingface 模型，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/  --model-name internlm-chat-7b<br></code></pre></td></tr></table></figure>
<p>以上命令都会启动一个本地对话界面，通过 Bash 可以与 LLM 进行对话。</p>
<h4 id="2-1-2-离线转换"><a href="#2-1-2-离线转换" class="headerlink" title="2.1.2 离线转换"></a>2.1.2 离线转换</h4><p>离线转换需要在启动服务之前，将模型转为 lmdeploy TurboMind  的格式，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 转换模型（FastTransformer格式） TurboMind</span><br>lmdeploy convert internlm-chat-7b /path/to/internlm-chat-7b<br></code></pre></td></tr></table></figure>
<p>这里我们使用官方提供的模型文件，就在用户根目录执行，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy convert internlm-chat-7b  /root/share/temp/model_repos/internlm-chat-7b/<br></code></pre></td></tr></table></figure>
<p><img  src="convert.png"  ><span class="image-caption">转换模型</span></p>
<p>执行完成后将会在当前目录生成一个 <code>workspace</code> 的文件夹。这里面包含的就是 TurboMind 和 Triton “模型推理”需要到的文件。</p>
<p>目录如下图所示。</p>
<p><img  src="tree1.png"  ><span class="image-caption">转换后的模型</span></p>
<p><code>weights</code> 和 <code>tokenizer</code> 目录分别放的是拆分后的参数和 Tokenizer。如果我们进一步查看 <code>weights</code> 的目录，就会发现参数是按层和模块拆开的，如下图所示。</p>
<p><img  src="tree2.png"  ><span class="image-caption">转换后的权重</span></p>
<p>每一份参数第一个 0 表示“层”的索引，后面的那个0表示 Tensor 并行的索引，因为我们只有一张卡，所以被拆分成 1 份。如果有两张卡可以用来推理，则会生成0和1两份，也就是说，会把同一个参数拆成两份。比如 <code>layers.0.attention.w_qkv.0.weight</code> 会变成 <code>layers.0.attention.w_qkv.0.weight</code> 和 <code>layers.0.attention.w_qkv.1.weight</code>。执行 <code>lmdeploy convert</code> 命令时，可以通过 <code>--tp</code> 指定（tp 表示 tensor parallel），该参数默认值为1（也就是一张卡）。</p>
<p><strong>关于Tensor并行</strong></p>
<p>Tensor并行一般分为行并行或列并行，原理如下图所示。</p>
<p><img  src="col.png"  ><span class="image-caption">列并行</span></p>
<p><img  src="row.png"  ><span class="image-caption">行并行</span></p>
<p>简单来说，就是把一个大的张量（参数）分到多张卡上，分别计算各部分的结果，然后再同步汇总。</p>
<h3 id="2-2-TurboMind-推理-命令行本地对话"><a href="#2-2-TurboMind-推理-命令行本地对话" class="headerlink" title="2.2  TurboMind 推理+命令行本地对话"></a>2.2  TurboMind 推理+命令行本地对话</h3><p>模型转换完成后，我们就具备了使用模型推理的条件，接下来就可以进行真正的模型推理环节。</p>
<p>我们先尝试本地对话（<code>Bash Local Chat</code>），下面用（Local Chat 表示）在这里其实是跳过 API Server 直接调用 TurboMind。简单来说，就是命令行代码直接执行 TurboMind。所以说，实际和前面的架构图是有区别的。</p>
<p>这里支持多种方式运行，比如Turbomind、PyTorch、DeepSpeed。但 PyTorch 和 DeepSpeed 调用的其实都是 Huggingface 的 Transformers 包，PyTorch表示原生的 Transformer 包，DeepSpeed 表示使用了 DeepSpeed 作为推理框架。Pytorch/DeepSpeed 目前功能都比较弱，不具备生产能力，不推荐使用。</p>
<p>执行命令如下。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Turbomind + Bash Local Chat</span><br>lmdeploy chat turbomind ./workspace<br></code></pre></td></tr></table></figure>
<p>启动后就可以和它进行对话了，如下图所示。</p>
<p><img  src="chat.png"  ><span class="image-caption">与本地部署的模型进行对话</span></p>
<p>输入后两次回车，退出时输入<code>exit</code> 回车两次即可。此时，Server 就是本地跑起来的模型（TurboMind），命令行可以看作是前端。</p>
<h3 id="2-3-TurboMind推理-API服务"><a href="#2-3-TurboMind推理-API服务" class="headerlink" title="2.3 TurboMind推理+API服务"></a>2.3 TurboMind推理+API服务</h3><p>在上面的部分我们尝试了直接用命令行启动 Client，接下来我们尝试如何运用 lmdepoy 进行服务化。</p>
<p>”模型推理/服务“目前提供了 Turbomind 和 TritonServer 两种服务化方式。此时，Server 是 TurboMind 或 TritonServer，API Server 可以提供对外的 API 服务。我们推荐使用 TurboMind，TritonServer 使用方式详见《附录1》。</p>
<p>首先，通过下面命令启动服务。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ApiServer+Turbomind   api_server =&gt; AsyncEngine =&gt; TurboMind</span><br>lmdeploy serve api_server ./workspace \<br>	--server_name 0.0.0.0 \<br>	--server_port 23333 \<br>	--instance_num 64 \<br>	--tp 1<br></code></pre></td></tr></table></figure>
<p>上面的参数中 <code>server_name</code> 和 <code>server_port</code> 分别表示服务地址和端口，<code>tp</code> 参数我们之前已经提到过了，表示 Tensor 并行。还剩下一个 <code>instance_num</code> 参数，表示实例数，可以理解成 Batch 的大小。执行后如下图所示。</p>
<p><img  src="api-deploy.png"  ><span class="image-caption">部署 api server</span></p>
<p>然后，我们可以新开一个窗口，执行下面的 Client 命令。如果使用官方机器，可以打开 vscode 的 Terminal，执行下面的命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ChatApiClient+ApiServer（注意是http协议，需要加http）</span><br>lmdeploy serve api_client http://localhost:23333<br></code></pre></td></tr></table></figure>
<p>如下图所示。</p>
<p><img  src="test-apiserver.png"  ><span class="image-caption">测试 api server</span></p>
<p>当然，刚刚我们启动的是 API Server，自然也有相应的接口。可以直接打开 <code>http://&#123;host&#125;:23333</code> 查看，如下图所示。</p>
<p><img  src="fastapi.png"  ><span class="image-caption">fastapi 演示</span></p>
<p>这里一共提供了 4 个 HTTP 的接口，任何语言都可以对其进行调用，我们以 <code>v1/chat/completions</code> 接口为例，简单试一下。</p>
<p>接口请求参数如下：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;model&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;internlm-chat-7b&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;messages&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;写一首冬天的诗&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;temperature&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0.7</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;top_p&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;n&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;max_tokens&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">512</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;stop&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;stream&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;presence_penalty&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;frequency_penalty&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;user&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;string&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;repetition_penalty&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;renew_session&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;ignore_eos&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure>
<p>请求结果如下。</p>
<p><img  src="test-fastapi.png"  ><span class="image-caption">测试 api</span></p>
<h3 id="2-4-网页-Demo-演示"><a href="#2-4-网页-Demo-演示" class="headerlink" title="2.4 网页 Demo 演示"></a>2.4 网页 Demo 演示</h3><p>这一部分主要是将 Gradio 作为前端 Demo 演示。在上一节的基础上，我们不执行后面的 <code>api_client</code> 或 <code>triton_client</code>，而是执行 <code>gradio</code>。</p>
<h4 id="2-4-1-TurboMind-服务作为后端"><a href="#2-4-1-TurboMind-服务作为后端" class="headerlink" title="2.4.1 TurboMind 服务作为后端"></a>2.4.1 TurboMind 服务作为后端</h4><p>API Server 的启动和上一节一样，这里直接启动作为前端的 Gradio。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Gradio+ApiServer。必须先开启 Server，此时 Gradio 为 Client</span><br>lmdeploy serve gradio http://0.0.0.0:23333 \<br>	--server_name 0.0.0.0 \<br>	--server_port 6006 \<br>	--restful_api True<br></code></pre></td></tr></table></figure>
<h4 id="2-4-2-TurboMind-推理作为后端"><a href="#2-4-2-TurboMind-推理作为后端" class="headerlink" title="2.4.2 TurboMind 推理作为后端"></a>2.4.2 TurboMind 推理作为后端</h4><p>当然，Gradio 也可以直接和 TurboMind 连接，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Gradio+Turbomind(local)</span><br>lmdeploy serve gradio ./workspace<br></code></pre></td></tr></table></figure>
<p>可以直接启动 Gradio，此时没有 API Server，TurboMind 直接与 Gradio 通信。</p>
<h3 id="2-5-TurboMind-推理-Python-代码集成"><a href="#2-5-TurboMind-推理-Python-代码集成" class="headerlink" title="2.5 TurboMind 推理 + Python 代码集成"></a>2.5 TurboMind 推理 + Python 代码集成</h3><p>前面介绍的都是通过 API 或某种前端与”模型推理/服务“进行交互，lmdeploy 还支持 Python 直接与 TurboMind 进行交互，如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> lmdeploy <span class="hljs-keyword">import</span> turbomind <span class="hljs-keyword">as</span> tm<br><br><span class="hljs-comment"># load model</span><br>model_path = <span class="hljs-string">&quot;/root/share/temp/model_repos/internlm-chat-7b/&quot;</span><br>tm_model = tm.TurboMind.from_pretrained(model_path, model_name=<span class="hljs-string">&#x27;internlm-chat-20b&#x27;</span>)<br>generator = tm_model.create_instance()<br><br><span class="hljs-comment"># process query</span><br>query = <span class="hljs-string">&quot;你好啊兄嘚&quot;</span><br>prompt = tm_model.model.get_prompt(query)<br>input_ids = tm_model.tokenizer.encode(prompt)<br><br><span class="hljs-comment"># inference</span><br><span class="hljs-keyword">for</span> outputs <span class="hljs-keyword">in</span> generator.stream_infer(<br>        session_id=<span class="hljs-number">0</span>,<br>        input_ids=[input_ids]):<br>    res, tokens = outputs[<span class="hljs-number">0</span>]<br><br>response = tm_model.tokenizer.decode(res.tolist())<br><span class="hljs-built_in">print</span>(response)<br></code></pre></td></tr></table></figure>
<p>在上面的代码中，我们首先加载模型，然后构造输入，最后执行推理。</p>
<p>加载模型可以显式指定模型路径，也可以直接指定 Huggingface 的 repo_id，还可以使用上面生成过的 <code>workspace</code>。这里的 <code>tm.TurboMind</code> 其实是对 C++ TurboMind 的封装。</p>
<p>构造输入这里主要是把用户的 query 构造成 InternLLM 支持的输入格式，比如上面的例子中， <code>query</code> 是“你好啊兄嘚”，构造好的 Prompt 如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">&lt;|System|&gt;:You are an AI assistant whose name is InternLM (书生·浦语).</span><br><span class="hljs-string">- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span><br><span class="hljs-string">- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span><br><span class="hljs-string"></span><br><span class="hljs-string">&lt;|User|&gt;:你好啊兄嘚</span><br><span class="hljs-string">&lt;|Bot|&gt;:</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>Prompt 其实就是增加了 <code>&lt;|System|&gt;</code> 消息和 <code>&lt;|User|&gt;</code> 消息（即用户的 <code>query</code>），以及一个 <code>&lt;|Bot|&gt;</code> 的标记，表示接下来该模型输出响应了。最终输出的响应内容如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;你好啊，有什么我可以帮助你的吗？&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="2-6-最佳实践"><a href="#2-6-最佳实践" class="headerlink" title="2.6 最佳实践"></a>2.6 最佳实践</h3><h4 id="2-6-1-方案实践"><a href="#2-6-1-方案实践" class="headerlink" title="2.6.1 方案实践"></a>2.6.1 方案实践</h4><p>首先说 “模型推理/服务”，推荐使用 TurboMind，使用简单，性能良好，相关的 Benchmark 对比如下。</p>
<p><img  src="benchmark.png"  ><span class="image-caption">Benchmark 效果</span></p>
<p>上面的性能对比包括两个场景：</p>
<ul>
<li>场景一（前4张图）：固定的输入、输出 token 数（分别1和2048），测试Token输出吞吐量（output token throughput）。</li>
<li>场景二（第5张图）：使用真实数据，测试吞吐量（request throughput）。</li>
</ul>
<p>场景一中，BatchSize=64时，TurboMind 的吞吐量超过 2000 token/s，整体比 DeepSpeed 提升约 5% - 15%；BatchSize=32时，比 Huggingface 的Transformers 提升约 3 倍；其他BatchSize时 TurboMind 也表现出优异的性能。</p>
<p>场景二中，对比了 TurboMind 和 vLLM 在真实数据上的吞吐量（request throughput）指标，TurboMind 的效率比 vLLM 高 30%。</p>
<p>大家不妨亲自使用本地对话（Local Chat）感受一下性能差别（2.2 节），也可以执行我们提供的 <code>infer_compare.py</code> 脚本，示例如下。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 执行 Huggingface 的 Transformer</span><br>python infer_compare.py hf<br><span class="hljs-comment"># 执行LMDeploy</span><br>python infer_compare.py lmdeploy<br></code></pre></td></tr></table></figure>
<p>LMDeploy应该是Transformers的3-5倍左右。</p>
<p>后面的 API 服务和 Client 就得分场景了。</p>
<ul>
<li>我想对外提供类似 OpenAI 那样的 HTTP 接口服务。推荐使用 TurboMind推理 + API 服务（2.3）。</li>
<li>我想做一个演示 Demo，Gradio 无疑是比 Local Chat 更友好的。推荐使用 TurboMind 推理作为后端的Gradio进行演示（2.4.2）。</li>
<li>我想直接在自己的 Python 项目中使用大模型功能。推荐使用 TurboMind推理 + Python（2.5）。</li>
<li>我想在自己的其他非 Python 项目中使用大模型功能。推荐直接通过 HTTP 接口调用服务。也就是用本列表第一条先启动一个 HTTP API 服务，然后在项目中直接调用接口。</li>
</ul>
<h4 id="2-6-2-模型配置实践"><a href="#2-6-2-模型配置实践" class="headerlink" title="2.6.2 模型配置实践"></a>2.6.2 模型配置实践</h4><p>不知道大家还有没有印象，在离线转换（2.1.2）一节，我们查看了 <code>weights</code> 的目录，里面存放的是模型按层、按并行卡拆分的参数，不过还有一个文件 <code>config.ini</code> 并不是模型参数，它里面存的主要是模型相关的配置信息。下面是一个示例。</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[llama]</span><br><span class="hljs-attr">model_name</span> = internlm-chat-<span class="hljs-number">7</span>b<br><span class="hljs-attr">tensor_para_size</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">head_num</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">kv_head_num</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">vocab_size</span> = <span class="hljs-number">103168</span><br><span class="hljs-attr">num_layer</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">inter_size</span> = <span class="hljs-number">11008</span><br><span class="hljs-attr">norm_eps</span> = <span class="hljs-number">1</span>e-<span class="hljs-number">06</span><br><span class="hljs-attr">attn_bias</span> = <span class="hljs-number">0</span><br><span class="hljs-attr">start_id</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">end_id</span> = <span class="hljs-number">2</span><br><span class="hljs-attr">session_len</span> = <span class="hljs-number">2056</span><br><span class="hljs-attr">weight_type</span> = fp16<br><span class="hljs-attr">rotary_embedding</span> = <span class="hljs-number">128</span><br><span class="hljs-attr">rope_theta</span> = <span class="hljs-number">10000.0</span><br><span class="hljs-attr">size_per_head</span> = <span class="hljs-number">128</span><br><span class="hljs-attr">group_size</span> = <span class="hljs-number">0</span><br><span class="hljs-attr">max_batch_size</span> = <span class="hljs-number">64</span><br><span class="hljs-attr">max_context_token_num</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">step_length</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">cache_max_entry_count</span> = <span class="hljs-number">0.5</span><br><span class="hljs-attr">cache_block_seq_len</span> = <span class="hljs-number">128</span><br><span class="hljs-attr">cache_chunk_size</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">use_context_fmha</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">quant_policy</span> = <span class="hljs-number">0</span><br><span class="hljs-attr">max_position_embeddings</span> = <span class="hljs-number">2048</span><br><span class="hljs-attr">rope_scaling_factor</span> = <span class="hljs-number">0.0</span><br><span class="hljs-attr">use_logn_attn</span> = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>
<p>其中，模型属性相关的参数不可更改，主要包括下面这些。</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">model_name</span> = llama2<br><span class="hljs-attr">head_num</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">kv_head_num</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">vocab_size</span> = <span class="hljs-number">103168</span><br><span class="hljs-attr">num_layer</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">inter_size</span> = <span class="hljs-number">11008</span><br><span class="hljs-attr">norm_eps</span> = <span class="hljs-number">1</span>e-<span class="hljs-number">06</span><br><span class="hljs-attr">attn_bias</span> = <span class="hljs-number">0</span><br><span class="hljs-attr">start_id</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">end_id</span> = <span class="hljs-number">2</span><br><span class="hljs-attr">rotary_embedding</span> = <span class="hljs-number">128</span><br><span class="hljs-attr">rope_theta</span> = <span class="hljs-number">10000.0</span><br><span class="hljs-attr">size_per_head</span> = <span class="hljs-number">128</span><br></code></pre></td></tr></table></figure>
<p>和数据类型相关的参数也不可更改，主要包括两个。</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">weight_type</span> = fp16<br><span class="hljs-attr">group_size</span> = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>
<p><code>weight_type</code> 表示权重的数据类型。目前支持 fp16 和 int4。int4 表示 4bit 权重。当 <code>weight_type</code> 为 4bit 权重时，<code>group_size</code> 表示 <code>awq</code> 量化权重时使用的 group 大小。</p>
<p>剩余参数包括下面几个。</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">tensor_para_size</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">session_len</span> = <span class="hljs-number">2056</span><br><span class="hljs-attr">max_batch_size</span> = <span class="hljs-number">64</span><br><span class="hljs-attr">max_context_token_num</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">step_length</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">cache_max_entry_count</span> = <span class="hljs-number">0.5</span><br><span class="hljs-attr">cache_block_seq_len</span> = <span class="hljs-number">128</span><br><span class="hljs-attr">cache_chunk_size</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">use_context_fmha</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">quant_policy</span> = <span class="hljs-number">0</span><br><span class="hljs-attr">max_position_embeddings</span> = <span class="hljs-number">2048</span><br><span class="hljs-attr">rope_scaling_factor</span> = <span class="hljs-number">0.0</span><br><span class="hljs-attr">use_logn_attn</span> = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>
<p>一般情况下，我们并不需要对这些参数进行修改，但有时候为了满足特定需要，可能需要调整其中一部分配置值。这里主要介绍三个可能需要调整的参数。</p>
<ul>
<li>KV int8 开关：<ul>
<li>对应参数为 <code>quant_policy</code>，默认值为 0，表示不使用 KV Cache，如果需要开启，则将该参数设置为 4。</li>
<li>KV Cache 是对序列生成过程中的 K 和 V 进行量化，用以节省显存。我们下一部分会介绍具体的量化过程。</li>
<li>当显存不足，或序列比较长时，建议打开此开关。</li>
</ul>
</li>
<li>外推能力开关：<ul>
<li>对应参数为 <code>rope_scaling_factor</code>，默认值为 0.0，表示不具备外推能力，设置为 1.0，可以开启 RoPE 的 Dynamic NTK 功能，支持长文本推理。另外，<code>use_logn_attn</code> 参数表示 Attention 缩放，默认值为 0，如果要开启，可以将其改为 1。</li>
<li>外推能力是指推理时上下文的长度超过训练时的最大长度时模型生成的能力。如果没有外推能力，当推理时上下文长度超过训练时的最大长度，效果会急剧下降。相反，则下降不那么明显，当然如果超出太多，效果也会下降的厉害。</li>
<li>当推理文本非常长（明显超过了训练时的最大长度）时，建议开启外推能力。</li>
</ul>
</li>
<li>批处理大小：<ul>
<li>对应参数为 <code>max_batch_size</code>，默认为 64，也就是我们在 API Server 启动时的 <code>instance_num</code> 参数。</li>
<li>该参数值越大，吞度量越大（同时接受的请求数），但也会占用更多显存。</li>
<li>建议根据请求量和最大的上下文长度，按实际情况调整。</li>
</ul>
</li>
</ul>
<h2 id="3-模型量化"><a href="#3-模型量化" class="headerlink" title="3 模型量化"></a>3 模型量化</h2><p>本部分内容主要介绍如何对模型进行量化。主要包括 KV Cache 量化和模型参数量化。总的来说，量化是一种以参数或计算中间结果精度下降换空间节省（以及同时带来的性能提升）的策略。</p>
<p>正式介绍 LMDeploy 量化方案前，需要先介绍两个概念：</p>
<ul>
<li>计算密集（compute-bound）: 指推理过程中，绝大部分时间消耗在数值计算上；针对计算密集型场景，可以通过使用更快的硬件计算单元来提升计算速。</li>
<li>访存密集（memory-bound）: 指推理过程中，绝大部分时间消耗在数据读取上；针对访存密集型场景，一般通过减少访存次数、提高计算访存比或降低访存量来优化。</li>
</ul>
<p>常见的 LLM 模型由于 Decoder Only 架构的特性，实际推理时大多数的时间都消耗在了逐 Token 生成阶段（Decoding 阶段），是典型的访存密集型场景。</p>
<p>那么，如何优化 LLM 模型推理中的访存密集问题呢？ 我们可以使用 <strong>KV Cache 量化</strong>和 <strong>4bit Weight Only 量化（W4A16）</strong>。KV Cache 量化是指将逐 Token（Decoding）生成过程中的上下文 K 和 V 中间结果进行 INT8 量化（计算时再反量化），以降低生成过程中的显存占用。4bit Weight 量化，将 FP16 的模型权重量化为 INT4，Kernel 计算时，访存量直接降为 FP16 模型的 1/4，大幅降低了访存成本。Weight Only 是指仅量化权重，数值计算依然采用 FP16（需要将 INT4 权重反量化）。</p>
<h3 id="3-1-KV-Cache-量化"><a href="#3-1-KV-Cache-量化" class="headerlink" title="3.1 KV Cache 量化"></a>3.1 KV Cache 量化</h3><h4 id="3-1-1-量化步骤"><a href="#3-1-1-量化步骤" class="headerlink" title="3.1.1 量化步骤"></a>3.1.1 量化步骤</h4><p>KV Cache 量化是将已经生成序列的 KV 变成 Int8，使用过程一共包括三步：</p>
<p>第一步：计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况。</p>
<ul>
<li>对于 Attention 的 K 和 V：取每个 Head 各自维度在所有Token的最大、最小和绝对值最大值。对每一层来说，上面三组值都是 <code>(num_heads, head_dim)</code> 的矩阵。这里的统计结果将用于本小节的 KV Cache。</li>
<li>对于模型每层的输入：取对应维度的最大、最小、均值、绝对值最大和绝对值均值。每一层每个位置的输入都有对应的统计值，它们大多是 <code>(hidden_dim, )</code> 的一维向量，当然在 FFN 层由于结构是先变宽后恢复，因此恢复的位置维度并不相同。这里的统计结果用于下个小节的模型参数量化，主要用在缩放环节（回顾PPT内容）。</li>
</ul>
<p>第一步执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 计算 minmax</span><br>lmdeploy lite calibrate \<br>  --model  /root/share/temp/model_repos/internlm-chat-7b/ \<br>  --calib_dataset <span class="hljs-string">&quot;c4&quot;</span> \<br>  --calib_samples 128 \<br>  --calib_seqlen 2048 \<br>  --work_dir ./quant_output<br></code></pre></td></tr></table></figure>
<p>在这个命令行中，会选择 128 条输入样本，每条样本长度为 2048，数据集选择 C4，输入模型后就会得到上面的各种统计值。值得说明的是，如果显存不足，可以适当调小 samples 的数量或 sample 的长度。</p>
<blockquote>
<p>这一步由于默认需要从 Huggingface 下载数据集，国内经常不成功。所以我们导出了需要的数据，大家需要对读取数据集的代码文件做一下替换。共包括两步：</p>
<ul>
<li>第一步：复制 <code>calib_dataloader.py</code> 到安装目录替换该文件：<code>cp /root/share/temp/datasets/c4/calib_dataloader.py  /root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/utils/</code></li>
<li>第二步：将用到的数据集（c4）复制到下面的目录：<code>cp -r /root/share/temp/datasets/c4/ /root/.cache/huggingface/datasets/</code> </li>
</ul>
</blockquote>
<p>第二步：通过 minmax 获取量化参数。主要就是利用下面这个公式，获取每一层的 K V 中心值（zp）和缩放值（scale）。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">zp = (min+max) / 2<br>scale = (max-min) / 255<br>quant: q = round( (f-zp) / scale)<br>dequant: f = q * scale + zp<br></code></pre></td></tr></table></figure>
<p>有这两个值就可以进行量化和解量化操作了。具体来说，就是对历史的 K 和 V 存储 quant 后的值，使用时在 dequant。</p>
<p>第二步的执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 通过 minmax 获取量化参数</span><br>lmdeploy lite kv_qparams \<br>  --work_dir ./quant_output  \<br>  --turbomind_dir workspace/triton_models/weights/ \<br>  --kv_sym False \<br>  --num_tp 1<br></code></pre></td></tr></table></figure>
<p>在这个命令中，<code>num_tp</code> 的含义前面介绍过，表示 Tensor 的并行数。每一层的中心值和缩放值会存储到 <code>workspace</code> 的参数目录中以便后续使用。<code>kv_sym</code> 为 <code>True</code> 时会使用另一种（对称）量化方法，它用到了第一步存储的绝对值最大值，而不是最大值和最小值。</p>
<p>第三步：修改配置。也就是修改 <code>weights/config.ini</code> 文件，这个我们在《2.6.2 模型配置实践》中已经提到过了（KV int8 开关），只需要把 <code>quant_policy</code> 改为 4 即可。</p>
<p>这一步需要额外说明的是，如果用的是 TurboMind1.0，还需要修改参数 <code>use_context_fmha</code>，将其改为 0。</p>
<p>接下来就可以正常运行前面的各种服务了，只不过咱们现在可是用上了 KV Cache 量化，能更省（运行时）显存了。</p>
<h4 id="3-1-2-量化效果"><a href="#3-1-2-量化效果" class="headerlink" title="3.1.2 量化效果"></a>3.1.2 量化效果</h4><p>官方给出了 <a href="https://huggingface.co/internlm/internlm-chat-7b">internlm-chat-7b</a> 模型在 KV Cache 量化前后的显存对比情况，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>batch_size</th>
<th>fp16 memory(MiB)</th>
<th>int8 memory(MiB)</th>
<th>diff(MiB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>22337</td>
<td>18241</td>
<td>-4096</td>
</tr>
<tr>
<td>16</td>
<td>30593</td>
<td>22369</td>
<td>-8224</td>
</tr>
<tr>
<td>32</td>
<td>47073</td>
<td>30625</td>
<td>-16448</td>
</tr>
<tr>
<td>48</td>
<td>63553</td>
<td>38881</td>
<td>-24672</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，KV Cache 可以节约大约 20% 的显存。</p>
<p>同时，还在 <a href="https://github.com/open-compass/opencompass">opencompass</a> 平台上测试了量化前后的精准度（Accuracy）对比情况，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>task</th>
<th>dataset</th>
<th>metric</th>
<th>int8</th>
<th>fp16</th>
<th>diff</th>
</tr>
</thead>
<tbody>
<tr>
<td>Language</td>
<td>winogrande</td>
<td>accuracy</td>
<td>60.77</td>
<td>61.48</td>
<td>-0.71</td>
</tr>
<tr>
<td>Knowledge</td>
<td>nq</td>
<td>score</td>
<td>2.69</td>
<td>2.60</td>
<td>+0.09</td>
</tr>
<tr>
<td>Reasoning</td>
<td>gsm8k</td>
<td>accuracy</td>
<td>33.28</td>
<td>34.72</td>
<td>-1.44</td>
</tr>
<tr>
<td>Reasoning</td>
<td>bbh</td>
<td>naive_average</td>
<td>20.12</td>
<td>20.51</td>
<td>-0.39</td>
</tr>
<tr>
<td>Understanding</td>
<td>openbookqa_fact</td>
<td>accuracy</td>
<td>82.40</td>
<td>82.20</td>
<td>+0.20</td>
</tr>
<tr>
<td>Understanding</td>
<td>eprstmt-dev</td>
<td>accuracy</td>
<td>90.62</td>
<td>88.75</td>
<td>+1.87</td>
</tr>
<tr>
<td>Safety</td>
<td>crows_pairs</td>
<td>accuracy</td>
<td>32.56</td>
<td>31.43</td>
<td>+1.13</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，精度不仅没有明显下降，相反在不少任务上还有一定的提升。可能的原因是，量化会导致一定的误差，有时候这种误差可能会减少模型对训练数据的拟合，从而提高泛化性能。量化可以被视为引入轻微噪声的正则化方法。或者，也有可能量化后的模型正好对某些数据集具有更好的性能。</p>
<p>总结一下，KV Cache 量化既能明显降低显存占用，还有可能同时带来精准度（Accuracy）的提升。</p>
<h3 id="3-2-W4A16-量化"><a href="#3-2-W4A16-量化" class="headerlink" title="3.2 W4A16 量化"></a>3.2 W4A16 量化</h3><h4 id="3-2-1-量化步骤"><a href="#3-2-1-量化步骤" class="headerlink" title="3.2.1 量化步骤"></a>3.2.1 量化步骤</h4><p>W4A16中的A是指Activation，保持FP16，只对参数进行 4bit 量化。使用过程也可以看作是三步。</p>
<p>第一步：同 3.1.1，不再赘述。</p>
<p>第二步：量化权重模型。利用第一步得到的统计值对参数进行量化，具体又包括两小步：</p>
<ul>
<li>缩放参数。主要是性能上的考虑（回顾 PPT）。</li>
<li>整体量化。</li>
</ul>
<p>第二步的执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 量化权重模型</span><br>lmdeploy lite auto_awq \<br>  --model  /root/share/temp/model_repos/internlm-chat-7b/ \<br>  --w_bits 4 \<br>  --w_group_size 128 \<br>  --work_dir ./quant_output<br></code></pre></td></tr></table></figure>
<p>命令中 <code>w_bits</code> 表示量化的位数，<code>w_group_size</code> 表示量化分组统计的尺寸，<code>work_dir</code> 是量化后模型输出的位置。这里需要特别说明的是，因为没有 <code>torch.int4</code>，所以实际存储时，8个 4bit 权重会被打包到一个 int32 值中。所以，如果你把这部分量化后的参数加载进来就会发现它们是 int32 类型的。</p>
<p>最后一步：转换成 TurboMind 格式。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 转换模型的layout，存放在默认路径 ./workspace 下</span><br>lmdeploy convert  internlm-chat-7b ./quant_output \<br>    --model-format awq \<br>    --group-size 128<br></code></pre></td></tr></table></figure>
<p>这个 <code>group-size</code> 就是上一步的那个 <code>w_group_size</code>。如果不想和之前的 <code>workspace</code> 重复，可以指定输出目录：<code>--dst_path</code>，比如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy convert  internlm-chat-7b ./quant_output \<br>    --model-format awq \<br>    --group-size 128 \<br>    --dst_path ./workspace_quant<br></code></pre></td></tr></table></figure>
<p>接下来和上一节一样，可以正常运行前面的各种服务了，不过咱们现在用的是量化后的模型。</p>
<p>最后再补充一点，量化模型和 KV Cache 量化也可以一起使用，以达到最大限度节省显存。</p>
<h4 id="3-2-2-量化效果"><a href="#3-2-2-量化效果" class="headerlink" title="3.2.2 量化效果"></a>3.2.2 量化效果</h4><p>官方在 NVIDIA GeForce RTX 4090 上测试了 4-bit 的 Llama-2-7B-chat 和 Llama-2-13B-chat 模型的 token 生成速度。测试配置为 BatchSize = 1，prompt_tokens=1，completion_tokens=512，结果如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>model</th>
<th>llm-awq</th>
<th>mlc-llm</th>
<th>turbomind</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-7B-chat</td>
<td>112.9</td>
<td>159.4</td>
<td>206.4</td>
</tr>
<tr>
<td>Llama-2-13B-chat</td>
<td>N/A</td>
<td>90.7</td>
<td>115.8</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，TurboMind 相比其他框架速度优势非常显著，比 mlc-llm 快了将近 30%。</p>
<p>另外，也测试了 TurboMind 在不同精度和上下文长度下的显存占用情况，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>model(context length)</th>
<th>16bit(2048)</th>
<th>4bit(2048)</th>
<th>16bit(4096)</th>
<th>4bit(4096)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-7B-chat</td>
<td>15.1</td>
<td>6.3</td>
<td>16.2</td>
<td>7.5</td>
</tr>
<tr>
<td>Llama-2-13B-chat</td>
<td>OOM</td>
<td>10.3</td>
<td>OOM</td>
<td>12.0</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，4bit 模型可以降低 50-60% 的显存占用，效果非常明显。</p>
<p>总而言之，W4A16 参数量化后能极大地降低显存，同时相比其他框架推理速度具有明显优势。</p>
<h3 id="3-3-最佳实践"><a href="#3-3-最佳实践" class="headerlink" title="3.3 最佳实践"></a>3.3 最佳实践</h3><p>本节是针对《模型量化》部分的最佳实践。</p>
<p>首先我们需要明白一点，服务部署和量化是没有直接关联的，量化的最主要目的是降低显存占用，主要包括两方面的显存：模型参数和中间过程计算结果。前者对应《3.2 W4A16 量化》，后者对应《3.1 KV Cache 量化》。</p>
<p>量化在降低显存的同时，一般还能带来性能的提升，因为更小精度的浮点数要比高精度的浮点数计算效率高，而整型要比浮点数高很多。</p>
<p>所以我们的建议是：在各种配置下尝试，看效果能否满足需要。这一般需要在自己的数据集上进行测试。具体步骤如下。</p>
<ul>
<li>Step1：优先尝试正常（非量化）版本，评估效果。<ul>
<li>如果效果不行，需要尝试更大参数模型或者微调。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step2：尝试正常版本+KV Cache 量化，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step3：尝试量化版本，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step4：尝试量化版本+ KV Cache 量化，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，使用方案。</li>
</ul>
</li>
</ul>
<p>简单流程如下图所示。</p>
<p><img  src="quant.png"  ><span class="image-caption">量化流程图</span></p>
<p>另外需要补充说明的是，使用哪种量化版本、开启哪些功能，除了上述流程外，<strong>还需要考虑框架、显卡的支持情况</strong>，比如有些框架可能不支持 W4A16 的推理，那即便转换好了也用不了。</p>
<p>根据实践经验，一般情况下：</p>
<ul>
<li>精度越高，显存占用越多，推理效率越低，但一般效果较好。</li>
<li>Server 端推理一般用非量化版本或半精度、BF16、Int8 等精度的量化版本，比较少使用更低精度的量化版本。</li>
<li>端侧推理一般都使用量化版本，且大多是低精度的量化版本。这主要是因为计算资源所限。</li>
</ul>
<p>以上是针对项目开发情况，如果是自己尝试（玩儿）的话：</p>
<ul>
<li>如果资源足够（有GPU卡很重要），那就用非量化的正常版本。</li>
<li>如果没有 GPU 卡，只有 CPU（不管什么芯片），那还是尝试量化版本。</li>
<li>如果生成文本长度很长，显存不够，就开启 KV Cache。</li>
</ul>
<p>建议大家根据实际情况灵活选择方案。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://github.com/InternLM/lmdeploy/">InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs.</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/665725861">仅需一块 3090 显卡，高效部署 InternLM-20B 模型 - 知乎</a></li>
</ul>
<h2 id="附录1：TritonServer-作为推理引擎"><a href="#附录1：TritonServer-作为推理引擎" class="headerlink" title="附录1：TritonServer 作为推理引擎"></a>附录1：TritonServer 作为推理引擎</h2><h3 id="TritonServer环境配置"><a href="#TritonServer环境配置" class="headerlink" title="TritonServer环境配置"></a>TritonServer环境配置</h3><blockquote>
<p>注意：本部分内容仅支持物理机上执行，不支持虚拟主机。</p>
</blockquote>
<p>使用 Triton Server 需要安装一下 Docker 及其他依赖。</p>
<p>先装一些基本的依赖。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">apt-get update<br>apt-get install cmake sudo -y<br></code></pre></td></tr></table></figure>
<p>然后是 Docker 安装。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Add Docker&#x27;s official GPG key:</span><br>sudo apt-get install ca-certificates curl gnupg<br>sudo install -m 0755 -d /etc/apt/keyrings<br>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg<br>sudo <span class="hljs-built_in">chmod</span> a+r /etc/apt/keyrings/docker.gpg<br><br><span class="hljs-comment"># Add the repository to Apt sources:</span><br><span class="hljs-built_in">echo</span> \<br>  <span class="hljs-string">&quot;deb [arch=<span class="hljs-subst">$(dpkg --print-architecture)</span> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \</span><br><span class="hljs-string">  <span class="hljs-subst">$(. /etc/os-release &amp;&amp; echo <span class="hljs-string">&quot;<span class="hljs-variable">$VERSION_CODENAME</span>&quot;</span>)</span> stable&quot;</span> | \<br>  sudo <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/docker.list &gt; /dev/null<br>sudo apt-get update<br><br><span class="hljs-comment"># install</span><br>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin<br></code></pre></td></tr></table></figure>
<p>安装后我们跑一个 HelloWorld。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># helloworld</span><br>sudo docker run hello-world<br></code></pre></td></tr></table></figure>
<p>可以看到类似下面的画面，表示运行成功。</p>
<p><img  src="triton-server.png"  ><span class="image-caption">triton server</span></p>
<h3 id="TritonServer推理-API服务"><a href="#TritonServer推理-API服务" class="headerlink" title="TritonServer推理+API服务"></a>TritonServer推理+API服务</h3><blockquote>
<p>注意：这部分需要 Docker 服务。</p>
</blockquote>
<p>这里我们把提供模型推理服务的引擎从 TurboMind 换成了 TritonServer，启动命令就一行。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ApiServer+Triton</span><br>bash workspace/service_docker_up.sh<br></code></pre></td></tr></table></figure>
<p>这里会启动一个 TritonServer 的容器，如下图所示。</p>
<p><img src="triton-server-run.png" alt=""></p>
<p>可以再开一个窗口执行 Client 命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ChatTritonClient + TritonServer（注意是gRPC协议，不要用http）</span><br>lmdeploy serve triton_client  localhost:33337<br></code></pre></td></tr></table></figure>
<p>结果如下图所示。</p>
<p><img src="triton-client.png" alt=""></p>
<h3 id="TritonServer-服务作为后端"><a href="#TritonServer-服务作为后端" class="headerlink" title="TritonServer 服务作为后端"></a>TritonServer 服务作为后端</h3><p>使用过程同 2.4.1 小节。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Gradio+TritonServer（注意是gRPC协议，不要用http）</span><br>lmdeploy serve gradio localhost:33337 \<br>	--server_name 0.0.0.0 \<br>	--server_port 6006<br></code></pre></td></tr></table></figure>
<p>结果如下图所示。</p>
<p><img src="triton-client-web.png" alt=""></p>
<h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><h3 id="TurboMind推理-API服务"><a href="#TurboMind推理-API服务" class="headerlink" title="TurboMind推理+API服务"></a>TurboMind推理+API服务</h3><p><img  src="story.png"  ><span class="image-caption">部署并使用 api 生成 300 字小故事</span></p>
<h3 id="TurboMind-推理-命令行本地对话"><a href="#TurboMind-推理-命令行本地对话" class="headerlink" title="TurboMind 推理+命令行本地对话"></a>TurboMind 推理+命令行本地对话</h3><p><img  src="chat.png"  ><span class="image-caption">与本地部署的模型进行对话</span></p>
<h3 id="TurboMind-推理-gradio"><a href="#TurboMind-推理-gradio" class="headerlink" title="TurboMind 推理+gradio"></a>TurboMind 推理+gradio</h3><p><img  src="gradio.png"  ><span class="image-caption">Gradio 部署</span></p>
<h3 id="KV-Cache-量化部署"><a href="#KV-Cache-量化部署" class="headerlink" title="KV Cache 量化部署"></a>KV Cache 量化部署</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 计算 minmax</span><br>lmdeploy lite calibrate \<br>  --model  /root/share/temp/model_repos/internlm-chat-7b/ \<br>  --calib_dataset <span class="hljs-string">&quot;c4&quot;</span> \<br>  --calib_samples 128 \<br>  --calib_seqlen 2048 \<br>  --work_dir ./quant_output<br></code></pre></td></tr></table></figure>
<p>这里在计算的时候需要下载 calibrate 数据集 c4，国内经常不成功。所以需要手动下载后对读取数据集的代码文件做一下替换。共包括两步：</p>
<ul>
<li>第一步：复制 <code>calib_dataloader.py</code> 到安装目录替换该文件：<code>cp /root/share/temp/datasets/c4/calib_dataloader.py  /root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/utils/</code></li>
<li>第二步：将用到的数据集（c4）复制到下面的目录：<code>cp -r /root/share/temp/datasets/c4/ /root/.cache/huggingface/datasets/</code></li>
</ul>
<p><img  src="max.png"  ><span class="image-caption">计算每一层的最大 GPU 占用</span></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 通过 minmax 获取量化参数</span><br>lmdeploy lite kv_qparams \<br>  --work_dir ./quant_output  \<br>  --turbomind_dir workspace/triton_models/weights/ \<br>  --kv_sym False \<br>  --num_tp 1<br></code></pre></td></tr></table></figure>
<p><img  src="qparam.png"  ><span class="image-caption">获取量化参数</span></p>
<p>之后修改 <code>weights/config.ini</code> 文件 <code>quant_policy=4</code> 表示开启 KV Cache 量化。</p>
<p>部署运行内存占用</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">watch vgpu-smi<br></code></pre></td></tr></table></figure>
<p><img  src="kv-cache-memory.png"  ><span class="image-caption">KV Cache 内存占用</span></p>
<h3 id="W4A16-量化"><a href="#W4A16-量化" class="headerlink" title="W4A16 量化"></a>W4A16 量化</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 计算 minmax</span><br>lmdeploy lite calibrate \<br>  --model  /root/share/temp/model_repos/internlm-chat-7b/ \<br>  --calib_dataset <span class="hljs-string">&quot;c4&quot;</span> \<br>  --calib_samples 128 \<br>  --calib_seqlen 2048 \<br>  --work_dir ./quant_output_awq<br></code></pre></td></tr></table></figure>
<p>之后量化权重</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 量化权重模型</span><br>lmdeploy lite auto_awq \<br>  --model  /root/share/temp/model_repos/internlm-chat-7b/ \<br>  --w_bits 4 \<br>  --w_group_size 128 \<br>  --work_dir ./quant_output_awq<br></code></pre></td></tr></table></figure>
<p>命令中 <code>w_bits</code> 表示量化的位数，<code>w_group_size</code> 表示量化分组统计的尺寸，<code>work_dir</code> 是量化后模型输出的位置。这里需要特别说明的是，因为没有 <code>torch.int4</code>，所以实际存储时，8个 4bit 权重会被打包到一个 int32 值中。所以，如果你把这部分量化后的参数加载进来就会发现它们是 int32 类型的。</p>
<p><img  src="awq.png"  ><span class="image-caption">AWQ 量化</span></p>
<p>最后一步：转换成 TurboMind 格式。</p>
<p>这个 <code>group-size</code> 就是上一步的那个 <code>w_group_size</code>。如果不想和之前的 <code>workspace</code> 重复，可以指定输出目录：<code>--dst_path</code>，比如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy convert  internlm-chat-7b ./quant_output_awq \<br>    --model-format awq \<br>    --group-size 128 \<br>    --dst_path ./workspace_quant_awq4<br></code></pre></td></tr></table></figure>
<p><img  src="convert-awq.png"  ><span class="image-caption">转换为 TurboMind 格式</span></p>
<p>目录结构如下：</p>
<p><img  src="tree-awq.png"  ><span class="image-caption">转换为 TurboMind 格式的 AWQ 模型结构</span></p>
<p>部署使用</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy chat turbomind ./workspace_quant_awq4<br></code></pre></td></tr></table></figure>
<p>内存占用</p>
<p><img  src="awq-memory.png"  ><span class="image-caption">AWQ 量化内存占用</span></p>
<p>可见内存占用只有 6G。</p>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>爱的艺术笔记</title>
    <url>/uncategorized/%E7%88%B1%E7%9A%84%E8%89%BA%E6%9C%AF/</url>
    <content><![CDATA[<h1 id="来自-Async-的-Tips"><a href="#来自-Async-的-Tips" class="headerlink" title="来自 Async 的 Tips"></a>来自 Async 的 Tips</h1><p>注意，不要过度，我们不必要太过迁就和顺从任何人。</p>
<p>人们不会珍惜轻易得到的东西，这些东西包括爱。</p>
<h1 id="爱是一门艺术"><a href="#爱是一门艺术" class="headerlink" title="爱是一门艺术"></a>爱是一门艺术</h1><h2 id="爱重要的是能力"><a href="#爱重要的是能力" class="headerlink" title="爱重要的是能力"></a>爱重要的是能力</h2><p>如果爱是一门艺术，那就要求人们有这方面的知识并付出努力。</p>
<p>大多数人认为爱情首先是自己能否被人爱，而不是自己有没有能力爱的问题。</p>
<p>因此对他们来说，关键是：我会被人爱吗？我如何才能值得被人爱？为了达到这一目的，他们采取了各种途径。男子通常采取的方法是在其社会地位所允许的范围内，尽可能地去获得名利和权力，而女子则是通过保持身段和服饰打扮使自己富有魅力；</p>
<p>而男女都喜欢采用的方式则是使自己具有文雅的举止，有趣的谈吐，乐于助人，谦虚和谨慎。</p>
<p>为了使自己值得被人爱而采用的许多方法与人们要在社会上获得成功所采用的方法雷同，即都是“要赢得朋友和对他人施加影响”。事实上，我们这个社会大多数人所理解的“值得被人爱”无非是赢得人心和对异性有吸引力这两种倾向的混合物而已。</p>
<p>产生在爱这件事上一无可学这一看法的第二个原因是人们认为爱的问题是一个对象问题，而不是能力问题。他们认为爱本身十分简单，困难在于找到爱的对象或被爱的对象。</p>
<p>一些事实：</p>
<ul>
<li><p>人与人之间的爱情关系也遵循同控制商品和劳动力市场一样的基本原则——等价交换。</p>
</li>
<li><p>人们往往把这种如痴如醉的入迷，疯狂的爱恋看作是强烈爱情的表现，而实际上这只是证明了这些男女过去是多么地寂寞。</p>
</li>
</ul>
<p>要认识爱情是一门艺术。人们要学会爱情，就得像学其他的艺术——如音乐，绘画，木工或者医疗艺术和技术一样的行动。</p>
<h1 id="爱情理论"><a href="#爱情理论" class="headerlink" title="爱情理论"></a>爱情理论</h1><h3 id="一、爱情是对人类生存问题的回答"><a href="#一、爱情是对人类生存问题的回答" class="headerlink" title="一、爱情是对人类生存问题的回答"></a>一、爱情是对人类生存问题的回答</h3><p>爱情的每一个理论必须要以人的理论、人的生存理论为前提。我们所能看到的动物的爱情或者更确切地说动物身上类似爱情的东西，主要是动物的一部分本能。在人身上只能看到这一本能的残余。人的存在的根本要点是人超越了动物界，超越了本能的适应性，脱离了自然——尽管人永远不可能完全脱离自然。</p>
<p>经历过孤寂的人必然会有恐惧感。实际上孤寂感是每种恐惧的根源。孤寂意味着与外界没有联系，不能发挥人的力量，意味着一筹莫展，不能把握世界，事物和人；意味着世界把我淹没，而我只能听之任之。所以孤寂是引起强烈恐惧感的根源，同时孤寂还会引起羞愧和负罪的感觉。</p>
<p>因此对人来说最大的需要就是克服他的孤独感和摆脱孤独的监禁。人在达到这一目的过程中的完全失败就会导致人的疯狂，因为人只有通过完全彻底地脱离周围世界，以至于不再感到与世隔绝，他对彻底孤独的恐惧感才会得到克服——因为他与之隔绝的世界从他的生活中消失了。</p>
<p>人——所有时代和生活在不同文化之中的人——永远面临同一个问题，即：如何克服这种孤独感，如何超越个人的天地，实现人类的大同。</p>
<p>人们对这一问题的回答在一定的范围则取决于人所达到的个性的高度。在一个孩子身上，“我”字几乎还没有形成。每个孩子都同母亲融为一体，只要母亲在他身旁，他就不会有孤独感。他的孤独感通过母亲的存在，同母亲的乳房和肌肤的接触而得到和缓。一直到孩子发育到产生孤独感和个性这个阶段，母亲的存在才不足以消除他的孤独感，他必须以其他的方法克服这种孤独感。</p>
<p>人类在孩提时代几乎是以同样的方式体验与大自然的和谐。大地、动物和植物完全是人的世界。人把自己看作和动物完全一致，这表现在人装扮成动物以及崇拜图腾或其他的动物神。但人类越脱离原始的纽带，就越疏远自然世界，就越要寻找摆脱孤独的途径。</p>
<p>达到这一目的的一种途径是不同形式的纵欲。例如自我引起——或借助于毒品——的恍惚状态就是一种形式的纵欲。</p>
<p>就是最穷的罗马人也能自豪地说我是罗马的公民罗马和罗马帝国就是他的家，他的祖国和世界。在今日的西方社会，同一组人结合仍然是克服孤独感最常用的方法。在这种结合中，参加者为了使自己属于这一组人而失去了大部分个性。如果我与他人完全一样，我的感情、思想与他人一致，我的衣着、习惯和看法都与这一组人的楷模看齐，我就可得救，就不会再经历可怕的孤独。</p>
<p>在当今资本主义社会，平等的概念发生了变化。今天“平等”指的是机器——也就是失去个性的人的平等。平等意味着“一个模式”而不是“统一”。这是一个抽象体的同一模式，是做同样的工作、寻求同样的享受，读同样的报纸，有同样的思想感情的人的模式。</p>
<p>现代社会鼓吹实现非个性化的平等理想，因为这个社会需要“人——原子”，这些人原子相互之间没有区别，汇集起来也能毫无摩擦地顺利地发挥作用，他们都服从同一个命令，尽管如此，每个人却都确信他们是在按自己的意愿办事。就象现代化的大规模生产要求产品规格化，社会的发展也要求人的规格化，并把这称为“平等”。</p>
<p>通过同一化达到人与人的结合既不是强烈的也不是激烈的过程，而是按照一个刻板的公式十分平静地进行。</p>
<p>我们是把爱情看作是对人类生存问题的成熟回答，还是指爱情的不成熟形式，也就是人们称为共生有机体的结合呢？下面我只是把前者看作是爱情，但我们的讨论却要从后者开始。</p>
<p>共生有机体结合的生物楷模是怀孕的母亲同胚胎之间的关系。他们既是两体，又是一体。他们生活在一起(共生)，他们相互需要。胎儿是母亲的一部分，并从她身上得到他所需要的一切，所以母亲就是他的世界。她抚养和保护胎儿，同时她自己的生活也因胎儿的存在而得到一种升华。在生理性的共生有机体的结合中，两者的身体互不依赖，但在心理上却相互依赖。</p>
<p>共生有机体结合的消极形式是服从——医学名词就是被虐癖。有被虐癖的人通过把自己变成他的引导者、启示者、保护者的一部分使自己摆脱孤独和与世隔绝感。保护者就是他的生命，没有保护者他就无法生存。不论保护者是人还是神，总之他的威力超越一切。他主宰一切，而自己什么也不是，被虐淫者必须成为其保护者的一部分，因为只有这样他才能分享保护者的伟大、威力和安全。被虐淫者从来不作任何决定和进行任何冒险，他从不孤独，但也决不独立。他不是一个完整的人，可以说还没有完全诞生。在宗教的语言中把崇拜的对象称为偶像，而被虐淫者对其保护者的崇拜超过了对偶像的崇拜。这种崇拜可以同生理要求和性要求相混合，在这种情况下被虐淫者的服从不仅是想象力的产物，而且是一种与全身有关的生理需要。另外还有屈服于命运、疾并有节奏感的音乐或者屈服于由吸毒和催眠状态引起的极度兴奋等病状——在所有这些情况下，犯病的人失去了他的完整性，把自己成为一个人或一件事物的工具；从而使他不用对生存的问题作出独立的和自由的回答。</p>
<p>共生有机体结合的积极形式是控制另一个人——与被虐癖相应的医学名词是施虐癖。施虐淫者就是通过把另一个人成为他自己的一部分而摆脱孤独，他吞并他的崇拜者，从而使自己身价百倍。</p>
<p>就象被他控制的人脱离不了他一样，施虐淫者也离不开他的崇拜者，双方都不能失去对方。区别只在于——施虐淫者命令、利用、损害和欺压对方，而对方则乐于被他左右。从现实的角度来看似乎他俩之间存在着很大的差别，但从更深的意义来看他俩的区别不比他俩的共同点重要，他俩的共同点是在结合的过程中双方都失去其独立性行完整性。如果我们理解这一点，就不难确定一般来说一个人会根据不同的对象作出施虐癖和被虐癖的反应。希特勒对其他人首先是施虐淫者，但面对他的命运、历史和自然的“威力”作出的却是被虐淫者的反应。</p>
<p>同共生有机体结合相对立的是成熟的爱情，那就是在保留自己完整性和独立性的条件下，也就是保持自己个性的条件下与他人合二为一。人的爱情是一种积极的力量，这种力量可以冲破人与人之间的高墙并使人与人结合。爱情可以使人克服孤寂和与世隔绝感，但同时又使人保持对自己的忠诚，保持自己的完整性和本来的面貌。在爱情中出现了两个生命合为一体，却依然保持两体的怪现象。</p>
<p>如果我们说，爱情是一项“积极的活动”，我们就会遇到“积极的活动”这个词有双重意义的问题。这个词的现代用法一般就是指人们通过付出劳动改变现存状态的行为。所以经商的人，学医的人，流水作业线上的工人，做椅子的木匠或者运动员都是积极活动的人。他们活动的共同点都是为了达到一个外部的目的。但这里我们都没有考虑产生积极性的根源。我们可以举一个例子加以说明。</p>
<p>有的人由于内心极度的不安或者孤独而狂热地工作，有的人则是为了升官发财。在这种情况下这个人就是一种狂热、一种热情的奴隶，而他的“积极性”实际上是一种“消极性”，因为他是受外力的驱使。他是一个受苦的人，而不是一个“行动”的人。另一方面人们往往把一个坐在椅子上沉思默想、观察和体验自己以及自己同世界关系的人看作是“消极的”，因为他什么也不“干”。实际上这种精神高度集中的禅坐是最高的积极性，是灵魂的积极性，只有那些内心自由和独立的人才能做到这点。“积极的活动”这一概念的一个意义，也就是现代应用的意义是指为了达到外部的目的而付出努力。这个词的另一个意义是运用人的蕴藏在内部的力量，不管是否达到外部的变化。斯宾诺莎精辟地解释了这个词的第二种意义。他把情绪分成积极的和消极的两种，分成“行动”和“狂热”。如果一个人是在积极的情绪支配下行动，他就是自由的，是情绪的主人。如果他是被一种消极的情绪所支配，那他就是受外力驱使者，是他自己都不了解的动机的对象。这样，斯宾诺莎最终得出结论认为，美德和控制自己是一回事。妒忌、野心和每种形式的贪婪是热情和狂热；相反爱情是一种行动，是运用人的力量，这种力量只有在自由中才能得到发挥，而且永远不会是强制的产物。</p>
<p>爱情是一种积极的，而不是消极的情绪。一般来说可以用另一个说法来表达，即爱情首先是给而不是得。</p>
<p>什么是“给”？这个问题看起来似乎很容易回答，实际上却很复杂并有双层意义。十分流行的误解是把“给”解释为放弃，被别人夺走东西或作出牺牲。一个性格还没有超越接受、利用或者贪婪阶段的人对给的理解就是这样。一个“重商主义”的人也准备给，但一定要通过交换。只“给”而没有“得”对他来说就是欺骗。那些基本上是非生产性性格结构的人则会有一种被别人拿走东西的感觉。因此这种类型的大多数人拒绝给予别人东西。而有些人却又把“给”变成一种自我牺牲的美德。他们认为，正因为“给”是痛苦的，所以应该这么做。给的美德就是准备牺牲，对他们来说，“给”比“得”好这一准则就是意味着宁可忍受损失也不要体验快乐。</p>
<p>有创造性的人对“给”的理解完全不同。他们认为“给”是力量的最高表现，恰恰是通过“给”，我才能体验我的力量，我的“富裕”，我的“活力”。体验到生命力的升华使我充满了欢乐。我感觉到自己生气勃勃，因而欣喜万分。“给”比“得”带来更多的愉快，这不是因为“给”是一种牺牲，而是因为通过“给”表现了我的生命力。</p>
<p>如果我们把这一原则用来解释各种特殊的现象，就不难认识这一原则的有效性。最基本的例子可以在性范畴里找到。男子性行为的最高峰就是一种给的行为：男子把自己的性器官交给女子，在达到性高潮的一刹那，他把精液给予对方。只要他不是阳痿，他就必须这么做。如果他不能给，他就是阳痿。女子也是如此，只不过表现形式复杂一点罢了。女子交出自己，她打开通向女性内部的大门，在接受的同时她也给予，如果她没有能力给，而只能得，她就是性冷淡。在女子身上给的行为还表现在她作为母亲的作用上。她把她的养料给予她肚中的胎儿，后来又给婴儿喂奶和给予母体的温暖。对女子来说不能给是极其痛苦的。</p>
<p>在物质世界范畴内给是财富。不是拥有财物的人是富裕的，而是给予他人东西的人才是富裕者。害怕受到损失的吝啬鬼，不管他拥有多少财产，从心理学角度来看，他是一个贫穷和可怜的人。愿意把自己的东西给予他人的人却是富有的，他感觉到自己是一个有能力帮助别人的人。只有那些连生活必需品都没有的人才不能体验帮助别人的乐趣。但是日常生活经验告诉我们，衡量有没有足够生活必需品的标准既取决于人的实际财产，也取决于人的性格本质。众所周知穷人往往比富人更愿意给。尽管如此，超过一定限度的贫困往往使许多人无法给，恰恰这一点是十分令人懊丧的——这不仅仅是因为从中可以看到穷人的贫困，同时也是因为穷人被剥夺了给所带来的欢乐。</p>
<p>但给的最重要范畴还不是物质范畴，而是人所具有的特殊范畴。一个人究竟能给予别人什么呢？他可以把他拥有的最宝贵的东西，他的生命给予别人。但这并不一定意味着他一定要为别人献出自己的生命，而是他应该把他内心有生命力的东西给予别人。他应该同别人分享他的欢乐、兴趣、理解力、知识、幽默和悲伤——简而言之一切在他身上有生命力的东西。通过他的给，他丰富了他人，同时在他提高自己生命感的同时，他也提高了对方的生命感。他给并不是为了得，但是通过他的给，不可避免地会在对方身上唤起某种有生命力的东西。因此他的给同时也包括了使接受者也成为一个给的人，而双方都会因为唤醒了内心的某种生命力而充满快乐。在给的行为中诞生了新的东西，给和得的人都会感谢这新的力量。这一点表现在爱情上就是：没有生命力就是没有创造爱情的能力。马克思极其优美地表达了上述思想。他说：“如果你以人就是人以及人同世界的关系是一种充满人性的关系为先决条件，那么你只能用爱去换爱，用信任换取信任。如果你想欣赏艺术，你必须是一个有艺术修养的人；如果你想对他人施加影响，你必须是一个有艺术修养的人；如果你想对他人施加影响，你必须是一个能促进和鼓舞他人的人。你同人及自然的每一种关系必须是你真正个人生活的一种特定的、符合你的意志对象的表现。如果你在爱别人，但却没有唤起他人的爱，也就是你的爱作为一种爱情不能使对方产生爱情，如果作为一个正在爱的人你不能把自己变成一个被人爱的人，那么你的爱情是软弱无力的，是一种不幸。”</p>
<p>不仅在爱情上“给”意味着“得”。教师向他的学生学习，演员受到观众的鼓舞，精神分析学家通过治愈他人的病而治愈自己的病也都如此，先决条件是给的人不应该把对方看作是他帮助的对象，而应该同对方建立一种真正的、创造性的紧密关系。</p>
<p>有没有能力把爱情作为一种给的行为取决于人的性格发展，这一事实似乎没有必要加以强调了。取得这一能力的先决条件是人要有一种占主导地位的生产性倾向。持有这种态度的人就克服了他的依赖性、自恋性（“自恋”这一概念是由弗洛伊德提出的，弗洛伊德把人的自我欣赏叫做“自恋”，他认为，“自恋”必先于他恋，这主要表现在儿童把与生俱来的里比多（指心能，尤其是性本能的能）用到自己身上。——译者注）以及剥削别人的要求，并能找到对自己的人性力量的信赖以及达到目的的勇气。如果缺乏这些特点，人们就害怕献出自己，也就是害怕去爱。</p>
<p>爱情的积极性除了有给的要素外，还有一些其他的基本要素。这些要素是所有爱的形式共有的，那就是：<strong>关心、责任心、尊重和了解</strong>。</p>
<p>在母爱中关心的要素表现得最为突出。如果有一个母亲拒绝给孩子喂食、洗澡和关心他身体的舒适，那么无论这位母亲如何强调她对孩子的爱，也不会有人相信她。但如果她关心孩子，她的爱就令人可信了。对动物和植物的爱亦是如此。如果有一位妇女对我们说她很爱花，可是我们却发现她忘记浇花，我们就不会相信她说的话。爱情是对生命以及我们所爱之物生长的积极的关心。如果缺乏这种积极的关心，那么这只是一种情绪，而不是爱情。爱情的这一要素在《约拿书》中得到很美的描绘。上帝吩咐约拿去尼尼微，向那里的居民宣布，如果他们不改邪归正，他们就将受到惩罚。约拿却不愿行使这一使命，他逃跑了，因为他担心尼尼微的居民将会悔过，从而求得上帝的宽恕。约拿是一个执法从严的人，但不是一个爱人之人。在他逃亡的路上，他发现自己躲在一条大鱼的肚子里，这条大鱼象征着隔绝和监禁，正是由于约拿缺乏仁爱和恻隐之心，所以才被送到这儿。上帝拯救了他，约拿去到尼尼微，向那里的居民宣告上帝的话，这时正如约拿担心的那样，尼尼微的居民回心转意，虔诚忏悔，上帝原谅了他们，答应不使全城覆没。约拿大为不悦和失望，他要看到“正义”，而不是仁爱。最后他坐在一棵树的阴影底下重又找回失去的安宁。这棵树本是上帝让它长高，好替约拿遮挡灼热的阳光。这时上帝却让这棵树枯死了，约拿十分沮丧，埋怨上帝。上帝回答说：“你为那棵一夜长、一夜死的树惋惜，虽然你既没有栽活它，也没有关心它。为什么我就不能惋惜尼尼微城内那十二万好坏不分的居民和那许许多多的动物呢？”上帝向约拿解释道，爱的本质是创造和培养，爱情和劳动是不可分割的。人们爱自己劳动的成果，人们为所爱之物而劳动。</p>
<p>关心和关怀还包括爱情的另一方面，即责任心。今天人们常常把责任心理解为是义务，是外部强加的东西。但是责任心这个词的本来意义是一件完全自觉的行动，是我对另一个生命表达出来或尚未表达出来的愿望的答复。“有责任”意味着有能力并准备对这些愿望给予回答。约拿对尼尼微的居民没有责任心，像该隐一样，他同样会提出这一问题“难道我应该是我弟弟的看守吗？”。一个爱的人的回答是，我兄弟的生命不仅与他自己有关，而且也同我有关。我应对其他的人负责就像对自己负责一样。这种责任心在母子关系中主要表现在母亲对孩子生理上的要求的关心。在成人之间则也包括关心对方的精神要求。</p>
<p>如果爱情没有第三个要素：尊重，那责任心就很容易变成控制别人和奴役别人。尊重别人不是惧怕对方。尊重这个词的出处就是有能力实事求是地正视对方和认识他独有的个性。尊重就是要努力地使对方能成长和发展自己，因此尊重决无剥削之意。我希望一个被我爱的人应该以他自己的方式和为了自己去成长、发展，而不是服务于我。如果我爱他人，我应该感到和他一致，而且接受他本来的面目，而不是要求他成为我希望的样子，以便使我能把他当作使用的对象。只有当我自己达到独立，在没有外援的情况下独立地走自己的路，即不想去控制和利用别人，只有在这种情况下，尊重对方才成为可能。只有在自由的基础上才会有爱情，正像在一首古老的法国歌曲中唱的那样“爱情是自由之子，永远不会是控制的产物”。</p>
<p>人们只有认识对方，了解对方才能尊重对方。如果不以了解为基础，关心和责任心都会是盲目的，而如果不是从关怀的角度出发去了解对方，这种了解也是无益的。了解的方式多种多样。成为爱情一要素的了解是要深入事物的内部，而不是满足于一知半解。我只有用他人的眼光看待他人，而把对自己的兴趣退居二位。我才能了解对方。譬如：我可以知道这个人在生气，即使他自己不表露出来。但我还可以更进一步地去了解他，然后就知道，他很害怕和不安，他感到孤独和受到良心的谴责。这样我就明白他的生气只是他内部更深的东西的反映，这时我眼中的他不再是一个发怒的人，而是一个处在恐惧和惶恐不安之中的受苦的人。</p>
<p>了解同爱情还有另一个基本的关系。希望同另一个人结合以逃避自我孤独的监禁同另一个完全符合人性的愿望有紧密的联系，那就是认识“人的秘密”。生命从其纯生物的角度来看是一个奇迹和秘密，而在人的范围内每个人对自己和对别人都是一个不可解答的秘密。我们认识自己，但尽管作了一切努力还是不认识自己，我们认识他人，但我们还是不认识他们，因为我们和他们都不是一回事。我们越深入我们生命的深处或另一个人的生命深处，我们离认识生命的目标就越远。尽管如此，我们不能阻止这种深入了解人的灵魂的秘密、了解人的核心，即“自我”的愿望将继续存在。</p>
<p>有一种可以认识这一秘密的令人绝望的可能性——那就是拥有掌握对方的全部权力，利用这种权力我可以随心所欲地支配他，让他按照我的意志去感受，去思想，把他变成一样东西，变成我的东西，我的财产。在这方面最明显的表现就是施虐淫者的极端作法，施虐淫者要求并能使一个人受苦，他折磨和迫使那个人泄露他的秘密。要求发现人的秘密是恣意暴行和破坏狂的基本动机。艾萨克——巴比尔(艾萨克——巴比尔(1894——1941)，苏联作家。——译者注)很清楚地表达了这一思想。他摘引俄国国内战争时一个军官的话，这个军官刚刚把他过去的主人踩死。军官说：“用一颗子弹——我想说——用一颗子弹只能把这个家伙干掉……开枪是永远不能深入他的灵魂，到达他作为一个人和有灵魂的地方。但我毫无顾忌，我已经不止一次踩死敌人，每次都超过一个小时。你知道吗——我想知道，生命到底是什么，我们天天遇到的生命到底是什么？”</p>
<p>在孩子身上我们经常能看到这条通向知识的捷径。孩子随手拿起一样东西，把它弄坏，以便认识这样东西。譬如他抓到一个蝴蝶，就很残忍地把翅膀折断，他要认识蝴蝶，迫使它交出自己的秘密。在这儿残暴有一个较深的动机：那就是希望认识事物和生命的秘密。</p>
<p>认识秘密的另一条途径是爱情。爱情是积极深入对方的表现。在这一过程中，我希望了解秘密的要求通过结合得到满足。在结合的过程中，我认识对方，认识自己，认识所有的人，但还是“一无所知”。我对生命的了解不是通过思想传导的知识，而是通过人唯一可以使用的方式——通过人与人的结合。施虐癖的产生是为了了解秘密，但却一无所得。我把一个生命一块一块的解体，我所能达到的就是这一生命被破坏。只有爱情才能带给我知识，在结合的过程中回答我提出的问题。在爱情中，在献身中，在深入对方中，我找到了自己，发现了自己，发现了我们双方，发现了人。</p>
<p>德尔斐的箴言“认识你自己”表达了我们要求认识自己和他人的愿望。这是全部心理学的渊源。因为这一愿望是要认识完整的人，认识他内心最深处的秘密，所以通常的知识，由思想传导的知识不能满足这一愿望。即使我们对自己的了解比现在高出一千倍，也不可能深入事物的最本质的东西。我们对自己是一个迷，别人对我们来说也永远会是一个迷。达到全部了解人的唯一途径是思想上的认识，也就是心理学的知识是实现通过爱情达到全面了解的一个条件。我必须客观地去认识对方和自己，以便使自己能够看到对方的现实状态或者能够克服幻想，克服我想象中的被歪曲了的他的图像。我只有客观地认识一个人，我才能在爱中了解他的真正本质。</p>
<p>另外我们还知道，我们永远不可能靠智力来了解人和宇宙的秘密，但可以通过爱情去把握它。心理学作为一门科学有其局限性。就像神学的逻辑结论是神秘主义，心理学的最终结论就是爱。</p>
<p>关心、责任心、尊重和了解是相互依赖的。在成熟的人身上可以看到这些态度的集中表现。成熟的人就是指能够创造性地发挥自己力量的人。成熟的人只想拥有他自己的劳动果实，放弃了获取全力和全知的自恋幻想，并有一种谦恭的态度。这一态度的基础是他内心的力量，单单这股力量就能使他进行真正的、创造性的劳动。</p>
<h3 id="二、父母和孩子之间的爱"><a href="#二、父母和孩子之间的爱" class="headerlink" title="二、父母和孩子之间的爱"></a>二、父母和孩子之间的爱</h3><p>母爱就其本质来说是无条件的。母亲热爱新生儿，并不是因为孩子满足了她的什么特殊的愿望，符合她的想象，而是因为这是她生的孩子。（我在这里提到的母爱或者父爱都是指“理想典型”，也就是马克斯——韦伯提到了的或者荣格的原型意义上的理想典型，而不是指每个母亲和每个父亲都以这种方式爱孩子。我更多的是指在母亲和父亲身上体现的那种本质。)无条件的母爱不仅是孩子，也是我们每个人最深的渴求。从另一个角度来看通过努力换取的爱往往会使人生疑。人们会想：也许我并没有给那个应该爱我的人带来欢乐，也许会节外生枝——总而言之人们害怕这种爱会消失。此外靠努力换取的爱常常会使人痛苦地感到：我之所以被人爱是因为我使对方快乐，而不是出于我自己的意愿——归根结蒂我不是被人爱，而是被人需要而已。鉴于这种情况，因此我们所有的人，无论是儿童还是成年人都牢牢地保留着对母爱的渴求，是不足为奇的。大多数的孩子有幸得到母爱(我们以后再谈在什么程度上得到母爱。）而成人身上的这种渴望更难得到实现。在令人满意的发展过程中，这种渴望始终是性爱的一个成分；但也经常出现在宗教形式，或者更多的是出现在神经病形式中。</p>
<p>同父亲的关系则完全不同。母亲是我们的故乡，是大自然、大地和海洋。而父亲不体现任何一种自然渊源。在最初几年内孩子同父亲几乎没有什么联系，在这个阶段父亲的作用几乎无法同母亲相比。父亲虽然不代表自然世界，却代表人类生存的另一个极端：即代表思想的世界，人所创造的法律、秩序和纪律等事物的世界。父亲是教育孩子，向孩子指出通往世界之路的人。</p>
<p>同父亲作用紧密相关的是另一个同社会经济发展有关的作用。随着私有制以及财产由一个儿子继承的现象出现，父亲就对那个将来要继承他财产的人特别感兴趣。父亲总是挑选他认为最合适的儿子当继承人，也就是与他最相像，因而也是最得他欢心的那个儿子。父亲是有条件的爱。父亲的原则是：“我爱你，因为你符合我的要求，因为你履行你的职责，因为你同我相像。”正如同无条件的母爱一样，有条件的父亲有其积极的一面，也有其消极的一面。消极的一面是父爱必须靠努力才能赢得，在辜负父亲期望的情况下，就会失去父爱。父爱的本质是：顺从是最大的道德，不顺从是最大的罪孽，不顺从者将会受到失去父爱的惩罚。父爱的积极一面也同样十分重要。因为父爱是有条件的，所以我可以通过自己的努力去赢得这种爱。与母爱不同，父爱可以受我的控制和努力的支配。</p>
<p>父母对孩子的态度符合孩子的要求。婴儿无论从身体还是心理上都需要母亲的无条件的爱和关怀。在六岁左右孩子就需要父亲的权威和指引。母亲的作用是给予孩子一种生活上的安全感，而父亲的任务是指导孩子正视他将来会遇到的种种困难。一个好母亲是不会阻止孩子成长和不会鼓励孩子求援的。母亲应该相信生活，不应该惶恐不安并把她的这种情绪传染给孩子。她应该希望孩子独立并最终脱离自己。父爱应该受一定的原则支配并提出一定的要求，应该是宽容的、耐心的，不应该是咄咄逼人和专横的。父爱应该使孩子对自身的力量和能力产生越来越大的自信心，最后能使孩子成为自己的主人，从而能够脱离父亲的权威。一个成熟的人最终能达到他既是自己的母亲，又是自己的父亲的高度。他发展了一个母亲的良知，又发展了一个父亲的良知。母亲的良知对他说：“你的任何罪孽，任何罪恶都不会使你失去我的爱和我对你的生命、你的幸福的祝福。”父亲的良知却说：“你做错了，你就不得不承担后果；最主要的是你必须改变自己，这样你才能得到我的爱。”成熟的人使自己同母亲和父亲的外部形象脱离，却在内心建立起这两个形象。同弗洛伊德的“超我”理论相反，人不是通过合并父亲和母亲，从而树立起这两个形象，而是把母亲的良知建筑在他自己爱的能力上，把父亲的良知建筑在自己的理智和判断力上。成熟的人既同母亲的良知，又同父亲的良知生活在一起，尽管两者看上去互为矛盾。如果一个人只发展父亲的良知，那他会变得严厉和没有人性；如果他只有母亲的良知，那他就有失去自我判断力的危险，就会阻碍自己和他人的发展。</p>
<p>人从同母亲的紧密关系发展到同父亲的紧密关系，最后达到综合，这就是人的灵魂健康和达到成熟的基础如果人不是这么发展就会导致神经玻限于篇幅，我不可能在这儿详细解释我的这一观点，只能简单扼要地提一下。</p>
<p>譬如造成神经病的一个原因可能是一个男孩有一个十分慈爱，却又很娇惯他的母亲，同时又有一个性格懦弱或者对孩子不感兴趣的父亲。在这种情况下，小男孩会牢牢地抓住同母亲的联系，发展成为一个十分依赖母亲的人。这种人往往孤立无援，需要得到保护，不可能获得父亲的一些特点：如纪律、独立性和驾驭生活的能力。他就会企图在所有的人身上寻找“母亲”的形象，有时在妇女身上，有时在有权威的男子身上。反之，如果母亲性情冷淡、麻木不仁或者十分专制，孩子就会把对母爱的需要转移到父亲身上，就会变成单一的向父亲方向发展的人。这样的人往往只服从于法律、秩序、权威的原则，却没有能力希望或者得到无条件的爱。如果他的父亲很有权威，同他的关系又很密切，就更会加强他的这一发展。其他的调查也得出这样的结论：即某些神经病形式，如强迫性神经病同患者的单一父亲联系有关，而另一些病状，如歇斯底里、酗酒，不能面对现实生活和厌世则是同母亲的单一联系所致。</p>
<h3 id="三、爱的对象"><a href="#三、爱的对象" class="headerlink" title="三、爱的对象"></a>三、爱的对象</h3><p>爱首先不是同一个特殊的人的关系，而更多的是一种态度，性格上的一种倾向。这种态度决定一个人同整个世界，而不是同爱的唯一“对象”的关系。如果一个人只爱他的对象，而对其他的人无动于衷，他的爱就不是爱，而是一种共生有机体的联系或者是一种更高级意义上的自私。尽管如此大多数人都认为爱情取决于对象，而不是能力。他们甚至认为专爱一个人就是强烈爱情的证明。我们在上面已经提到过这一错误的结论。正因为人们不是把爱情看作是一种积极的行动，灵魂的一股力量，所以他们认为只要找到爱的对象就行，别的东西自然而然就会产生。可以把这一态度同想画一张画的人作一比较：这个人虽然想画画，但他不是去学绘画这门艺术，而是强调他首先要找到他愿意画的合适的对象。如果他找到了这么一样东西，他也就能画了。如果我确实爱一个人，那么我也爱其他的人，我就会爱世界，爱生活。如果我能对一个人说：“我爱你”，我也应该可以说：“我在你身上爱所有的人，爱世界，也爱我自己。”</p>
<p>认为爱情是一种同所有人相关，而不是只关系一个人的观点并不意味着不同形式的爱情在爱情的对象方面没有区别。</p>
<h4 id="1、博爱"><a href="#1、博爱" class="headerlink" title="1、博爱"></a>1、博爱</h4><p>一切爱的形式都以博爱为基础。作者指的博爱就是对所有的人都有一种责任感，关心、尊重和了解他人，也就是愿意提高其他人的生活情趣。</p>
<h4 id="2、母爱"><a href="#2、母爱" class="headerlink" title="2、母爱"></a>2、母爱</h4><p>虽然对母亲的动机各有解释，但最重要的动机是我们称之为“超越自己”的追求。这一追求属于人的最基本要求，并以人的觉悟和下列事实为基础：即人对自己的纯生物作用不满，他不能忍受自己仅仅是被扔进这一世界的小卒。他一定要感到自己是创造者，是能超越处于被创造者消极地位的生命。满足这一要求有许多可能性，最自然和最基本的途径就是母亲对自己创造物的关怀和爱。在孩子身上母亲超越了自我，她对孩子的爱使她的生活产生新的意义。(正因为男子不能通过生育来满足超越自己的要求，所以他只能通过用双手创造物体和创造思想来证明他的创造能力。)</p>
<p>但是孩子必须长大，必须脱离母体和母亲的乳房，必须成为一个完整的、独立的生命。母亲的真正本质在于关心孩子的成长，这也就意味着也关心母亲和孩子的分离。这里我们就可以看到母爱和性爱的区别。在性爱中两个迄今为止分开的人结合在一起，而在母亲中过去是一体的两个人分开了。母爱不仅应该允许这一分离，而且还应该希望并促成这一分离。只有在这个阶段，母爱才成为一项艰巨的任务，因为这时就要求母亲无私并能贡献出一切，除了被爱者的幸福一无所求，但恰恰在这点上许多母亲都失败了。自恋的、专制的和贪婪的妇女在孩子尚小的时候，可以是一个很疼爱孩子的母亲。但是当孩子处于同母亲分离的阶段时，只有那些真正有能力爱的妇女，那些觉得给比得更幸福的妇女，那些生命之根底很扎实的妇女才会继续是一个疼爱孩子的母亲。</p>
<p>对正在成长的孩子的爱，这种忘我无私的母爱也许是爱的最困难的形式。但是由于母亲对孩子的爱是那么自然，所以往往给人一种容易做到的假象。正因为难以做到这点，所以只有那些有能力爱的妇女，那些热爱丈夫，热爱其他孩子，热爱陌生人和人类的妇女才能成为真正爱孩子的母亲。在这个意义上，没有能力爱的妇女当她们的孩子幼小时，可以是一个很娇惯孩子的母亲，但永远成不了爱孩子的母亲。检验这一点的试金石是看一个母亲愿意不愿意忍受同孩子的分离，以及在分离后能不能继续爱孩子。</p>
<h4 id="3、性爱"><a href="#3、性爱" class="headerlink" title="3、性爱"></a>3、性爱</h4><p>博爱是同等人之间的爱，母爱是对需要帮助的人的爱，虽然这两者之间有很大的区别，但它们却有一个共同点：那就是按它们的本质，它们的爱不属于一个人。如果我爱我周围的人，我也爱所有的人，如果我爱我的一个孩子，我也爱其他的孩子以及所有需要我帮助的孩子。同这两种类型不同的是性爱，性爱要求完全彻底地实现合二为一，要求自己同他人完全融会。按其性质，这种类型的爱是专一的，不是包罗万象的，因此这种爱也是爱的最能迷惑人的形式。</p>
<p>首先这种爱常常会同“堕入情网”的爆炸式的经历混为一谈，在这种情况下，两个人之间的所有隔阂突然都消失了。正像上面已经提到过的那样，按其本质这种突如其发的强烈感受是注定短命的。当陌生人成为亲密的人，就没有需要克服的障碍了，就不需要作出努力去达到真正的接近。爱者对被爱者的了解同对自己的了解一样多，也许我应该说——一样的少。如果体验对方达到一定的深度，那你对对方就不会那么熟悉——而克服两个人之间的障碍的奇迹就会一天天地重复。但大多数人无论对自己还是对别人都是了解得非常快，而且很快就觉得一览无余了，这恰恰是因为他们只了解了人的表面，而没有深入内心。对他们来说，人与人之间的亲密首先是通过性结合得以实现的。正因为他们觉得他们同别人的隔离首先是一种肉体上的隔离，所以肉体的结合对他们来说就意味着克服人与人的隔离。</p>
<p>另外，对许多人来说还有一系列克服人与人隔离的方法。讲述自己的生活，叙述自己的希望和恐惧，谈出自己幼稚的或者不成熟的梦想，以及找到面对世界的共同利益——所有这一切都是克服人与人之间隔离的途径。甚至表露自己的愤怒和仇恨，毫无顾忌地交心也都被看出是亲密的表现。也许从中就能解释一些夫妇常常感受到的相互之间的那股不正常的吸引力：那就是只有当他们一起睡觉或者发泄了相互的憎恨后，他们会突然感到两个人之间的亲密关系。但是这种类型的“亲密”有一个特点，那就是随着时间的推移会逐渐消失。后果就是人们要在另一个人身上，在另一个陌生人身上寻求爱。而那个陌生人又会成为“亲密”的人，新的爱情经历又会是十分强烈和幸福，然后又逐渐消失，一直到希望进行新的征服，得到新的爱情的要求重又出现——并永远幻想着新的爱情会和以前完全不同。</p>
<p>同时性要求的欺骗性又会加强这种幻想。性要求的目的旨在达到结合，而绝不仅仅是生理上的要求和为了释放折磨人的压力。这时对孤独的恐惧会加强这种要求，此外占有欲和被占有欲，虚荣心以及人的破坏性都会加强性要求——当然爱情也会加强这一要求。看起来性要求是同每一种强烈的感情混杂在一起，并因此而得到加剧，所以爱情也会加强这一要求。</p>
<p>大多数人认为性要求是同爱情联系在一起的，所以他们很容易得出具有迷惑性的结论：即如果两个人互相愿意占有对方的身体，他们就是互爱了。爱情毫无疑问会引起性结合的要求，在有爱情的情况下，这种生理关系就不会带有占有或被占有的野心和欲望，而是充满了温柔。如果生理上的结合要求不是以爱情为基础，如果性爱不具有博爱的成分，那么只会造成一种纯生理的暂时的结合。性的吸引力虽然在一刹那间会造成两者结合的幻觉，但是如果没有爱情，在这次结合后留下来的只有陌生的感觉，他们之间的距离没有缩校他们仍是一对陌生人，他们不是觉得羞愧，就是相互憎恨，因为他们比过去更强烈地感受到在幻觉消失后留下来的这种陌生感。温柔绝不是如弗洛伊德所说是性本能的升华，而是博爱的一种直接表现，既表现在爱的生理形式中，也表现在爱的非生理形式中。</p>
<p>性爱具有一种博爱和母爱都不具备的独占性。必须进一步研究性爱的这种独占性。性爱的这种独占性经常被错误地解释为是一种互为占有的联系。我们经常看到互为相爱，但对其他人却毫无情感的男女。他们的这种爱实际上是一种共同的自私，这些人往往把自己同所爱之人等同起来，并通过把一个人分成两个人的办法来克服人与人之间的隔绝。他们以为这样做就能克服孤独。但正因为他们远远脱离同时代的人，所以他们之间实际上也是隔绝和互为陌生的，结合对他们来说只是一种幻觉。性爱是具有独占性，但同时也是通过爱一个人，进而爱全人类，爱一切生命。性爱的独占性只表现在我只同一个人完全地、即在灵魂和肉体上融会为一体。性爱只有在性结合这点上，在生活的全部范围彻底献身这一点上排斥他人，而不是在一个更深的博爱意义上。</p>
<p>如果男女双方确实相爱，他们的性爱就具备一个先决条件——那就是我从我生命的本质出发去爱对方，并且去体验对方的本质。人就其本质来看都是一样的，我们既是整体的部分；又是整体，因此实际上爱谁都一样。从根本上来看爱情是意志的行为，是人作的一项把全部生命交付对方的决定。这一点也正是婚姻是不可解除的观点和许多传统婚姻形式的思想基矗在这些传统的婚姻形式中配偶不经自行选择，而是由被人挑血人们相信“先结婚，后恋爱”的说法。在现代西方世界这种观点被视为是完全错误的。人们认为爱情是一种自发的感情反应，人们会突然被一种无法抗拒的感情所控制。这里人们只看到两个人的特点，而没有看到——所有的男人都是亚当的一部分，所有的女人都是夏娃的一部分这一事实。人们拒绝认识性爱的一个重要因素：即意志的因素。爱一个人不仅是一种强烈的感情——而且也是一项决定，一种判断，一个诺言。如果爱情仅仅是一种感情，那爱一辈子的诺言就没有基矗一种感情容易产生，但也许很快就会消失。如果我的爱光是感情，而不同时又是一种判断和一项决定的话，我如何才能肯定我们会永远保持相爱呢？</p>
<p>从这一立场出发也许可以得出下列结论：即爱情只是意志的行为，献身的行为，爱谁原则上不起任何作用。不管婚姻是别人撮合的还是自行决定的——一经缔结，意志应该能够保证爱情的继续存在。看起来持有这种观点的人并没有认识到人的本质的矛盾性和性爱的矛盾性。我们所有的人是一体——但尽管如此我们每个人又都是只存在一次、不可重复的生命。从我们都是一体的意义上来看，我们能从博爱出发爱每一个人；但从我们是不一样的角度出发，性爱就要求具有特定的、独一无二的、完全是个性的成分，这种成分只存在于几个人，而不是在所有的人的中间。</p>
<p>　　因此这两种观点——一种认为性爱完全是两个人之间的吸引力，是两个特殊的人之间绝无仅有的联系；另一种观点认为性爱只是意志的行为——都是正确的；也许应该这么说，真理既不在这边，也不在那边。所以认为夫妇关系不好应该马上解除婚姻同在任何情况下都不允许解除婚姻的观点都是错误的。</p>
<h4 id="4、自爱"><a href="#4、自爱" class="headerlink" title="4、自爱"></a>4、自爱</h4><p>心理观察是否证实了在自爱和爱别人之间存在着一个基本矛盾的观点？自爱和利己是一码事，还是互为对立？此外，现代人的利己难道确实是一种对具有一切理性和感情可能性的自我的爱，还是对此有不同的解释？利己同自爱完全一样还是利己恰恰是缺少自爱的结果呢？在我们用心理学的观点分析利己和自爱以前，我们必须分析一下自爱和爱别人是相互排斥的这一错误的逻辑结论。如果把他人当作人来爱是美德，而不是罪恶的话，那么爱自己也应该是美德，因为我也是一个人，有关人的一切概念都与我有关。因此上述原则本身就是矛盾的。圣经中“爱他人如同爱己”的说法说明了对自己的完整性和独特性的尊重，爱自己，理解自己同尊重、爱和谅解别人是不可分割的。爱我同爱另一个生命是紧密相连的。</p>
<p>这里我们就触及到了使我们得出这些结论的一些心理上的先决条件。概括如下：我们的感情和态度的对象不仅是其他人，也包括我们自己。对别人的态度同对我们自己的态度互不矛盾，而是平行存在。从这一点出发来解答我们的问题就意味着爱别人和爱我们自己不是两者择一，恰恰相反：一切有能力爱别人的人必定也爱自己。原则上爱自己和爱别人是不可分的。真正的爱是内在创造力的表现，包括关怀、尊重、责任心和了解诸因素。爱不是一种消极的冲动情绪，而是积极追求被爱人的发展和幸福，这种追求的基础是人的爱的能力。</p>
<p>爱另外一个人这一事实就是爱的力量的具体体现。在爱中包含的原则上的肯定是针对所爱之人，而这个人又体现了人类以及人性。对一个人的爱包括了对所有这样的人的爱。“分工”的形式：爱自己的家庭却不爱他人，是缺乏爱的能力的表现。对人类的爱是对一个特定的人的爱的先决条件，尽管对人类的爱从其产生来看是通过对某些特定的人的爱发展起来的。</p>
<p>从中可以得出我自己也是我的爱的对象，同他人没有区别的结论。对自己的生活、幸福、成长以及自由的肯定是以爱的能力为基础的，这就是说，看你有没有能力关怀人、尊重人，有无责任心和是否了解人。如果一个人有能力创造性地爱，那他必然也爱自己，但如果他只爱别人，那他就是没有能力爱。</p>
<p>我们可以假设：爱自己和爱他人平行存在，——那我们如何来解释显然是排斥一切关心他人的利己呢？利己者只对自己感兴趣，一切为我所用，他们体会不到“给”的愉快，而只想“得”。周围的一切，凡是能从中取利的，他们才感兴趣。</p>
<p>利己者眼里只有自己，总是按照对自己是否有利的标准来判断一切人和一切事物，他们原则上没有爱的能力。这一结论难道不正好证明了对自己的关心和对别人的关心只能两者择一吗？是不是应该把利己和自爱看作是一回事才正确呢？但如果这么认为就完全错了，这一错误在自爱这个问题上已经导致许多不正确的结论。利己和自爱绝不是一回事，实际上是互为矛盾的。利己的人不是太爱自己，而是太不爱自己。缺乏对自己的爱和关心表明了这个人内心缺少生命力，并会使他感到空虚和失望。在必要时这个不幸和胆怯的人会通过各种其他的满足来弥补他失去的幸福。他看上去似乎非常关心自己，实际上只是试图通过对自己的关心去掩盖和补充自己缺乏爱的能力。弗洛伊德的观点是利己者就是自恋者，他们把对别人的爱用到自己身上。利己者没有爱别人的能力这是对的，当他们也同样没有能力爱自己。</p>
<p>如果我们把利己同在一个过度忧虑的孩子的母亲身上可以看到的那种占有欲作一比较，就更容易了解什么是利己。母亲一方面真诚地相信，她对自己的孩子特别地好，但另一方面她确实能感觉到对她宠爱的对象有一种几乎已经觉察不到的敌意。母亲之所以对孩子这么忧虑重重，并不是因为她太爱孩子，而是因为她要以此来弥补自己缺乏爱孩子的能力。</p>
<p>我们的这一关于利己本质的理论符合精神分析学家在治疗“忘我”症时所获得的经验。“忘我”是神经病的一种症兆，在为数不少的患者身上可以看到这种症兆，只是这些人一般来说不是受这种症兆，而是受到与这一症兆有关的其他的病兆，如厌世、虚弱、失去工作能力和处理不好爱情问题等的折磨。但是“忘我”不是像我上面所说的被看作是一种病兆，在大多数情况下“忘我”被看作是值得自豪的、唯一令人满意的性格特点。“忘我”的人一无所求，他只为“别人活着”，而且因为不重视自己而感到自豪。但一旦他发现，尽管他那么忘我可还是感到不幸，他同别人的关系仍然不令人满意，他就会感到吃惊。精神分析表明，这种“忘我”是一种病兆，而且常常会是主要病兆之一。患者没有能力爱，也没有能力使自己快活，他对生活充满了敌意，在他的忘我后面隐藏着一种很强的常常是自己意识不到的自私性，我们只有把他的“忘我”看作是一种病兆，使他克服缺乏创造力的缺点，也就是克服造成“忘我”以及其他病兆的根源，他才会得到痊愈。</p>
<p>忘我的本质特别表现在对其他人的影响上——在我们的文化中最常见的表现是“忘我”的母亲对自己孩子的影响。母亲认为孩子可以通过她的“忘我”认识到什么是被人爱，认识并学会什么是爱。但是她的“忘我”所造成的效果往往违背她的意愿。孩子们并没有表现出他们是幸福的，他们是被人爱的；他们一个个胆小，紧张，担心受母亲的责备并想方设法满足母亲的愿望。一般来说他们会受到母亲的那种隐蔽在深处的对生活的敌意和恐惧的传染，他们更多地是能感觉到，而不是认识到这点。总而言之，“忘我”的母亲的影响同利己者的影响并无多大区别，而且常常是前者甚于后者；因为母亲的忘我会阻止孩子对自己提出批评。孩子们的生活在一种不能使母亲失望的压力下，在道德的假面具下人们在教育他们要轻视生活。如果有机会，可以观察一下一个能真正自爱的母亲对孩子会产生什么影响，从而可以确定，再没有比一个能自爱的母亲在体验爱情、欢乐和幸福方面对孩子产生更积极的影响了。</p>
<p>爱克哈特(爱克哈特，中世纪德意志神秘主义哲学家和神学家。他认为上帝即万物，万物即上帝；通过自己的灵性，人即可与上帝合而为一，与万物混成一体，获得真正的自由。——译者注)有一句格言，最精辟地总结了关于自爱的思想。他说：“你若爱己，那就会爱所有的人如爱己。你若对一个人的爱少于爱己，如果你不是爱所有的人如同爱己，如果你不是在一个人身上爱所有的人——因为这个人就是上帝和人。一个既爱自己又爱他人如同爱己的人就是这样的人，一个值得这样评价的人。”</p>
<h1 id="爱情及其在当代西方社会的衰亡"><a href="#爱情及其在当代西方社会的衰亡" class="headerlink" title="爱情及其在当代西方社会的衰亡"></a>爱情及其在当代西方社会的衰亡</h1><p>如果爱情是那些具有创造性和成熟性格的人的一种能力，那么由此可以得出结论：每一个在一个特定社会生活的人的爱的能力取决于这一社会对这个人的性格的影响。当我们谈到当代西方社会的爱情时，我们要提出下列问题，即西方文明的社会结构以及这一社会结构产生的精神是否会促进爱情的发展。提出这一问题就意味着要对此作出否定的回答。任何一个客观地观察我们西方生活的人都会毫不犹豫地说爱情——博爱、母爱和性爱在西方是罕见的现象，许多假爱情的形式取代了它们的位置，而这些假爱情的形式实际上只是爱情的衰亡的形式。</p>
<p>资本主义社会一方面是以政治上的自由原则，另一方面是以市场作为调整一切经济活动，因此也是调节一切社会关系的原则为基础的。货物市场决定进行货物交换的条件。劳动力市场调节劳动力的买卖。有用的物和有用的人的精力和技巧都变成价值，这些价值根据市场的条件自愿公平地进行交换。譬如说鞋吧，一旦市场上没人问津，即使鞋本身是有用和必需的，也会失去任何经济价值（交换价值）。人的力气和技巧亦是如此。资本的拥有者可以购买劳动力，并命令劳动力为其资本的有利投资而劳动。劳动力的拥有者必须根据当时的市场条件出售其劳动力，才不至于挨饿。这种经济结构反映在价值的高低级别上。资本统治劳动力，无生命的物体要比劳动力，要比人的才能和一切有生命的物体价值要高。占有要高于存在。</p>
<p>这一结构从一开始就是资本主义的基础，尽管这一社会结构至今仍然是现代资本主义的标志，但有一些因素起了变化，这些因素赋予现代资本主义新的特点，并对现代人的性格结构产生深远的影响。我们看到资本主义发展的结果是资本不断集中。大企业不断地扩大，而小企业越来越受排挤。在大企业中，资本的所有权越来越同资本的管理权分开。几十万股票持有者是企业的“占有者”。管理企业的则是管理官僚阶层，他们虽然薪俸甚高，但企业并不属于他们。这些官僚不仅对获取大量利润感兴趣，而且也热衷于不断扩大企业，从而不断扩大他们自己的权力。资本的日趋集中和强大的管理官僚阶层的形成也表现在工人运动的发展中。工会把劳动力组织起来，使得工人不必在劳动力市场上孤军作战。工人成为大工会的成员，而这些大工会也同样被强大的官僚阶层所管理，并代表工人去同工业巨头对峙。无论在资本领域，还是在劳动力领域，个人的主动性被官僚阶层所取代。越来越多的人失去独立性，依附于庞大的经济帝国的官僚阶层。</p>
<p>资本集中带来的另一个决定性特点是劳动组织的特殊形式，这也是现代资本主义的特点之一。高度集中、分工严密的企业导致一种新的劳动组织，在这一组织中个人失去了个性，而成为机器中一个可以随时调换的齿轮。现代资本主义中个人的问题可以归纳如下：</p>
<p>现代资本主义需要大批能在一起协调工作的人。这些人对消费的需求越来越高，但他们的口味是标准化的，既容易受到控制，又能预测。现代资本主义需要的人是一方面能感觉到自己是自由和独立的并相信自己不屈服于任何权威、原则和良心，另一方面他们又准备执行命令，完成别人交给的任务，服服贴贴地进入社会这部机器中去，规规矩矩地听人摆布，自愿服从领导，盲目地受人指挥——只有一个例外，那就是他们要不遗余力地干活，永远地发挥作用和力争晋升。</p>
<p>那结果是什么呢？如果就是现代人对自己、对同代人和对大自然产生异化。他变成一种商品，体验到自己的生命力实际是一笔资本，这笔资本在既定的市场条件下要给他带来最大的利润。人与人之间的关系从本质上来看是互为陌生的，是自动机器之间的关系，其安全感的基础就是要想方设法靠拢一群人，在思想、感情和行动中同这一群人保持一致。虽然每个人都努力同别人接近，但实际上都是孤独的，充满了不安全感、恐惧感和负罪感。只要人与人之间的隔膜得不到克服，这种感觉就会不断出现。但我们的文明提供了各种可能性，使人们感觉不到这种孤独。这首先就是人们每天都重复着千篇一律僵化的机械性工作，这种工作秩序使他们不再自觉地感到人追求超越和统一的基本要求。但是光靠这个还不行，因此人就通过享受，通过娱乐工业提供的音乐、画片，以及通过不断地购买新的物品去减少这种尚未意识到的绝望。事实上现代人很像休克斯勒尔（休克斯勒尔(1894——1963),美国作家，深受佛教的影响。——译者注）在他的《美丽的新世界》一书中描绘的那付样子：“营养充分，穿戴讲究，性欲得到满足，但却没有自我，同他同时代的人也只有表面的接触。”现代人的宗旨正如休克斯勒尔简明扼要地总结的那样是“今朝有酒今朝醉”或者是“今日，人人幸福”的颂词。现代人的幸福就是享受，就是满足消费和同一群人同化的要求。他们消费商品、图片、食品、饮料、香烟、人、杂志、书籍、电影，真是无其不有。世界只是为了填饱他们的肚子，就象一个巨大的苹果，一个巨大的酒瓶和一个巨大的乳房，而我们是婴儿，永远在期待，在希望，却永远是个失意者。我们的性格努力地适应进行交换、接受和消费的要求。所有的一切——精神的和物质的东西——都成为交换和消费的对象。</p>
<p>至于爱情，当然也完全符合现代人的社会性格。自动机器是不会爱的，它们只能交换“一揽子特性”，想做一笔好买卖。在这一异化了的结构中，人在爱情上的基本要求是“结伴”思想，这在婚姻中表现得尤为突出。在无数宣传美满婚姻的文章中，一对毫无摩擦的伴侣被奉为是理想的结合。这一宣传同社会要求职员应得心应手的标准毫无两样。这个职员必须“相应独立”，是一个很好的合作者，宽容，同时又具有进取心，对生活的要求又很高。正像婚姻顾问对我们介绍的那样，一个丈夫应该理解他的“妻子”，并是她的帮手。他应该赞赏她妻子的新衣服，也要称赞她做的饭菜。而每当丈夫疲劳不堪、怨气十足地回家来时，妻子则应该体谅他，当丈夫谈到职业上的麻烦事时，妻子应该注意听他讲。如果丈夫忘记了她的生日，妻子不应该生气，而应该通情达理。所有这一切无非是表明这两个人的关系如上了油一样毫无摩擦，但这两个人一辈子都会互不了解，永远达不到“中心关系”，而是敬如宾客，只是尽力使对方舒适而已。这样的爱情和婚姻概念实际上是强调保护自己免遭不可忍受的孤独感的侵袭。在“爱情”中人们终于找到了避风港。两个人结成用以反对全世界的同盟，却把这种两个人的自私看作是爱情和信赖。</p>
<p>强调结伴的精神，强调相互之间的宽容是一个比较新的发展。在第一次世界大战后的那段日子起作用的是另一种爱情公式。那时性的相互满足是令人满意的爱情关系，特别是幸福婚姻的基础。人们认为造成许多不幸婚姻的原因是夫妇在性生活上不能很好地“配合”，而根源是缺乏对性生活的正确态度，也就是一方或双方都没能很好地掌握性生活的技巧。为了“消除”这种缺陷和帮助那些不能相爱的不幸的夫妇，许多书里都提供了各种正确的性态度的建议和说明，并多多少少许诺只要这样，幸福和爱情就会油然而生。其基本思想是：爱情是性生活得到满足的产物，如果男女双方学会在性生活上使对方满足，他俩就会相爱。这一点完全符合社会上流行的幻想，即正确的技术不仅能解决工业生产的问题，也能解决人的问题。人们没有看到，与此相反的观点才恰恰是正确的。</p>
<p>爱情不是性满足的结果，而是性的幸福，甚至掌握所谓的性技巧也是爱情的结果。如果一定要证实这一观点，除了日常的观察外，还可以求助于精神分析治疗的许多具体实例。对最经常出现的性问题的研究——妇女的性冷淡，男子心理上的各种严重的或不太严重的阳痿形式——表明产生这些问题的原因不在于缺乏技巧，而是这些男女的胆怯心理使他们失去爱的能力。害怕异性、憎恨异性是造成这些困难的原因，这些困难阻止他们献出自己和自发地行动，使他们在生理上无法忍受异性的靠近。如果一个有性障碍的人能从他的恐惧和憎恨中摆脱出来，他就会获得爱的能力，他的性问题也就解决了。如果不能摆脱出来，即使有再多的性技术的知识也无济于事。</p>
<p>把爱情看作是性满足的产物，把爱情看作是结伴思想和防止孤独的避风港，这两种观点是西方社会中爱情衰亡的两种“正常”形式，是由社会决定并造成的爱情病理学。这种病理学有许多个性化的形式，其结局都是自觉地受苦。这些形式被精神分析学家和越来越多的外行称为神经玻下面通过几个例子简明地说明一些经常出现的形式。</p>
<p>造成精神病态爱情的基本条件是“相爱的”一方或双方都牢牢地抓住父亲或母亲的形象，并把他以前对父亲或母亲怀有的感情、期待和恐惧成年后都转移到“所爱者”身上。这些人从来没有超越儿童阶段，成年后还在寻找儿童时代的联系。在这种环境下，这些人在感情生活方面始终是停留在二岁、五岁或十二岁的阶段，但他们的智力和社会能力却符合他们的实际年龄。在严重的情况下，感情上的这种不成熟状态会破坏其社会生活；在不那么严重的情况下这一冲突只限于个人亲密关系的范畴。</p>
<p>我们再回到我们前面已经提到过的以父亲为中心或以母亲为中心的讨论。下面的例子与我们现在经常能看到的病态爱情关系有关，也就是男子在感情发育过程中始终停留在同母亲的联系上。这些男子从来没有断奶，他们始终感到自己是孩子，他们需要母亲的保护、母爱、温暖、关怀和欣赏。他们需要无条件的母爱——得到这种爱只需要一个条件，那就是他们需要这种爱，他们是母亲的孩子，弱小无力。这些人在企图赢得一个女子的爱时，往往和蔼可亲，风度翩翩；如果他们成功了，仍然会保持这副样子。但他们同这个女子的关系（实际上同对所有的人的关系一样）都是表面的，而且不负责任。他们的目的是被人爱，而不是爱自己。在这种类型的人身上往往可以看到很强的虚荣心和没有完全暴露的远大志向。如果他们找到“合适”的妻子，他们就信心十足，觉得自己占了全世界的上风；这时他们对其他人也会和蔼可亲，温文尔雅。但在过了一段时间后他的妻子不再符合他的想象，就会出现冲突和摩擦。如果他妻子不始终如一地欣赏他，如果她要求有自己的生活，希望得到爱和保护，如果她——在极端的情况下——不准备原谅他的外遇（或者不流露对此有一种颇为欣赏的兴趣），这时他就会感到受到很大的伤害和失望。一般来说他还会用“妻子不爱他，自私或者专制”的说法把他的这种感情简单化。很明显，“慈母”对她的令人着迷的“儿子”的任何一个小小的疏忽都被看作是缺乏爱情的表现。这些男子一般来说把他们的文雅举止，和他们愿意使别人高兴的愿望同真正的爱情混淆起来，并因此得出他们受到不公正对待的结论。他们自以为是伟大的恋人，对妻子的不满抱怨不休。</p>
<p>只有在很少的情况下，一个以母亲为中心的男子才能正常地生活。如果他们的母亲是以一种升华的方式“爱他”（也许她虽然专制，但不具有破坏性），如果他的妻子同他的母亲是相同的类型，如果他的特殊才能能使他发挥他的魅力和赢得他人的欣赏（某些杰出的政治家就是这种情况），那么从社会角度来看他已经是“很好地纳入”社会。即使他从来没有达到一个更成熟的精神高度。但是在不少上面所说的有利条件下——当然这是一种更经济的情况——他的爱情升华（尽管不少他们的社会生活）会是巨大的失望；当这种类型的人一旦觉得他被众人所抛弃，就会出现冲突，在很多情况下会产生强烈恐惧和厌世的念头。</p>
<p>在另一种给位严重的病态爱情形式中，患者同母亲的联系更深，也更缺乏理性。在这种情况下，形象地说，问题不在于病人想回到母亲爱护的双臂之中或者给予养料的乳房，而是回到母球接受一切——和破坏一切——的怀抱里。如果说精神健康的本质在于脱离母亲的子宫，进入世界，那么严重精神病的本质就是被母体所吸引，要重新回到母体——也就是被夺走生命。这种联系往往出现在和母亲的关系中，他们的母亲以这种接受——破坏的方式同孩子联系在一起。又时她们是以爱的名义，有时是以履行责任的名义要在自己身上保留孩子，保留成长的孩子以及成年后的孩子。只有通过她们，孩子才能呼吸。这些男子除了一些侮辱女性的表面关系外不可能爱别的女人。她们不能自由和独立，而只能永远是一个残废者或者是一个罪犯。</p>
<p>母亲的具有破坏性的侵吞性的一面是母亲形象中坏的一面。母亲不仅能赋予生命，而且能夺走生命。母亲是活跃生活、也是破坏生活之人。她能创造爱的奇迹——但没有人比她更能伤害人。在宗教的象征中（如印度女神时母和在梦的象征中都可以经常找到母亲的两个截然相反的方面。</p>
<p>神经机能病态的另一个完全不同的形式可以从同父亲相关的病例中找到。</p>
<p>一个相应的例子是一个男子有一个性冷淡感情内向的母亲，而父亲却把他的爱和全部的兴趣倾注在孩子身上（这一部分是母亲冷淡的结果）。他是一个“好父亲”，同时也很专横。他如果对儿子的行为满意，他就称赞他，送给他礼物，对他很亲切。一旦他对儿子不满，他就会退居一旁或者咒骂儿子。除了父亲的疼爱以为一无所有的孩子就以一种奴隶的方式同父亲联系在一起。他的生活主要目标就是要使父亲高兴——如果他做到了，他就感到幸福、安全和满足。但如果他犯了错误，做了错事，如果他不能讨父亲的欢心，他就感到空虚、没人爱他或受到唾弃。再后来的生活中，这个人总之寻找一个他能以同样的方式与之联系的父亲形象。他的一生始终是依照他说否得到父亲的称赞而上下起落。在社会上这些人常常能获得很大的成功，他们认真、值得信赖和勤奋——先决条件是他们所选定的父亲形象要善于正确地对待他们。他们同女仔的关系则是小心翼翼和有距离的。妇女对他们来说没有中心意义；他们一般对妇女颇有点轻视，这种轻视往往被他们对妇女的像父亲对小姑娘那般的关系所掩饰。一开始，由于他们的男性特点，他们会给妇女留下一些印象；但是一旦嫁给这些男子的妇女发现他们自己在丈夫的生活中只起第二位作用，——而父亲的形象其主要作用——，她们就会越来越失望。但是也有例外的情况，那就是如果其中碰巧也是一父亲为中心的类型——这样她同一个对待她如同一个任性的孩子那样的男子在一起就会感到幸福。</p>
<p>更为复杂的病态爱情形式往往出现在下面的那种人身上，这些人的父母互不相爱，但又善于控制自己，他们既不争吵也不流露自己的不满。同时这些父母同子女的关系也很不自然。一个姑娘在叫里感受到的只是“规规矩矩”的气氛，但同父亲或母亲没有很多接触，因此留在姑娘心中的只是混乱和害怕的清晰。这个姑娘永远不知道父母的感受和想法。在这样的家庭气氛中始终存在着一种不可知和空虚的成分。后果是姑娘完全隐退到自己的小天地里去，而她的这一态度一直可以保持到她后来的爱情关系中去。另外这种回避也是不断滋长的恐惧情绪以及在这个世界无根底的感受所致，最终会导致被虐癖的倾向，因为这是可以体验强力刺激的唯一机会。这些妇女常常愿意他们的丈夫和她们吵闹，而不是正常地、理智地与他们相处，因为只有这样才能够使她们暂时地失去紧张和恐惧的感受。因此她们往往会不自觉地去激怒丈夫，一结束折磨人的空虚。</p>
<p>下面还将介绍几种经常出现的非理性的爱情形式，但不再分析原因——即同年时代发展的一些特殊因素。</p>
<p>不乏少见的假爱情的一种形式——这种形式又常常被人们称为“伟大的爱情”（经常出现在小说和电影里）——是偶像化的爱情。一个没有达到产生自我感觉高度的人（这种自我感觉的基础是创造性地发挥自己的力量）倾向于把自己所爱的人“神化”。他同自己的力量异化并把自己的力量反射到他所爱之人身上，他所爱之人被当做一切爱情、光明和祝福的源泉而受到他的崇拜。在这一过程中，人失去了对他自己力量的觉悟，在被爱者身上失去自己，而不是找到自己。但是从长远来看，由于没有一个人能符合崇拜者的心愿，当然不可避免地就会出现失望，而解决这一问题的方法就是寻找一个新的偶像——有时候会出现恶性循环。这种偶像化爱情形式开始时的特征是爱情体验的强烈性和突发性。这种形式的爱情常常被看作是真正的伟大的爱情；但是恰恰是这种所谓的强烈性和深度性却表现了那些恋爱者的饥渴和孤独。也许不必过分强调的是，我们常常可以看到这种爱情形式相结合的男女在严重的情况下会给人一对疯子的印象。</p>
<p>另一种假爱情的形式就是人们称之为多愁善感的爱情。这种爱情的本质就是它只能存在于想象中，而不是存在于同另一个人实实在在的结合之中。这类爱情最广泛的形式是用代用品使自己满足，那就是消费爱情电影、爱情小说和爱情歌曲。通过消费这些东西可以使一切没有实现的对爱情、人与人结合和亲近的向往得到满足。那些无力拆除自己与伴侣之间那堵高墙的男女，当他们在银幕上看到悲欢离合的情侣时，会身临其境，感动得热泪盈眶。对许多夫妇来说，银幕是他们体验爱情的唯一可能性——不仅自己是这样，而且两个人会一起成为他人爱恋故事的观众。这要爱情是一个白日梦，他们就能加入进来，但如果爱情成为两个真实的人之间的一种现实关系——他们就僵化了。</p>
<p>多愁善感的爱情的另一种表现是吧现时推移到过去，一队夫妇可以通过回忆过去的爱情而受到深深的感动，虽然他们当时根本就没有感受到爱。这种情况和幻想未来的爱情完全一样。不知有多少定过婚的男女或新婚夫妇仍在憧憬未来爱情的想法，尽管她们现在已经开始感觉到对方的无聊。这种倾向符合作为现代人标志的一般态度。现在人不是生活在过去就是生活在未来，但不是现时。他们满怀感伤地回忆童年和母亲——或者为未来制定伟大的计划。不管是通过参与别人的非真正的爱情经历来体验爱情，还是通过吧现时推移到过去和未来的方法来躲避爱情的现实，这些抽象的和异化的爱情形式其作用就和鸦片一样，都是为了减轻现实、人的孤独和与世隔绝所带来的痛苦。</p>
<p>神经病态爱情的另一种形式是一套投射做法。这种投射做法能导致回避自己的问题，从而把注意力放到“所爱者”的错误和缺点上。个人在这方面的态度同民族和国家的态度没什么两样。有些人对他人的每一个细微错误的反应都十分灵敏，而对自己的问题和弱点却不闻不问，他们永远是在考虑如何指责对方或者教育对方。如果——常常是这种情况——男女双方都热衷于这么做，那他们俩之间的爱情关系就成为相互的投射。如果我们是专横或无主见的，我就指责对方有这些缺点，并且根据我的性格不是要求他改正就是为此要惩罚他。而对方也同我一样地行事——这样两个人都能回避自己的问题，因此这两个人也就不能采取使他们自己进一步发展的步骤。</p>
<p>投射的另一种形式是把自己的问题投射到孩子身上。首先这种反射常常反射在希望生孩子的愿望上。有些人之所以要孩子是因为他们想把自己的生存问题反射到孩子身上。如果当一个人感到自己没有能力赋予自己的生活一种意义时，他就会试图在他的孩子的生活里找到生活的意义。但是这必然会在自己和孩子身上造成失败的结果。失败的第一个原因是因为每一个人的生存问题只能有自己解决，而不能通过一个代理者。另外一个原因是有这种打算的人恰恰缺乏必要的能力、以引导孩子解决自己的生存问题。同时孩子还往往被当作投射的对象，以缓和父母之间的紧张关系。这些父母常使用的理论就是为了使孩子不是去一个共同的家，所以不愿离婚。但深入的调查结果表明：在这样的“共同的家”中笼罩着的那种紧张和不幸的气氛往往比公开的决裂对孩子的损害更大，因为公开的决裂至少表示一个人有能力通过一项勇敢地决定来借宿无法忍受的状况。</p>
<p>这里还必须提及一个经常出现的错误,一种幻想，即认为爱情必定意味没有冲突。按“在任何情况下都应避免痛苦和悲伤”的世俗之见，所以现代人也认为，爱情就是意味着没有冲突。他们还以他们所见之争吵都是毁灭性的争论，对双方都没有好处的事实作为理论依据。但是真正的原因在于大多数人的“冲突”实际上都是为了避免真正的冲突。这些冲突只是对一些鸡毛蒜皮的小事产生分歧而已，而这些小事按其本质来看是无法澄清或者无法解决的。但人与人之间的真正冲突——那些不应该被遮掩，也不应该投射到别处的冲突，那些属于人的内在现实并能在人的心灵深处体验到的冲突——绝不是毁灭性的。这些冲突会得到澄清，会带来一种净化，从而是双方能变得更有知识，更坚强。现在我得把我上面讲过的东西再强调一下。</p>
<p>爱情只能产生于这样两个人中间，这两个人都从他们生存的圈子里跳出来并互相结合，同时他们每个人都又能脱离自我中心去体验自己。只有这种“中心体验”才是人的现实，才是生活，才是爱情的基础。这样体验到的爱情是不断地挑战，这种爱情不是避风港，而是一种共同的努力、成长和劳动。如果两个人能从自己的生命的本质出发，体验到通过与自觉地一致，与对方结成一体，而不是逃离自我，那么在这样的基本事实面前，就连和谐、冲突、欢乐和悲伤这样的东西也就只能退居第二位了。“爱情的存在只有一个证明：那就是双方联系的深度和每个所爱之人的活力和生命力。这也是我们所能看到的爱情的唯一成果。”</p>
<p>正如自动机器不能相爱一样，自动机器也不可能爱神，因此神爱所达到的衰亡程度与人爱的衰亡程度相等。这一事实同有些人认为我们是发生在我们时代的宗教复兴的见证人的观点大相径庭。在没有比这种观点更荒唐的了。我们所经历的（即使有例外）无非是回到把神偶像化的时代，和把对神的爱变成符合异化了的人的性格结构。从新捡起把神偶像化的做法是很容易识破的。我们社会上的许多人胆小怕事，没有原则，也没有信赖，除了活下去外没有任何目标，因此他们仍然是孩子并希望在他们需要帮助的时候，能得救于父亲或母亲。在宗教文化中，譬如中世纪的宗教文化中，就是一般人都把神看作是帮助他们的父亲和母亲，这确实是事实。但是他们对待神的态度是很严肃的，他们把按照神的旨意去生活看作是他们生活的目标。可今天已经看不到这种努力了。日常生活同一切宗教价值已截然分开。生活的目的仅仅是为了寻求物质上的享受和劳动力市场上的成功。我们在世界范畴内活动的原则基础是冷漠和自私（后者常常被“个人主义”或者个人“能动性”的叫法所取代）。生活在真正宗教文化中的人也许可以同一个八岁的儿童相比较，儿童一方面把父亲看作是拯救者，但另一方面他已经开始把父亲的教诲和原则接受到自己生活中去。而现代人却像一个三岁的孩子，只有需要夫妻时才招呼他，而自己一个人能玩时，也会很高兴。</p>
<h1 id="爱的实践"><a href="#爱的实践" class="headerlink" title="爱的实践"></a>爱的实践</h1><p>行使任何一门艺术都需要有一些基本的东西，木匠艺术、医疗技术和爱的艺术都是如此。首先要求有纪律。如果没有纪律，我将会一事无成。如果我是凭一时的“兴致”去行事，这也许会成为使我感到愉快的一种嗜好，但我永远成不了大师。这里所指的纪律不是实践一门特殊艺术所要求的纪律（如每天要坚持练习几个小时），而是贯穿人的一生的纪律。也许有人会说，对当代人来说莫过于比学校纪律更容易的事了；难道当代人不是每天都要遵守劳动八小时的纪律吗？但实际情况是，在工作之外人很少能表现出一点自我纪律来。一旦他不工作，就十分懒散，无所事事——用一句好听的话来表达就是他想“轻视一下”。但恰恰是这种什么也不想干的意愿是对生活秩序作出的一种反响。正因为人们被迫每天八小时为别人的目标付出力气，以一种劳动节奏规定的方式工作，所以他就要反叛，而这种反叛就采取了无所作为的态度。另外他在反对权威的斗争中对每一种纪律都已抱有怀疑的态度。不管这种纪律是非理性的权威强加给自己的还是自己给自己规定的理性的纪律。但如果没有纪律，生活就会七零八落、混乱和没有集中。</p>
<p>集中是掌握艺术的一个必要条件，这一点是无需证明的了。每一个试图学会一门艺术的人都了解这一点。但是在我们这个社会集中比自我纪律还有罕见，我们的文化已经导致一种非集中的、分散的、史无前例的生还方式。人们往往同时干几件事：看书、听收音机、谈话、抽烟、吃饭和喝酒。人成为消费者，张开大嘴坐着，贪婪地吞下一切东西：画片、烧酒和知识。这种缺乏集中的想象特别表现在我们现在已经很难一个人安静地坐着。大多数人已经不会一个人安静的坐着，不说话，不抽烟、不看书和不喝酒。他们很快就会变得不安起来，他们一定要动嘴或者动手。（抽烟就是缺乏集中的一个症状，抽烟的人即动手、动嘴、也懂眼睛和鼻子。）</p>
<p>第三个因素是耐心。学过艺术的人都值得要达到目的就必须要有耐心。想尽可能快的取得结果的人永远也学不会一门艺术。尽管如此，对现代人来说耐心同纪律和集中一样是难以做到的。我们整个工业系统提倡的恰恰是耐心的反面，那就是要快。我们所有的机器都是为了达到快的目的：汽车和飞机把我们很快地送到预定的地点——而且要越快越好。以一半的世界生产同样多的产品的机器要比旧的和运转慢的机器好一倍。当然这里有重要的经济原因，但是正如同许多其他的方面一样，这一点也体现了人的价值原是由经济价值所决定。对机器是好的东西必然对人也是好的——这听起来似乎合乎逻辑。现代人认为如果他不很快地处理事情，就会失去时间，可他并不知道他如何利用他由此赢得的时间——除了只会无聊地打发时间。学会一门艺术还有另一个条件那就是对掌握这门艺术要有极大的兴趣。如果一门艺术没有最高意义的话，那没有一个学徒会学这门艺术。他最多成为一名业余能手，但不可能成为大师。这一条件对爱的艺术如同对其他的艺术一样同为重要。但看起来，在爱的艺术中，业余能手的人数要远远超过大师。</p>
<p>在谈及学会一门艺术的一般条件时还必须提及一点，那就是人们从来不是一开始就直接地学会一门艺术，而总是间接地学会这门艺术。一开始人们必须学会许多其他的、而且看起来经常是同这门艺术无关的东西，然后才开始学子这门艺术。木匠学徒要先学会刨木头，学钢琴的人要先练习音阶，而学习禅宗射箭艺术的人则要先练习呼吸。为了使自己成为一门艺术的大师，必须把一生献给这门艺术。在实践这门艺术时，自己要成为工具并保持一定的状态，以适应需要完成的任务。这一点在实践爱的艺术上就意味着所有想成为大师的人应该把生活的每一个阶段训练纪律、集中和耐心作为实践爱的艺术的开端。</p>
<p>那么如何训练纪律呢？我们的爷爷辈能更好地回答这一问题。他们会建议我们早起，不要过奢华的生活，要努力工作。但这种类型的纪律也有不利的一面。这种纪律死板，是把节余的道德放在首位，并且在很多方面与生活为敌。但作为对这类纪律的回答，今天越来越大的倾向是以一种怀疑的目光来对待每种纪律，并以一种懒洋洋的无所事事来找到八小时之外的平衡点。每天早晨按时起床，按时进行一定的活动，如禅坐、看书、听音乐和散步，不做或者有限度地做一些分散注意力的是如看侦探小说和电影，不暴饮暴食——这些都是明显的基本要求。但是最重要的是不要把纪律看作是外部强加的东西，而应该成为自我意志的体现，应该感到这是一种愉快，并且逐渐习惯于一种生活态度，一旦放弃它，便会若有所思。我们西方扔对纪律观念最令人遗憾的看法（对其他的到底亦是如此）是，他们认为纪律必会使人难受或不适，纪律只有达到这种效果，才是“有用的”。但是东方人很久以来就认识到，与身心有益的东西必定使人舒适，即使开始的时候需要克服一定的阻力。</p>
<p>生活在我们文化中的人很难做到集中，因为我们的全部文化似乎都是为了“分散注意力”和反对培养集中的能力。最重要的步骤是要学会一个人单独呆着，而且不看书，不听广播，不抽烟和不喝酒。有没有集中的能力表现在能不能单独地呆着——而这种能力优势学会爱的一个条件。正因为我不能自力更生，所以我只能把自己同另一个人连在一起。这个人也许就是我的生命的拯救者，但是这种关系同爱情无关。是否能一个人呆着居然成为有无能力爱的条件之一，这一点会令人奇怪。但每一个试图这么做的人将会知道做到这点是很难的。随着时间的推移他会坐立不安，甚至会感到有点害怕。于是他就会找出各种借口为自己放弃练习辩护，他会说一个人呆着毫无意义，是愚蠢的，太浪费时间，等等。他在练习的过程中还会确定，一个人呆着的时候，各种各样的念头都会冒出来，困扰你。他会突然发现他正在打算这一天还能干些什么，他在思考工作中遇到的困难或者是考虑今天晚上上哪里去。但是做一些简单的练习就能帮助他集中，譬如：轻松地坐着（即不要懒散，也不要紧张），把眼睛闭上，努力使自己的眼前出现一片白色，并排除一切干扰自己的画面和念头。然后可试着观察自己的呼吸——不要去想它，也不要去影响它，而只是要意思到自己在呼吸。另外还要试着得到一种“自我”的感受；我=我的自己+我的力量的中心+我的世界的创造者。至少每天早晨要做二十分钟这样的练习（如果有可能还有延长）和每晚睡觉前坚持练习。除这些练习外还有学会专心做一切事：专心听音乐、看书、谈话或欣赏图画。如果专心地干，那么干什么就无关紧要了，无论干什么，重要的或者不重要的都会增加一层现实意义，因为干事的人是完全开放的。为了学会集中要尽量避免无意义的谈话，也就是不能成为谈话的谈话。如果两个人在谈论他俩所熟悉的一棵树的生长情况，或者在评论刚才吃过的面包的味道，或者在回忆他们职业上的共同经历，他俩的谈话可能是重要的，这就看他俩是否真的在谈论一件经历过的事，还是就抽象的东西交换看法。另外有关政治或者宗教的谈话也肯能是毫无意义的，如果交谈者只是老生常谈，没有亲身经历的体会，只是交换一下看法而已。我这里还要补充一点，那就是不仅要逃避无聊的谈话，而且还要避免同不三不四的人来往。我这里指的不仅是要回避那些有破坏性的凶恶之人，应该回避他们，因为这些人会使人消沉和压抑，而且还指那些内心无生命力的人，那些思想和谈话都没有内容的人，这些人不是在谈话而是在闲扯，他们不会思考，指挥把一套套的世俗之见搬出来。当然不可能永远回避这些人，有时也没有费这么做的必要。在和这样的人接触中如果你不是像他们所想的那样，闲扯一通，而是直率地和与人为善改变自己的态度，这一方面是因为你的反应使他们大吃一惊，另一方面也是因为他们自己也渴望从杜撰和陈词滥调中摆脱出来，走向现实。</p>
<p>在同别人的关系中要记住首先就意味着要有听别人讲话的能力。大多数人认为他们是在听别人讲话，而且还帮对方出主意，可实际上他们根本没听进去。他们不重视别人的话，漫不经心地回答对方。后果是这样的谈话往往使他们感到疲倦。他们认为如果他们记住地听对方讲话，就会跟疲倦。可他们想错了，每一件聚精会神完成的事会使人清醒（尽管干完时候出现能恢复的自然疲劳状态）。而懒懒散散的干事只能使人产生倦意——同时这些人在夜里也很难入睡。集中意味着要完全地在现时地生活，而不是赶着这事想那事。那些相爱的人应当首先练习集中，这事理所当然的。他们必须学会亲近对方并向对方开放，而不是像通常所见的那样相互回避。万事开头难，这句俗话对练习集中也适用。人们常常会有永远达不到目的的感觉。所以显而易见练习集中还必须要有耐心。如果人们不知道学会每一种事都要有一个过程，都应自己给自己施加压力，那就永远不会学会集中。要想知道什么是耐心，只有观察幼儿学走路就行。孩子一次、二次、三次跌倒在地上，可他还是坚持着走下去，一直到不摔跤为止。有孩子学走路的耐心和集中他会作出多么大的成绩啊！</p>
<p>集中还要求另一样东西，那就是对自己要保持清醒。如果解释呢？难道应该不断思考自己、“分析”自己，还是别的？如果我们把一个人对一部机器的高度的注意力做例子，也许就能很容易地回答这个问题。譬如有汽车的人对自己的汽车总是很警觉的，任何一种细微的声响和马达功能的变化都不会被他放过。统一开车的人对马路表面的变化、前后车辆的速度及方向变化也十分灵敏。尽管如此开车的人并没有去思考这些现象，他只是处在一种清醒状态之中，对他所集中于的事（也就是安全行驶）所发生的任何变化都会做出反应。</p>
<p>我们用母亲对婴儿的态度做例子就能说明什么是对一个生命的清醒的关注。在婴儿还没有表达以前，母亲就能感觉到婴儿体内的一些变化、他的愿望和需要。婴儿一叫或一哭，母亲就会醒来，虽然平时比这更响的声音都不会吵醒她。这说明了母亲对孩子的每一种生命的表现都是很清醒的，母亲即不害怕，也不担心，而是助于一种清醒的平衡状态，能接受孩子发出的每一个重要信号。我们也可以以同样的方式清醒地面对自己。譬如在感觉到累或者消沉的时候，不应该听之任之和用随时可以捡来的消沉的想法去加剧这种感受，而应该问问自己：到底发生了什么？为什么我那么一蹶不振？同样在我们生气或者迷惑不解的时候，在我们开始想入非非的时候，都应该这样问自己。在所有这种情况下，终于的是要觉察内心的活动，而不是用各种各样的方法去找到为自己辩护的借口。这样我们就此次会听到内心的一种声音，在向我们讲述，为什么会害怕、消沉或者迷惑不解。一般人对自己体内的活动都有一定的警觉性；能感觉到每种变化和甚至能发现几乎感觉不到的疼痛。注意身体的变化是比较容易做到的，因为大多数人都了解自己的健康状态。电脑上对心灵变化就不可能那么灵敏，因为许多人还从来没见过一个对自己的内心活动保持清醒的人。对他们来说，衡量内心活动是否正常的标准是他们的父母和亲戚，或者是他们加入的社会集团，只有他们同这一标准没有区别，他们就决定自己很“正常”，也就没有兴趣去观察与他们不同的生活态度。譬如有许多人还从来没有见过一个有爱的能力的人或者一个独立完整的、具有勇气和能集中干事的人。为了能清醒地面对自己，必须要有一个设想，要知道什么叫做健康地、活跃地、充满人性地发挥人的作用。可是如果我们既没有在童年时代，也没有在后来的岁月里有过这种体验，我们又如何能得出这样的结论呢？对这个问题肯定不会有简单的回答，但这个问题却击中了我们教育制度的要害。</p>
<p>在传授知识方面，我们忘记了对人的发展来说是作重要的教诲，呢就是一个成熟的和慈爱的人的现身教育。在外面自己文化的某系阶段，或者在中国和印度，最有影响的是德高望重的人。教师不仅或不首先是传授知识，他的任务还包括培养学生具有一定的人的品质。在当代资本主义社会——这一点也适用于俄国共产主义，值得钦佩和作为榜样的人绝不是因为他们具有高尚的品质。电影明星、播音员，一些新闻记者以及政界和经济界的巨头是老百姓的榜样。这些人的主要资本常常是因为他们能够扬名四海。但尽管如此，情况还不至于糟到使人绝望。如果我们想一想，像阿尔贝特-施威策这样的人能在美国和其他地方出名，如果我们看到有许多可行的办法，能使我们的青年一代熟悉那些活着的和死去的优秀人物，并通过这些人了解人能不断完善自己，如果我们想到文学和所有艺术的那些不朽之作，我们就相信有可能去传授人应该如何清醒地、充满活力的生活。如果我们做不到这点，我们很可能会面临有一天我们整个的文化传统崩溃的下场。我们的文化传统首先不是以传授一些思想和知识为基础，而是传授做人的态度。如果我们的后代不在能经历这一传统，我们五千年的文化就会崩溃，即使人们孩子继续不断地传授和发展知识。</p>
<p>上面我们分析了在行驶所有艺术时必须具备的条件，现在我要谈一下掌握爱情艺术的必不可少的特殊条件。根据我对爱情本质的论述，获得爱的能力的主要条件是克服自恋。自恋倾向是人的一种态度，具有这种态度的人体验到的现实只是内心活动，主要是他们自己的贪婪和恐惧，对他们来说，外部世界的现象本身是不现实的，只有对他们有利或者威胁他们的食物才有意义。同自恋相反的是客观性，客观性就是对人和事物都有开放的态度，能实事求是地看待事物。这个意义上的客观性就是能从表面深入现象核心的实现主义。同自恋相反，客观性的基础不是与外部世界毫无关系，而是有强烈的联系。精神病的所有形式是没有客观性的极端形式，在这个意义上就是对客观事物没有开放的能力。对精神病患者来说，如果有现实的话，那也只存在于他的内心，就是他的恐惧和欲望。外部世界对他来说只是他内心世界的象征，只是他的创造物。我们做梦的时候，也有类似的现象。在梦里发生的具体事件象征内心活动，即使如此，睡梦中的无名还是深信梦里发生的事同我们在清醒状态感觉到的现实一样真实。</p>
<p>但是梦和精神病仅仅是缺乏客观性的极端例子。我们每个人对世界都有一个非客观的图像，一个被我们的自恋倾向所歪曲了的图像。难道还需要我举例说明吗？只要我们观察自己和邻居或者看报的话，是不难找到这样的例子的，只是由于自恋的程度不一，歪曲客观的程度也有高低。譬如一位妇女打电话给他的医生，告诉他，她很想当日下午去他那儿看病。医生说，他今天下午没有空，明天下午才有空。可能个妇女却说：“医生，可我住的地方离您那儿才五分钟的路！”这位妇女不能理解她住的虽然近却不能节省医生的时间。她完全是从自恋的角度出发看问题的：因为她节省了时间，所以医生也节省了时间。对她来说“我”是唯一的现实。</p>
<p>不那么极端的例子——或者也许仅仅是不能吗明显而已——常常出现在人与人的关系中。真不知有多少父母首先关兴趣的是他们的孩子是否听话，是否使他们高兴等等，而不是孩子自己经历了些什么，是怎么经历的——当然这些经历与父母无关。真不知有多少丈夫认为他们的妻子很专横，实际上是由于他们自己同母亲的幼稚的联系而把妻子的被一个要求解释为是限制他们的“自由”。真不知又有多少妻子认为她们的丈夫无能或者软弱，而实际上仅仅是因为丈夫不符合他们童年时代想象中的光彩夺目的骑士啊！</p>
<p>对其他民族缺乏客观性就更常见，也更具有危险性。一个民族会突然地把另一个民族看成是劣等的和敌对的，却自认为本民族体系一切优秀的和高贵的品质。敌人的行为用一种尺度衡量，而自己的行为却用另一种尺度衡量。甚至敌人的善意举动也被看作是险恶用心的产物，只是为了遮住我们和世界的眼目而已，可另一面有用高贵的动机来为自己的坏行为辩护。如果我们考察一下民族与民族、人与人之间的关系，我们一定会得出下列结论：客观性是例外，而不同程度的自恋是常规。</p>
<p>能进行客观思考的能力就是理智，以理智为基础的感情是谦恭。我们只有摆脱了童年时代妄图得到全知、全能的幻想，才能有客观性和运用自己的理智。</p>
<p>这在爱的艺术的实践上的表现在：能否学会爱取决于人的自恋程度和能不断培养自己的谦恭、客观性和理智。我们应该一辈子为此努力。谦恭和客观性同爱情一样不能只限于生活的一些范围。如果我对陌生人没有客观的态度，那我对自己的家也不会真正客观，反之亦然。</p>
<p>我想学会爱的艺术，我就应该在任何情况下都力求客观，并且能注意到在什么情况下我没有力求客观，并对此保持清醒的态度。我应该努力去认识一个被我自恋歪曲了的人的形象同这个人的实际面目，也就是同我的利益、困难和恐惧无关的实际面目之间的区别。有无客观性和理智是学会爱的艺术的一个关键性条件，人们应该对所有与自己所接触的人都能保持理智和客观。如果我们只对所爱之人保持客观，而以为对其他人就不需要客观性，那我们很快就能发现我们既不能处理好自己同所爱之人的关系，也处理不好同其他人的关系。</p>
<p>爱的能力取决于我们本人成熟的程度，以及在我们同世界和同自己的关系中能不能发展一种创造性的倾向。这种脱离自己的过程、诞生和成熟的过程需要另一种品质作为必不可少的条件：那就是信仰。爱情是以信仰为基础的<br>什么是信仰?难道信仰就一定是信仰上帝或者别的宗教教义?信仰是否同理智和理性的思考对立？信仰是不是只是一种无法证明的没有根底的知识呢?首先应该区别合理的信仰和非合理的信仰。我理解的非合理的信仰是指服从一种非理性权威的信仰(信仰一个人或者一种理想)。与此相反，合理的信仰是扎根于自已思想或感情体验的一种坚定的信念。合理的信仰首先不是信仰什么东西，而是一种确认，这种确认是符合建筑在自己真实经历上的坚定的信念。信仰是全部人格的一个性格特点，而不是同某些被看作为对的思想内容有关的东西。</p>
]]></content>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
</search>
