<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Internlm-01-书生·浦语大模型全链路开源体系</title>
    <url>/internlm/internlm-01/</url>
    <content><![CDATA[<h1 id="书生·浦语大模型全链路开源体系"><a href="#书生·浦语大模型全链路开源体系" class="headerlink" title="书生·浦语大模型全链路开源体系"></a>书生·浦语大模型全链路开源体系</h1><h2 id="大模型概述"><a href="#大模型概述" class="headerlink" title="大模型概述"></a>大模型概述</h2><h3 id="大模型成为了热门关键词"><a href="#大模型成为了热门关键词" class="headerlink" title="大模型成为了热门关键词"></a>大模型成为了热门关键词</h3><p>大模型成为发展通用人工智能的重要途经，图中展示了大模型的热度增长和 OpenAI GPT 系列的迭代过程。</p>
<p><img  src="image.png"  ><span class="image-caption">大模型成为热门关键词</span></p>
<h3 id="大模型成为了发展通用人工智能的重要途径"><a href="#大模型成为了发展通用人工智能的重要途径" class="headerlink" title="大模型成为了发展通用人工智能的重要途径"></a>大模型成为了发展通用人工智能的重要途径</h3><p><img  src="image-20240104194412083.png"  ><span class="image-caption">大模型成为了发展通用人工智能的重要途径</span></p>
<h2 id="书生·浦语大模型"><a href="#书生·浦语大模型" class="headerlink" title="书生·浦语大模型"></a>书生·浦语大模型</h2><h3 id="开源历程"><a href="#开源历程" class="headerlink" title="开源历程"></a>开源历程</h3><p><img  src="image-20240104194442165.png"  ><span class="image-caption">书生·浦语的开源历程</span></p>
<p>书生·浦语大模型从 6.7 开始，开源了 InternLM-7B、InternLM-Chat-7B、Lagent、InternLM-20B 等项目。</p>
<h3 id="系列模型"><a href="#系列模型" class="headerlink" title="系列模型"></a>系列模型</h3><p><img  src="image-20240104194623711.png"  ><span class="image-caption">书生·浦语系列模型</span></p>
<h3 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h3><p><img  src="image-20240104194715248.png"  ><span class="image-caption">性能比较</span></p>
<p>可以看出，书生·浦语 20B 模型全面领先相近量级的开源模型，并且达到了和 Llama2-70B 相近的水平。</p>
<h2 id="书生·浦语全链条开源体系"><a href="#书生·浦语全链条开源体系" class="headerlink" title="书生·浦语全链条开源体系"></a>书生·浦语全链条开源体系</h2><h3 id="体系概述"><a href="#体系概述" class="headerlink" title="体系概述"></a>体系概述</h3><p><strong><a href="https://github.com/orgs/InternLM/repositories?type=all">InternLM (github.com)</a></strong></p>
<p><img  src="image-20240104194911061.png"  ><span class="image-caption">体系概述</span></p>
<h3 id="数据开源"><a href="#数据开源" class="headerlink" title="数据开源"></a>数据开源</h3><p><img  src="image-20240104194948553.png"  ><span class="image-caption">数据开源</span></p>
<h3 id="预训练开源"><a href="#预训练开源" class="headerlink" title="预训练开源"></a>预训练开源</h3><p><img  src="image-20240104195041865.png"  ><span class="image-caption">预训练开源</span></p>
<h3 id="微调开源"><a href="#微调开源" class="headerlink" title="微调开源"></a>微调开源</h3><p><strong><a href="https://github.com/InternLM/xtuner">InternLM/xtuner: A toolkit for efficiently fine-tuning LLM (InternLM, Llama, Baichuan, Qwen, ChatGLM) (github.com)</a></strong></p>
<p><img  src="image-20240104195250624.png"  ><span class="image-caption">微调开源</span></p>
<h3 id="评测开源"><a href="#评测开源" class="headerlink" title="评测开源"></a>评测开源</h3><p><strong><a href="https://github.com/open-compass/OpenCompass/">open-compass/opencompass: OpenCompass is an LLM evaluation platform, supporting a wide range of models (LLaMA, LLaMa2, ChatGLM2, ChatGPT, Claude, etc) over 50+ datasets. (github.com)</a></strong></p>
<p><img  src="image-20240104195322676.png"  ><span class="image-caption">评测开源-题目类型</span></p>
<p><img  src="image-20240104195342281.png"  ><span class="image-caption">评测开源-OpenCompass</span></p>
<p><img  src="image-20240104195354357.png"  ><span class="image-caption">OpenCompass 架构</span></p>
<h3 id="部署开源"><a href="#部署开源" class="headerlink" title="部署开源"></a>部署开源</h3><p><strong><a href="https://github.com/InternLM/lmdeploy">InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs. (github.com)</a></strong></p>
<p><img  src="image-20240104195614821.png"  ><span class="image-caption">部署开源</span></p>
<h3 id="智能体开源"><a href="#智能体开源" class="headerlink" title="智能体开源"></a>智能体开源</h3><p><strong><a href="https://github.com/InternLM/lagent">InternLM/lagent: A lightweight framework for building LLM-based agents (github.com)</a></strong></p>
<p><img  src="image-20240104195658280.png"  ><span class="image-caption">智能体开源</span></p>
<h3 id="智能体工具箱"><a href="#智能体工具箱" class="headerlink" title="智能体工具箱"></a>智能体工具箱</h3><p><strong><a href="https://github.com/InternLM/agentlego">InternLM/agentlego: Enhance LLM agents with versatile tool APIs (github.com)</a></strong></p>
<p><img  src="image-20240104195750076.png"  ><span class="image-caption">智能体工具箱</span></p>
<h2 id="从模型到应用"><a href="#从模型到应用" class="headerlink" title="从模型到应用"></a>从模型到应用</h2><p><img  src="image-20240104194844022.png"  ><span class="image-caption">如何从模型到应用</span></p>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>Internlm-02-浦语大模型趣味 Demo</title>
    <url>/internlm/internlm-02/</url>
    <content><![CDATA[<h1 id="浦语大模型趣味-Demo"><a href="#浦语大模型趣味-Demo" class="headerlink" title="浦语大模型趣味 Demo"></a>浦语大模型趣味 Demo</h1><h2 id="大模型及-InternLM-模型简介"><a href="#大模型及-InternLM-模型简介" class="headerlink" title="大模型及 InternLM 模型简介"></a>大模型及 InternLM 模型简介</h2><h3 id="什么是大模型"><a href="#什么是大模型" class="headerlink" title="什么是大模型"></a>什么是大模型</h3><p>大模型是指在机器学习或人工智能领域中具有巨大参数数量和强大计算能力的模型。它们利用海量数据进行训练，拥有数十亿甚至数千亿个参数。大模型的崛起归因于数据量增长、计算能力提升和算法优化等因素。它们在自然语言处理、计算机视觉、语音识别等任务中展现出惊人性能，常采用深度神经网络结构，如Transformer、BERT、GPT等。</p>
<p>这些模型的优势在于能够捕捉和理解数据中更复杂、抽象的特征和关系。通过大规模参数的学习，它们可以提高泛化能力，在未经大量特定领域数据训练的情况下表现优异。然而，它们也面临着挑战，如巨大计算资源需求、高昂训练成本、对大规模数据的依赖和可解释性等问题。因此，在性能、成本和道德等方面需要权衡考量其应用和发展。</p>
<h2 id="InternLM-模型全链条开源"><a href="#InternLM-模型全链条开源" class="headerlink" title="InternLM 模型全链条开源"></a>InternLM 模型全链条开源</h2><p>包括了 InternLM、Lagent、浦语·灵笔等项目，详情可见：<br><a href="https://github.com/InternLM/InternLM">InternLM</a><br><a href="https://enableasync.github.io/internlm/internlm-01/">EnableAsync 的博客</a></p>
<h2 id="InternLM-Chat-7B-智能对话-Demo"><a href="#InternLM-Chat-7B-智能对话-Demo" class="headerlink" title="InternLM-Chat-7B 智能对话 Demo"></a>InternLM-Chat-7B 智能对话 Demo</h2><p>InternLM已经开源了一个70亿参数的基础模型和一个专为实际场景量身定制的聊天模型。该模型具有以下特点：</p>
<ul>
<li>它利用数万亿高质量标记进行训练，建立了强大的知识库。</li>
<li>支持8,000的上下文窗口长度，能够处理更长的输入序列并具备更强的推理能力。</li>
<li>为用户提供了多功能工具集，灵活构建自己的工作流程。</li>
</ul>
<h3 id="demo-代码"><a href="#demo-代码" class="headerlink" title="demo 代码"></a>demo 代码</h3><p>最简单的 <code>cli_demo</code> 代码如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM<br><br><br>model_name_or_path = <span class="hljs-string">&quot;/root/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span><br><br>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=<span class="hljs-literal">True</span>)<br>model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=<span class="hljs-literal">True</span>, torch_dtype=torch.bfloat16, device_map=<span class="hljs-string">&#x27;auto&#x27;</span>)<br>model = model.<span class="hljs-built_in">eval</span>()<br><br>system_prompt = <span class="hljs-string">&quot;&quot;&quot;You are an AI assistant whose name is InternLM (书生·浦语).</span><br><span class="hljs-string">- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span><br><span class="hljs-string">- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>messages = [(system_prompt, <span class="hljs-string">&#x27;&#x27;</span>)]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;=============Welcome to InternLM chatbot, type &#x27;exit&#x27; to exit.=============&quot;</span>)<br><br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>    input_text = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;User  &gt;&gt;&gt; &quot;</span>)<br>    input_text.replace(<span class="hljs-string">&#x27; &#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    <span class="hljs-keyword">if</span> input_text == <span class="hljs-string">&quot;exit&quot;</span>:<br>        <span class="hljs-keyword">break</span><br>    response, history = model.chat(tokenizer, input_text, history=messages)<br>    messages.append((input_text, response))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;robot &gt;&gt;&gt; <span class="hljs-subst">&#123;response&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></p>
<h3 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h3><p>运行效果如下图所示：</p>
<p><img  src="story.png"  ><span class="image-caption">InternLM生成小故事</span></p>
<h2 id="Lagent-智能体工具调用-Demo"><a href="#Lagent-智能体工具调用-Demo" class="headerlink" title="Lagent 智能体工具调用 Demo"></a>Lagent 智能体工具调用 Demo</h2><p>Lagent 是一个轻量级、开源的基于大语言模型的智能体（agent）框架，支持用户快速地将一个大语言模型转变为多种类型的智能体，并提供了一些典型工具为大语言模型赋能。通过 Lagent 框架可以更好的发挥 InternLM 的全部性能。</p>
<h3 id="demo-代码-1"><a href="#demo-代码-1" class="headerlink" title="demo 代码"></a>demo 代码</h3><p>教程中提供了一个 web demo 如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> copy<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st<br><span class="hljs-keyword">from</span> streamlit.logger <span class="hljs-keyword">import</span> get_logger<br><br><span class="hljs-keyword">from</span> lagent.actions <span class="hljs-keyword">import</span> ActionExecutor, GoogleSearch, PythonInterpreter<br><span class="hljs-keyword">from</span> lagent.agents.react <span class="hljs-keyword">import</span> ReAct<br><span class="hljs-keyword">from</span> lagent.llms <span class="hljs-keyword">import</span> GPTAPI<br><span class="hljs-keyword">from</span> lagent.llms.huggingface <span class="hljs-keyword">import</span> HFTransformerCasualLM<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SessionState</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize session state variables.&quot;&quot;&quot;</span><br>        st.session_state[<span class="hljs-string">&#x27;assistant&#x27;</span>] = []<br>        st.session_state[<span class="hljs-string">&#x27;user&#x27;</span>] = []<br><br>        <span class="hljs-comment">#action_list = [PythonInterpreter(), GoogleSearch()]</span><br>        action_list = [PythonInterpreter()]<br>        st.session_state[<span class="hljs-string">&#x27;plugin_map&#x27;</span>] = &#123;<br>            action.name: action<br>            <span class="hljs-keyword">for</span> action <span class="hljs-keyword">in</span> action_list<br>        &#125;<br>        st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>] = &#123;&#125;<br>        st.session_state[<span class="hljs-string">&#x27;model_selected&#x27;</span>] = <span class="hljs-literal">None</span><br>        st.session_state[<span class="hljs-string">&#x27;plugin_actions&#x27;</span>] = <span class="hljs-built_in">set</span>()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">clear_state</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Clear the existing session state.&quot;&quot;&quot;</span><br>        st.session_state[<span class="hljs-string">&#x27;assistant&#x27;</span>] = []<br>        st.session_state[<span class="hljs-string">&#x27;user&#x27;</span>] = []<br>        st.session_state[<span class="hljs-string">&#x27;model_selected&#x27;</span>] = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;chatbot&#x27;</span> <span class="hljs-keyword">in</span> st.session_state:<br>            st.session_state[<span class="hljs-string">&#x27;chatbot&#x27;</span>]._session_history = []<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">StreamlitUI</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, session_state: SessionState</span>):<br>        self.init_streamlit()<br>        self.session_state = session_state<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_streamlit</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize Streamlit&#x27;s UI settings.&quot;&quot;&quot;</span><br>        st.set_page_config(<br>            layout=<span class="hljs-string">&#x27;wide&#x27;</span>,<br>            page_title=<span class="hljs-string">&#x27;lagent-web&#x27;</span>,<br>            page_icon=<span class="hljs-string">&#x27;./docs/imgs/lagent_icon.png&#x27;</span>)<br>        <span class="hljs-comment"># st.header(&#x27;:robot_face: :blue[Lagent] Web Demo &#x27;, divider=&#x27;rainbow&#x27;)</span><br>        st.sidebar.title(<span class="hljs-string">&#x27;模型控制&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_sidebar</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Setup the sidebar for model and plugin selection.&quot;&quot;&quot;</span><br>        model_name = st.sidebar.selectbox(<br>            <span class="hljs-string">&#x27;模型选择：&#x27;</span>, options=[<span class="hljs-string">&#x27;gpt-3.5-turbo&#x27;</span>,<span class="hljs-string">&#x27;internlm&#x27;</span>])<br>        <span class="hljs-keyword">if</span> model_name != st.session_state[<span class="hljs-string">&#x27;model_selected&#x27;</span>]:<br>            model = self.init_model(model_name)<br>            self.session_state.clear_state()<br>            st.session_state[<span class="hljs-string">&#x27;model_selected&#x27;</span>] = model_name<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;chatbot&#x27;</span> <span class="hljs-keyword">in</span> st.session_state:<br>                <span class="hljs-keyword">del</span> st.session_state[<span class="hljs-string">&#x27;chatbot&#x27;</span>]<br>        <span class="hljs-keyword">else</span>:<br>            model = st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>][model_name]<br><br>        plugin_name = st.sidebar.multiselect(<br>            <span class="hljs-string">&#x27;插件选择&#x27;</span>,<br>            options=<span class="hljs-built_in">list</span>(st.session_state[<span class="hljs-string">&#x27;plugin_map&#x27;</span>].keys()),<br>            default=[<span class="hljs-built_in">list</span>(st.session_state[<span class="hljs-string">&#x27;plugin_map&#x27;</span>].keys())[<span class="hljs-number">0</span>]],<br>        )<br><br>        plugin_action = [<br>            st.session_state[<span class="hljs-string">&#x27;plugin_map&#x27;</span>][name] <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> plugin_name<br>        ]<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;chatbot&#x27;</span> <span class="hljs-keyword">in</span> st.session_state:<br>            st.session_state[<span class="hljs-string">&#x27;chatbot&#x27;</span>]._action_executor = ActionExecutor(<br>                actions=plugin_action)<br>        <span class="hljs-keyword">if</span> st.sidebar.button(<span class="hljs-string">&#x27;清空对话&#x27;</span>, key=<span class="hljs-string">&#x27;clear&#x27;</span>):<br>            self.session_state.clear_state()<br>        uploaded_file = st.sidebar.file_uploader(<br>            <span class="hljs-string">&#x27;上传文件&#x27;</span>, <span class="hljs-built_in">type</span>=[<span class="hljs-string">&#x27;png&#x27;</span>, <span class="hljs-string">&#x27;jpg&#x27;</span>, <span class="hljs-string">&#x27;jpeg&#x27;</span>, <span class="hljs-string">&#x27;mp4&#x27;</span>, <span class="hljs-string">&#x27;mp3&#x27;</span>, <span class="hljs-string">&#x27;wav&#x27;</span>])<br>        <span class="hljs-keyword">return</span> model_name, model, plugin_action, uploaded_file<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_model</span>(<span class="hljs-params">self, option</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the model based on the selected option.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> option <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>]:<br>            <span class="hljs-keyword">if</span> option.startswith(<span class="hljs-string">&#x27;gpt&#x27;</span>):<br>                st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>][option] = GPTAPI(<br>                    model_type=option)<br>            <span class="hljs-keyword">else</span>:<br>                st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>][option] = HFTransformerCasualLM(<br>                    <span class="hljs-string">&#x27;/root/model/Shanghai_AI_Laboratory/internlm-chat-7b&#x27;</span>)<br>        <span class="hljs-keyword">return</span> st.session_state[<span class="hljs-string">&#x27;model_map&#x27;</span>][option]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_chatbot</span>(<span class="hljs-params">self, model, plugin_action</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the chatbot with the given model and plugin actions.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> ReAct(<br>            llm=model, action_executor=ActionExecutor(actions=plugin_action))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render_user</span>(<span class="hljs-params">self, prompt: <span class="hljs-built_in">str</span></span>):<br>        <span class="hljs-keyword">with</span> st.chat_message(<span class="hljs-string">&#x27;user&#x27;</span>):<br>            st.markdown(prompt)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render_assistant</span>(<span class="hljs-params">self, agent_return</span>):<br>        <span class="hljs-keyword">with</span> st.chat_message(<span class="hljs-string">&#x27;assistant&#x27;</span>):<br>            <span class="hljs-keyword">for</span> action <span class="hljs-keyword">in</span> agent_return.actions:<br>                <span class="hljs-keyword">if</span> (action):<br>                    self.render_action(action)<br>            st.markdown(agent_return.response)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render_action</span>(<span class="hljs-params">self, action</span>):<br>        <span class="hljs-keyword">with</span> st.expander(action.<span class="hljs-built_in">type</span>, expanded=<span class="hljs-literal">True</span>):<br>            st.markdown(<br>                <span class="hljs-string">&quot;&lt;p style=&#x27;text-align: left;display:flex;&#x27;&gt; &lt;span style=&#x27;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#x27;&gt;插    件&lt;/span&gt;&lt;span style=&#x27;width:14px;text-align:left;display:block;&#x27;&gt;:&lt;/span&gt;&lt;span style=&#x27;flex:1;&#x27;&gt;&quot;</span>  <span class="hljs-comment"># noqa E501</span><br>                + action.<span class="hljs-built_in">type</span> + <span class="hljs-string">&#x27;&lt;/span&gt;&lt;/p&gt;&#x27;</span>,<br>                unsafe_allow_html=<span class="hljs-literal">True</span>)<br>            st.markdown(<br>                <span class="hljs-string">&quot;&lt;p style=&#x27;text-align: left;display:flex;&#x27;&gt; &lt;span style=&#x27;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#x27;&gt;思考步骤&lt;/span&gt;&lt;span style=&#x27;width:14px;text-align:left;display:block;&#x27;&gt;:&lt;/span&gt;&lt;span style=&#x27;flex:1;&#x27;&gt;&quot;</span>  <span class="hljs-comment"># noqa E501</span><br>                + action.thought + <span class="hljs-string">&#x27;&lt;/span&gt;&lt;/p&gt;&#x27;</span>,<br>                unsafe_allow_html=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">isinstance</span>(action.args, <span class="hljs-built_in">dict</span>) <span class="hljs-keyword">and</span> <span class="hljs-string">&#x27;text&#x27;</span> <span class="hljs-keyword">in</span> action.args):<br>                st.markdown(<br>                    <span class="hljs-string">&quot;&lt;p style=&#x27;text-align: left;display:flex;&#x27;&gt;&lt;span style=&#x27;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#x27;&gt; 执行内容&lt;/span&gt;&lt;span style=&#x27;width:14px;text-align:left;display:block;&#x27;&gt;:&lt;/span&gt;&lt;/p&gt;&quot;</span>,  <span class="hljs-comment"># noqa E501</span><br>                    unsafe_allow_html=<span class="hljs-literal">True</span>)<br>                st.markdown(action.args[<span class="hljs-string">&#x27;text&#x27;</span>])<br>            self.render_action_results(action)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">render_action_results</span>(<span class="hljs-params">self, action</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Render the results of action, including text, images, videos, and</span><br><span class="hljs-string">        audios.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">isinstance</span>(action.result, <span class="hljs-built_in">dict</span>)):<br>            st.markdown(<br>                <span class="hljs-string">&quot;&lt;p style=&#x27;text-align: left;display:flex;&#x27;&gt;&lt;span style=&#x27;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#x27;&gt; 执行结果&lt;/span&gt;&lt;span style=&#x27;width:14px;text-align:left;display:block;&#x27;&gt;:&lt;/span&gt;&lt;/p&gt;&quot;</span>,  <span class="hljs-comment"># noqa E501</span><br>                unsafe_allow_html=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;text&#x27;</span> <span class="hljs-keyword">in</span> action.result:<br>                st.markdown(<br>                    <span class="hljs-string">&quot;&lt;p style=&#x27;text-align: left;&#x27;&gt;&quot;</span> + action.result[<span class="hljs-string">&#x27;text&#x27;</span>] +<br>                    <span class="hljs-string">&#x27;&lt;/p&gt;&#x27;</span>,<br>                    unsafe_allow_html=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;image&#x27;</span> <span class="hljs-keyword">in</span> action.result:<br>                image_path = action.result[<span class="hljs-string">&#x27;image&#x27;</span>]<br>                image_data = <span class="hljs-built_in">open</span>(image_path, <span class="hljs-string">&#x27;rb&#x27;</span>).read()<br>                st.image(image_data, caption=<span class="hljs-string">&#x27;Generated Image&#x27;</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;video&#x27;</span> <span class="hljs-keyword">in</span> action.result:<br>                video_data = action.result[<span class="hljs-string">&#x27;video&#x27;</span>]<br>                video_data = <span class="hljs-built_in">open</span>(video_data, <span class="hljs-string">&#x27;rb&#x27;</span>).read()<br>                st.video(video_data)<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;audio&#x27;</span> <span class="hljs-keyword">in</span> action.result:<br>                audio_data = action.result[<span class="hljs-string">&#x27;audio&#x27;</span>]<br>                audio_data = <span class="hljs-built_in">open</span>(audio_data, <span class="hljs-string">&#x27;rb&#x27;</span>).read()<br>                st.audio(audio_data)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    logger = get_logger(__name__)<br>    <span class="hljs-comment"># Initialize Streamlit UI and setup sidebar</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ui&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:<br>        session_state = SessionState()<br>        session_state.init_state()<br>        st.session_state[<span class="hljs-string">&#x27;ui&#x27;</span>] = StreamlitUI(session_state)<br><br>    <span class="hljs-keyword">else</span>:<br>        st.set_page_config(<br>            layout=<span class="hljs-string">&#x27;wide&#x27;</span>,<br>            page_title=<span class="hljs-string">&#x27;lagent-web&#x27;</span>,<br>            page_icon=<span class="hljs-string">&#x27;./docs/imgs/lagent_icon.png&#x27;</span>)<br>        <span class="hljs-comment"># st.header(&#x27;:robot_face: :blue[Lagent] Web Demo &#x27;, divider=&#x27;rainbow&#x27;)</span><br>    model_name, model, plugin_action, uploaded_file = st.session_state[<br>        <span class="hljs-string">&#x27;ui&#x27;</span>].setup_sidebar()<br><br>    <span class="hljs-comment"># Initialize chatbot if it is not already initialized</span><br>    <span class="hljs-comment"># or if the model has changed</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;chatbot&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state <span class="hljs-keyword">or</span> model != st.session_state[<br>            <span class="hljs-string">&#x27;chatbot&#x27;</span>]._llm:<br>        st.session_state[<span class="hljs-string">&#x27;chatbot&#x27;</span>] = st.session_state[<br>            <span class="hljs-string">&#x27;ui&#x27;</span>].initialize_chatbot(model, plugin_action)<br><br>    <span class="hljs-keyword">for</span> prompt, agent_return <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(st.session_state[<span class="hljs-string">&#x27;user&#x27;</span>],<br>                                    st.session_state[<span class="hljs-string">&#x27;assistant&#x27;</span>]):<br>        st.session_state[<span class="hljs-string">&#x27;ui&#x27;</span>].render_user(prompt)<br>        st.session_state[<span class="hljs-string">&#x27;ui&#x27;</span>].render_assistant(agent_return)<br>    <span class="hljs-comment"># User input form at the bottom (this part will be at the bottom)</span><br>    <span class="hljs-comment"># with st.form(key=&#x27;my_form&#x27;, clear_on_submit=True):</span><br><br>    <span class="hljs-keyword">if</span> user_input := st.chat_input(<span class="hljs-string">&#x27;&#x27;</span>):<br>        st.session_state[<span class="hljs-string">&#x27;ui&#x27;</span>].render_user(user_input)<br>        st.session_state[<span class="hljs-string">&#x27;user&#x27;</span>].append(user_input)<br>        <span class="hljs-comment"># Add file uploader to sidebar</span><br>        <span class="hljs-keyword">if</span> uploaded_file:<br>            file_bytes = uploaded_file.read()<br>            file_type = uploaded_file.<span class="hljs-built_in">type</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;image&#x27;</span> <span class="hljs-keyword">in</span> file_type:<br>                st.image(file_bytes, caption=<span class="hljs-string">&#x27;Uploaded Image&#x27;</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;video&#x27;</span> <span class="hljs-keyword">in</span> file_type:<br>                st.video(file_bytes, caption=<span class="hljs-string">&#x27;Uploaded Video&#x27;</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;audio&#x27;</span> <span class="hljs-keyword">in</span> file_type:<br>                st.audio(file_bytes, caption=<span class="hljs-string">&#x27;Uploaded Audio&#x27;</span>)<br>            <span class="hljs-comment"># Save the file to a temporary location and get the path</span><br>            file_path = os.path.join(root_dir, uploaded_file.name)<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> tmpfile:<br>                tmpfile.write(file_bytes)<br>            st.write(<span class="hljs-string">f&#x27;File saved at: <span class="hljs-subst">&#123;file_path&#125;</span>&#x27;</span>)<br>            user_input = <span class="hljs-string">&#x27;我上传了一个图像，路径为: &#123;file_path&#125;. &#123;user_input&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                file_path=file_path, user_input=user_input)<br>        agent_return = st.session_state[<span class="hljs-string">&#x27;chatbot&#x27;</span>].chat(user_input)<br>        st.session_state[<span class="hljs-string">&#x27;assistant&#x27;</span>].append(copy.deepcopy(agent_return))<br>        logger.info(agent_return.inner_steps)<br>        st.session_state[<span class="hljs-string">&#x27;ui&#x27;</span>].render_assistant(agent_return)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))<br>    root_dir = os.path.join(root_dir, <span class="hljs-string">&#x27;tmp_dir&#x27;</span>)<br>    os.makedirs(root_dir, exist_ok=<span class="hljs-literal">True</span>)<br>    main()<br><br></code></pre></td></tr></table></figure></p>
<h3 id="demo-运行方式"><a href="#demo-运行方式" class="headerlink" title="demo 运行方式"></a>demo 运行方式</h3><p>demo 运行方式如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">streamlit run /root/code/lagent/examples/react_web_demo.py --server.address 127.0.0.1 --server.port 6006<br></code></pre></td></tr></table></figure>
<h3 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h3><p><img  src="lagent-1.png"  ><span class="image-caption">Lagent效果展示1</span></p>
<p><img  src="lagent-2.png"  ><span class="image-caption">Lagent效果展示2</span></p>
<p><img  src="lagent-3.png"  ><span class="image-caption">Lagent效果展示3</span></p>
<p><img  src="lagent-4.png"  ><span class="image-caption">Lagent效果展示4</span></p>
<p>可以看到，lagent能够通过 python 代码解决一些数学问题，而对于很困难的数学问题，解决起来会出现一些问题。</p>
<h2 id="浦语·灵笔图文理解创作-Demo"><a href="#浦语·灵笔图文理解创作-Demo" class="headerlink" title="浦语·灵笔图文理解创作 Demo"></a>浦语·灵笔图文理解创作 Demo</h2><p><strong>浦语·灵笔</strong>是基于<a href="https://github.com/InternLM/InternLM/tree/main">书生·浦语</a>大语言模型研发的视觉-语言大模型，提供出色的图文理解和创作能力，具有多项优势：</p>
<ul>
<li><p><strong>图文交错创作</strong>: 浦语·灵笔可以为用户打造图文并貌的专属文章。生成的文章文采斐然，图文相得益彰，提供沉浸式的阅读体验。这一能力由以下步骤实现：</p>
<ol>
<li><strong>理解用户指令，创作符合要求的长文章</strong>。</li>
<li><strong>智能分析文章，自动规划插图的理想位置，确定图像内容需求。</strong></li>
<li><strong>多层次智能筛选，从图库中锁定最完美的图片。</strong></li>
</ol>
</li>
<li><p><strong>基于丰富多模态知识的图文理解</strong>: 浦语·灵笔设计了高效的训练策略，为模型注入海量的多模态概念和知识数据，赋予其强大的图文理解和对话能力。</p>
</li>
<li><strong>杰出性能</strong>: 浦语·灵笔在多项视觉语言大模型的主流评测上均取得了最佳性能，包括<a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation">MME Benchmark</a> (英文评测), <a href="https://opencompass.org.cn/leaderboard-multimodal">MMBench</a> (英文评测), <a href="https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard">Seed-Bench</a> (英文评测), <a href="https://opencompass.org.cn/leaderboard-multimodal">CCBench</a>(中文评测), <a href="https://opencompass.org.cn/leaderboard-multimodal">MMBench-CN</a> (中文评测)。<h3 id="demo-代码-2"><a href="#demo-代码-2" class="headerlink" title="demo 代码"></a>demo 代码</h3></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/code<br>git <span class="hljs-built_in">clone</span> https://gitee.com/internlm/InternLM-XComposer.git<br><span class="hljs-built_in">cd</span> /root/code/InternLM-XComposer<br>git checkout 3e8c79051a1356b9c388a6447867355c0634932d  <span class="hljs-comment"># 最好保证和教程的 commit 版本一致</span><br></code></pre></td></tr></table></figure>
<h3 id="demo-运行方式-1"><a href="#demo-运行方式-1" class="headerlink" title="demo 运行方式"></a>demo 运行方式</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/code/InternLM-XComposer<br>python examples/web_demo.py  \<br>    --folder /root/model/Shanghai_AI_Laboratory/internlm-xcomposer-7b \<br>    --num_gpus 1 \<br>    --port 6006<br></code></pre></td></tr></table></figure>
<h3 id="效果展示-1"><a href="#效果展示-1" class="headerlink" title="效果展示"></a>效果展示</h3><p><img  src="灵笔-1.png"  ><span class="image-caption">浦语·灵笔效果展示1</span></p>
<p><img  src="灵笔-2.png"  ><span class="image-caption">浦语·灵笔效果展示2</span></p>
<p><img  src="灵笔-3.png"  ><span class="image-caption">浦语·灵笔效果展示3</span></p>
<h2 id="huggingface-hub-下载文件"><a href="#huggingface-hub-下载文件" class="headerlink" title="huggingface_hub 下载文件"></a>huggingface_hub 下载文件</h2><p><img  src="huggingface_hub_download.png"  ><span class="image-caption">使用 huggingface_hub 库下载文件</span></p>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>Kind 的一些使用心得</title>
    <url>/uncategorized/kind/</url>
    <content><![CDATA[<h1 id="Kind-的一些使用心得"><a href="#Kind-的一些使用心得" class="headerlink" title="Kind 的一些使用心得"></a>Kind 的一些使用心得</h1><p>因为 Kind 启动相比于 Minikube 更快，而且支持多 Node，所以现在换成了 Kind，这里记录一些 Kind 的使用心得。</p>
<h2 id="1-Kind-安装"><a href="#1-Kind-安装" class="headerlink" title="1. Kind 安装"></a>1. Kind 安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64<br><span class="hljs-built_in">chmod</span> +x ./kind<br><span class="hljs-built_in">mv</span> ./kind /usr/bin/kind<br></code></pre></td></tr></table></figure>
<h2 id="2-使用-Kind-创建含有两个-Node-的-kubernetes-集群"><a href="#2-使用-Kind-创建含有两个-Node-的-kubernetes-集群" class="headerlink" title="2. 使用 Kind 创建含有两个 Node 的 kubernetes 集群"></a>2. 使用 Kind 创建含有两个 Node 的 kubernetes 集群</h2><h3 id="1-创建配置文件"><a href="#1-创建配置文件" class="headerlink" title="1. 创建配置文件"></a>1. 创建配置文件</h3><p>这里我创建了两个 Node，使用以下配置文件，并将其命名为 <code>kind.yaml</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># a cluster with 1 control-plane nodes and 2 workers</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span><br><span class="hljs-attr">nodes:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br></code></pre></td></tr></table></figure>
<h3 id="2-创建集群"><a href="#2-创建集群" class="headerlink" title="2. 创建集群"></a>2. 创建集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">sudo kind create cluster --config kind.yaml<br></code></pre></td></tr></table></figure>
<p>这里需要注意的点有：</p>
<ol>
<li>不要设置集群 name，在我本地，如果设置了 name 会导致 kubeconfig 无法导出。</li>
<li>要使用 sudo，在我本地，如果不使用 sudo 会导致无法创建集群，原因未知。</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">Creating cluster <span class="hljs-string">&quot;kind&quot;</span> ...<br> ✓ Ensuring node image (kindest/node:v1.21.1) 🖼<br> ✓ Preparing nodes 📦 📦 📦  <br> ✓ Writing configuration 📜 <br> ✓ Starting control-plane 🕹️ <br> ✓ Installing CNI 🔌 <br> ✓ Installing StorageClass 💾 <br> ✓ Joining worker nodes 🚜 <br>Set kubectl context to <span class="hljs-string">&quot;kind-kind&quot;</span><br>You can now use your cluster with:<br><br>kubectl cluster-info --context kind-kind<br></code></pre></td></tr></table></figure>
<p>如果出现以上信息表示创建成功，可以进行下一步。</p>
<h3 id="3-导出-kubeconfig"><a href="#3-导出-kubeconfig" class="headerlink" title="3. 导出 kubeconfig"></a>3. 导出 kubeconfig</h3><figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">kind export kubeconfig<br></code></pre></td></tr></table></figure>
<p>如果不使用这一步，会导致使用 <code>kubectl</code> 的时候必须加上 <code>sudo</code>，否则无法连接到 kubernetes。</p>
<h2 id="3-安装-kubernetes-dashboard"><a href="#3-安装-kubernetes-dashboard" class="headerlink" title="3. 安装 kubernetes-dashboard"></a>3. 安装 kubernetes-dashboard</h2><h3 id="1-使用-helm-安装-dashboard"><a href="#1-使用-helm-安装-dashboard" class="headerlink" title="1. 使用 helm 安装 dashboard"></a>1. 使用 helm 安装 dashboard</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Add kubernetes-dashboard repository</span><br>helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/<br>helm repo update<br><span class="hljs-comment"># Deploy a Helm Release named &quot;dashboard&quot; using the kubernetes-dashboard chart</span><br>helm install dashboard kubernetes-dashboard/kubernetes-dashboard<br></code></pre></td></tr></table></figure>
<h3 id="2-转发-dashboard-pod"><a href="#2-转发-dashboard-pod" class="headerlink" title="2. 转发 dashboard pod"></a>2. 转发 dashboard pod</h3><p>这一步的目的是在本地访问部署了 dashboard 的 pod</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> POD_NAME=$(kubectl get pods -n default -l <span class="hljs-string">&quot;app.kubernetes.io/name=kubernetes-dashboard,app.kubernetes.io/instance=dashboard&quot;</span> -o jsonpath=<span class="hljs-string">&quot;&#123;.items[0].metadata.name&#125;&quot;</span>)<br>  <span class="hljs-built_in">echo</span> https://127.0.0.1:8443/<br>  kubectl -n default port-forward <span class="hljs-variable">$POD_NAME</span> 8443:8443<br></code></pre></td></tr></table></figure>
<p>之后会提示</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">Forwarding from 127.0.0.1:8443 -&gt; 8443<br>Forwarding from [::1]:8443 -&gt; 8443<br></code></pre></td></tr></table></figure>
<p>说明转发成功，此时访问 <a href="https://127.0.0.1:8443/">https://127.0.0.1:8443/</a> ，注意是 https</p>
<h3 id="2-1-或者可以不转发使用-service-暴露服务"><a href="#2-1-或者可以不转发使用-service-暴露服务" class="headerlink" title="2.1 或者可以不转发使用 service 暴露服务"></a>2.1 或者可以不转发使用 service 暴露服务</h3><p>这里为了测试使用了 NodePort 方式暴露</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">kubectl expose deploy dashboard-kubernetes-dashboard --name dashboard-nodeport --port 8443 --target-port=8443 --<span class="hljs-built_in">type</span>=NodePort<br></code></pre></td></tr></table></figure>
<h3 id="3-生成-token"><a href="#3-生成-token" class="headerlink" title="3. 生成 token"></a>3. 生成 token</h3><p>不出意外 dashboard 需要 token 来登录，使用以下步骤来生成 token：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">kubectl create serviceaccount dashboard -n default<br>kubectl create rolebinding def-ns-admin --clusterrole=admin --serviceaccount=default:def-ns-admin<br>kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=default:dashboard<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">kubectl describe sa dashboard<br>Name:                dashboard<br>Namespace:           default<br>Labels:              &lt;none&gt;<br>Annotations:         &lt;none&gt;<br>Image pull secrets:  &lt;none&gt;<br>Mountable secrets:   dashboard-token-vzzjn<br>Tokens:              dashboard-token-vzzjn<br>Events:              &lt;none&gt;<br></code></pre></td></tr></table></figure>
<p>这里可以看到 <code>dashboard-token-vzzjn</code> 就是我们需要的 token，使用以下命令显示具体内容：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">kubectl describe secret dashboard-token-vzzjn<br></code></pre></td></tr></table></figure>
<p>之后就可以将具体的 token 粘贴在 dashboard 中登录。</p>
]]></content>
      <tags>
        <tag>k8s, kind</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 的一些使用心得</title>
    <url>/linux/linux/</url>
    <content><![CDATA[<h1 id="关闭-kde-文件索引程序"><a href="#关闭-kde-文件索引程序" class="headerlink" title="关闭 kde 文件索引程序"></a>关闭 kde 文件索引程序</h1><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">balooctl <span class="hljs-built_in">suspend</span><br>balooctl <span class="hljs-built_in">disable</span><br></code></pre></td></tr></table></figure>
<h1 id="Linux-下抓-HTTPS-包"><a href="#Linux-下抓-HTTPS-包" class="headerlink" title="Linux 下抓 HTTPS 包"></a>Linux 下抓 HTTPS 包</h1><h2 id="使用-MITMProxy"><a href="#使用-MITMProxy" class="headerlink" title="使用 MITMProxy"></a>使用 MITMProxy</h2><ol>
<li>运行 MITMProxy</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">docker run --<span class="hljs-built_in">rm</span> -it -p 18080:8080 -p 127.0.0.1:8081:8081 -v ~/.mitmproxy:/home/mitmproxy/.mitmproxy  mitmproxy/mitmproxy mitmweb --web-host 0.0.0.0 --<span class="hljs-built_in">set</span> block_global=<span class="hljs-literal">false</span> --<span class="hljs-built_in">set</span> ssl_insecure=<span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure>
<ol>
<li>导入证书至浏览器或其他工具</li>
<li>使用代理访问 HTTPS 页面</li>
</ol>
<h1 id="更新-ubuntu-22-04-之后网易云音乐无法使用"><a href="#更新-ubuntu-22-04-之后网易云音乐无法使用" class="headerlink" title="更新 ubuntu 22.04 之后网易云音乐无法使用"></a>更新 ubuntu 22.04 之后网易云音乐无法使用</h1><p>修改 <code>/opt/netease/netease-cloud-music/netease-cloud-music.bash</code> 为以下内容<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/sh</span><br>HERE=<span class="hljs-string">&quot;<span class="hljs-subst">$(dirname <span class="hljs-string">&quot;<span class="hljs-subst">$(readlink -f <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;0&#125;</span>&quot;</span>)</span>&quot;</span>)</span>&quot;</span><br><span class="hljs-built_in">export</span> LD_LIBRARY_PATH=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;HERE&#125;</span>&quot;</span>/libs<br><span class="hljs-built_in">export</span> QT_PLUGIN_PATH=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;HERE&#125;</span>&quot;</span>/plugins <br><span class="hljs-built_in">export</span> QT_QPA_PLATFORM_PLUGIN_PATH=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;HERE&#125;</span>&quot;</span>/plugins/platforms<br><span class="hljs-built_in">cd</span> /lib/x86_64-linux-gnu/<br><span class="hljs-built_in">exec</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;HERE&#125;</span>&quot;</span>/netease-cloud-music <span class="hljs-variable">$@</span><br><br></code></pre></td></tr></table></figure></p>
<h1 id="关闭无用启动项"><a href="#关闭无用启动项" class="headerlink" title="关闭无用启动项"></a>关闭无用启动项</h1><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 查看启动项</span><br><span class="hljs-built_in">ls</span> -l /etc/xdg/autostart<br><br><span class="hljs-comment"># 重命名</span><br>sudo <span class="hljs-built_in">mv</span> something something.bak<br></code></pre></td></tr></table></figure>
<h1 id="Vmware-更新内核失败"><a href="#Vmware-更新内核失败" class="headerlink" title="Vmware 更新内核失败"></a>Vmware 更新内核失败</h1><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/mkubecek/vmware-host-modules.git<br>git checkout &lt;your_version&gt;<br>sudo make<br>sudo make install<br></code></pre></td></tr></table></figure>
<h1 id="双系统-Windows-更新失败"><a href="#双系统-Windows-更新失败" class="headerlink" title="双系统 Windows 更新失败"></a>双系统 Windows 更新失败</h1><p>我这里双系统 Windows 更新失败的原因是 Windows 引导出现了问题，可以进入 Windows 输入 <code>msconfig</code> 查看引导选项卡下是否有内容，我是用过 systemd boot 来引导的 Windows，所以没有出现内容。</p>
<p>在 BIOS 中更改成直接引导 Windows 之后便可以正常更新了。</p>
<h1 id="按时间降序最近安装的程序"><a href="#按时间降序最近安装的程序" class="headerlink" title="按时间降序最近安装的程序"></a>按时间降序最近安装的程序</h1><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> $(<span class="hljs-built_in">ls</span> -1t /var/log/dpkg.log*); <span class="hljs-keyword">do</span> <br>      zcat -f <span class="hljs-variable">$x</span> |<span class="hljs-built_in">tac</span> |grep -e <span class="hljs-string">&quot; install &quot;</span> -e <span class="hljs-string">&quot; upgrade &quot;</span>; <br><span class="hljs-keyword">done</span> | awk -F <span class="hljs-string">&quot;:a&quot;</span> <span class="hljs-string">&#x27;&#123;print $1 &quot; :a&quot; $2&#125;&#x27;</span> |column -t<br></code></pre></td></tr></table></figure>
<h1 id="常用的一些-gnome-extensions"><a href="#常用的一些-gnome-extensions" class="headerlink" title="常用的一些 gnome extensions"></a>常用的一些 gnome extensions</h1><h2 id="Unite"><a href="#Unite" class="headerlink" title="Unite"></a>Unite</h2><p>最大化时隐藏标题栏</p>
<h2 id="Clear-Top-Bar"><a href="#Clear-Top-Bar" class="headerlink" title="Clear Top Bar"></a>Clear Top Bar</h2><p>状态栏变成透明的</p>
<h2 id="ddterm"><a href="#ddterm" class="headerlink" title="ddterm"></a>ddterm</h2><p>按 <code>F10</code> 快速启动命令行，再按 <code>F10</code> 隐藏，十分方便</p>
<h2 id="Desktop-Icons-NG-DING"><a href="#Desktop-Icons-NG-DING" class="headerlink" title="Desktop Icons NG(DING)"></a>Desktop Icons NG(DING)</h2><p>在桌面上显示图标</p>
<h2 id="Lock-Keys"><a href="#Lock-Keys" class="headerlink" title="Lock Keys"></a>Lock Keys</h2><p>可以显示当前大小写状况</p>
<h2 id="NetSpeed"><a href="#NetSpeed" class="headerlink" title="NetSpeed"></a>NetSpeed</h2><p>显示当前网速</p>
<h2 id="TopIcons-Plus（在-gnome-40-之后使用-Ubuntu-Appindicators-替代）"><a href="#TopIcons-Plus（在-gnome-40-之后使用-Ubuntu-Appindicators-替代）" class="headerlink" title="TopIcons Plus（在 gnome 40 之后使用 Ubuntu Appindicators 替代）"></a>TopIcons Plus（在 gnome 40 之后使用 Ubuntu Appindicators 替代）</h2><p>在顶部显示图标</p>
<h2 id="Dash-to-Dock"><a href="#Dash-to-Dock" class="headerlink" title="Dash to Dock"></a>Dash to Dock</h2><p>在底部智能显示一个 Dock</p>
<h1 id="换-MAC-地址"><a href="#换-MAC-地址" class="headerlink" title="换 MAC 地址"></a>换 MAC 地址</h1><p>有的时候需要更换 linux 的 ip 地址：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">sudo ifconfig eth0 down<br>sudo ifconfig wlo1 hw ether 02:42:41:7d:b7:6e<br>sudo ifconfig wlo1 up<br></code></pre></td></tr></table></figure>
<p>这里 <code>eth0</code> 是网络 interface，ether 之后的参数就是 MAC 地址</p>
<h1 id="输入法"><a href="#输入法" class="headerlink" title="输入法"></a>输入法</h1><h2 id="Fcitx-失效"><a href="#Fcitx-失效" class="headerlink" title="Fcitx 失效"></a>Fcitx 失效</h2><ul>
<li><p>使用 im-config 修复</p>
</li>
<li><p>可能是 fcitx 没有正常启动，即还是 ibus，可以修改 ~/.pam_environment</p>
</li>
<li><p>使用 <code>fcitx5-diagnose</code> 命令根据提示设置环境变量</p>
</li>
<li><p>删除 <code>/etc/profile.d/pop-im-ibus.sh</code> （pop os）</p>
<p> <code>/etc/profile.d/pop-im-ibus.sh</code> （源文件： /etc/gdm3/Xsession ）设置了环境变量 <code>XMODIFIERS</code> ，在 <code>/etc/X11/Xsession.d/70im-config_launch</code> 中有如下代码：</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">if</span> [ -z <span class="hljs-string">&quot;<span class="hljs-variable">$XMODIFIERS</span>&quot;</span> ] &amp;&amp; \  <span class="hljs-comment"># 如果环境变量 XMODIFIERS 没有被设置</span><br>   ...<br>   <span class="hljs-comment"># 设置环境变量以启动用户指定的输入法</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure>
<p> 因为 <code>XMODIFIERS</code> 被设置了，所以 <code>设置环境变量以启动用户指定的输入法</code> 没有执行，所以 fcitx 没有被启动。</p>
<p> <code>/etc/profile.d/pop-im-ibus.sh</code> 第一次出现于 <code>pop-os_20.10_amd64_intel_4.iso</code> （发布于 2020 年 12 月中旬）</p>
<p> 相关 issue，<a href="https://github.com/pop-os/pop/issues/1445">https://github.com/pop-os/pop/issues/1445</a></p>
</li>
</ul>
<h1 id="Dash-to-dock"><a href="#Dash-to-dock" class="headerlink" title="Dash to dock"></a>Dash to dock</h1><h2 id="Dash-to-dock-重叠问题"><a href="#Dash-to-dock-重叠问题" class="headerlink" title="Dash to dock 重叠问题"></a>Dash to dock 重叠问题</h2><p>   Pop os 自带的 Dock 与 Dash to dock 发生了重叠</p>
   <figure class="highlight shell"><table><tr><td class="code"><pre><code class="hljs shell">cd /usr/share/gnome-shell/extensions<br>sudo mv cosmic-dock@system76.com cosmic-dock@system76.com.bak # 关闭自带的 dock<br></code></pre></td></tr></table></figure>
<p>   之后重启 gnome 即可解决</p>
<h1 id="Alt-Tab-时阻止相同应用叠加"><a href="#Alt-Tab-时阻止相同应用叠加" class="headerlink" title="Alt + Tab 时阻止相同应用叠加"></a>Alt + Tab 时阻止相同应用叠加</h1><p>在 gnome 设置中，打开 keyboard shortcut，将 <code>Switch windows</code> 设置为 <code>Alt + Tab</code> ，而不是默认的 <code>Switch applications</code>。</p>
<p>参考：<a href="https://superuser.com/questions/394376/how-to-prevent-gnome-shells-alttab-from-grouping-windows-from-similar-apps">https://superuser.com/questions/394376/how-to-prevent-gnome-shells-alttab-from-grouping-windows-from-similar-apps</a></p>
<h1 id="fluxion"><a href="#fluxion" class="headerlink" title="fluxion"></a>fluxion</h1><h2 id="扫描不到热点"><a href="#扫描不到热点" class="headerlink" title="扫描不到热点"></a>扫描不到热点</h2><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">sudo airmon-ng<br>sudo airmon-ng start fluxwl0<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> FLUXIONAirmonNG=1<br></code></pre></td></tr></table></figure>
<p>执行上述命令后再运行 fluxion 即可。</p>
<h2 id="解除-53-端口被-systemd-resolved-占用"><a href="#解除-53-端口被-systemd-resolved-占用" class="headerlink" title="解除 53 端口被 systemd-resolved 占用"></a>解除 53 端口被 systemd-resolved 占用</h2><ol>
<li>先停用 systemd-resolved 服务</li>
</ol>
<figure class="highlight nsis"><table><tr><td class="code"><pre><code class="hljs nsis"><span class="hljs-params">system</span>ctl stop <span class="hljs-params">system</span>d-resolved<br></code></pre></td></tr></table></figure>
<ol>
<li>编辑 /etc/systemd/resolved.conf 文件</li>
</ol>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">vi <span class="hljs-regexp">/etc/</span>systemd/resolved.conf<br></code></pre></td></tr></table></figure>
<ol>
<li>换下面说明更改，然后按一下“esc”键，再输入“:wq”（不要输入引号），回车保存即可。</li>
</ol>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[Resolve]</span><br><span class="hljs-attr">DNS</span>=<span class="hljs-number">8.8</span>.<span class="hljs-number">8.8</span>  <span class="hljs-comment">#取消注释，增加dns</span><br><span class="hljs-comment">#FallbackDNS=</span><br><span class="hljs-comment">#Domains=</span><br><span class="hljs-comment">#LLMNR=no</span><br><span class="hljs-comment">#MulticastDNS=no</span><br><span class="hljs-comment">#DNSSEC=no</span><br><span class="hljs-comment">#Cache=yes</span><br><span class="hljs-attr">DNSStubListener</span>=<span class="hljs-literal">no</span>  <span class="hljs-comment">#取消注释，把yes改为no</span><br></code></pre></td></tr></table></figure>
<ol>
<li>最后运行下面命令即可。</li>
</ol>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk">ln -sf <span class="hljs-regexp">/run/</span>systemd<span class="hljs-regexp">/resolve/</span>resolv.conf <span class="hljs-regexp">/etc/</span>resolv.conf<br></code></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title>Rust的一些学习心得</title>
    <url>/rust/rust/</url>
    <content><![CDATA[<h1 id="Rust-标准库-trait"><a href="#Rust-标准库-trait" class="headerlink" title="Rust 标准库 trait"></a>Rust 标准库 trait</h1><p>假设有以下变量：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">let</span> <span class="hljs-variable">t</span> = T::<span class="hljs-title function_ invoke__">new</span>()<br></code></pre></td></tr></table></figure>
<h2 id="impl-From-lt-U-gt-for-T"><a href="#impl-From-lt-U-gt-for-T" class="headerlink" title="impl From&lt;U&gt; for T"></a><code>impl From&lt;U&gt; for T</code></h2><p>如果为 <code>T</code> 实现了 <code>From&lt;U&gt;</code> 则可以通过 <code>T::from(U)</code> 得到 <code>T</code>。</p>
<p>例如 <code>String</code> 实现了 <code>From&lt;&amp;str&gt;</code>，所以 <code>String</code> 可以从 <code>&amp;str</code> 生成。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">let</span> <span class="hljs-variable">string</span> = <span class="hljs-string">&quot;hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><span class="hljs-keyword">let</span> <span class="hljs-variable">other_string</span> = <span class="hljs-type">String</span>::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;hello&quot;</span>);<br><br><span class="hljs-built_in">assert_eq!</span>(string, other_string);<br></code></pre></td></tr></table></figure>
<p><code>impl Into&lt;U&gt; for T</code></p>
<p>如果为 <code>T</code> 实现了 <code>Into&lt;U&gt;</code> 则可以通过 <code>t.into()</code> 消耗自己得到 <code>U</code>。</p>
<p>例如 <code>String</code> 类型实现了 <code>Into&lt;Vec&lt;u8&gt;&gt;</code>。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">is_hello</span>&lt;T: <span class="hljs-built_in">Into</span>&lt;<span class="hljs-type">Vec</span>&lt;<span class="hljs-type">u8</span>&gt;&gt;&gt;(s: T) &#123;<br>   <span class="hljs-keyword">let</span> <span class="hljs-variable">bytes</span> = <span class="hljs-string">b&quot;hello&quot;</span>.<span class="hljs-title function_ invoke__">to_vec</span>();<br>   <span class="hljs-built_in">assert_eq!</span>(bytes, s.<span class="hljs-title function_ invoke__">into</span>());<br>&#125;<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = <span class="hljs-string">&quot;hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><span class="hljs-title function_ invoke__">is_hello</span>(s);<br></code></pre></td></tr></table></figure>
<p>在实际编程中，用来接收多种类型的参数，如 <code>Into&lt;String&gt;</code> 可以同时接收 <code>String</code> 和 <code>&amp;str</code>。</p>
<h2 id="impl-AsRef-lt-U-gt-for-T"><a href="#impl-AsRef-lt-U-gt-for-T" class="headerlink" title="impl AsRef&lt;U&gt; for T"></a><code>impl AsRef&lt;U&gt; for T</code></h2><p>如果为 <code>T</code> 实现了 <code>AsRef&lt;U&gt;</code> 则可以通过 <code>t.as_ref()</code> 得到 <code>&amp;U</code>。</p>
<p>注：</p>
<ol>
<li>与 <code>Into&lt;U&gt;</code> 不同的是，<code>AsRef&lt;U&gt;</code> 只是类型转换，<code>t</code> 对象本身没有被消耗；</li>
<li><code>T: AsRef&lt;U&gt;</code> 中的 <code>T</code>，可以接受 资源拥有者（owned）类型，共享引用（shared referrence）类型 ，可变引用（mutable referrence）类型。</li>
</ol>
<p>例如 <code>String</code> 和 <code>&amp;str</code> 都实现了 <code>AsRef&lt;str&gt;</code>：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">is_hello</span>&lt;T: <span class="hljs-built_in">AsRef</span>&lt;<span class="hljs-type">str</span>&gt;&gt;(s: T) &#123;<br>   <span class="hljs-built_in">assert_eq!</span>(<span class="hljs-string">&quot;hello&quot;</span>, s.<span class="hljs-title function_ invoke__">as_ref</span>());<br>&#125;<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = <span class="hljs-string">&quot;hello&quot;</span>;<br><span class="hljs-title function_ invoke__">is_hello</span>(s);<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = <span class="hljs-string">&quot;hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><span class="hljs-title function_ invoke__">is_hello</span>(s);<br></code></pre></td></tr></table></figure>
<h2 id="impl-AsMut-lt-U-gt-for-T"><a href="#impl-AsMut-lt-U-gt-for-T" class="headerlink" title="impl AsMut&lt;U&gt; for T"></a><code>impl AsMut&lt;U&gt; for T</code></h2><p>如果为 <code>T</code> 实现了 <code>AsRef&lt;U&gt;</code> 则可以通过 <code>t.as_mut()</code> 得到 <code>&amp;mut U</code>。</p>
<h2 id="impl-Borror-lt-U-gt-for-T"><a href="#impl-Borror-lt-U-gt-for-T" class="headerlink" title="impl Borror&lt;U&gt; for T"></a><code>impl Borror&lt;U&gt; for T</code></h2><p>如果 <code>T</code> 实现了 <code>Borrow&lt;U&gt;</code>，那么，<code>t</code> 可执行 <code>.borrow()</code> 操作，即 <code>t.borrow()</code>。操作的结果，我们得到了一个类型为 <code>&amp;U</code> 的新引用。</p>
<p><code>Borrow</code> 可以认为是 <code>AsRef</code> 的严格版本，它对普适引用操作的前后类型之间附加了一些其它限制。</p>
<p><code>Borrow</code> 的前后类型之间要求必须有内部等价性。不具有这个等价性的两个类型之间，不能实现 <code>Borrow</code>。</p>
<p><code>AsRef</code> 更通用，更普遍，覆盖类型更多，是 <code>Borrow</code> 的超集。</p>
<p>举例：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::borrow::Borrow;<br><br><span class="hljs-keyword">fn</span> <span class="hljs-title function_">check</span>&lt;T: Borrow&lt;<span class="hljs-type">str</span>&gt;&gt;(s: T) &#123;<br>    <span class="hljs-built_in">assert_eq!</span>(<span class="hljs-string">&quot;Hello&quot;</span>, s.<span class="hljs-title function_ invoke__">borrow</span>());<br>&#125;<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = <span class="hljs-string">&quot;Hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><br><span class="hljs-title function_ invoke__">check</span>(s);<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = <span class="hljs-string">&quot;Hello&quot;</span>;<br><br><span class="hljs-title function_ invoke__">check</span>(s);<br></code></pre></td></tr></table></figure>
<h2 id="impl-BorrowMut-lt-U-gt-for-T"><a href="#impl-BorrowMut-lt-U-gt-for-T" class="headerlink" title="impl BorrowMut&lt;U&gt; for T"></a><code>impl BorrowMut&lt;U&gt; for T</code></h2><p>如果 <code>T</code> 实现了 <code>BorrowMut&lt;U&gt;</code>，那么，<code>t</code> 可执行 <code>.borrow_mut()</code> 操作，即 <code>t.borrow_mut()</code>。操作的结果我们得到类型为 <code>&amp;mut U</code> 的一个可变（mutable）引用。</p>
<h2 id="impl-ToOwned-for-T"><a href="#impl-ToOwned-for-T" class="headerlink" title="impl ToOwned for T"></a><code>impl ToOwned for T</code></h2><p><code>ToOwned</code> 为 <code>Clone</code> 的普适版本。它提供了 <code>.to_owned()</code> 方法，用于类型转换。</p>
<p>有些实现了 <code>Clone</code> 的类型 <code>T</code> 可以从引用状态实例 <code>&amp;T</code> 通过 <code>.clone()</code> 方法，生成具有所有权的 <code>T</code> 的实例。但是它只能由 <code>&amp;T</code> 生成 <code>T</code>。而对于其它形式的引用，<code>Clone</code> 就无能为力了。</p>
<p>而 <code>ToOwned</code> trait 能够从任意引用类型实例，生成具有所有权的类型实例。</p>
<h2 id="impl-Deref-for-T"><a href="#impl-Deref-for-T" class="headerlink" title="impl Deref for T"></a><code>impl Deref for T</code></h2><p><code>Deref</code> 是 <code>deref</code> 操作符 <code>*</code> 的 trait，比如 <code>*v</code>。</p>
<p>一般理解，<code>*t</code> 操作，是 <code>&amp;t</code> 的反向操作，即试图由资源的引用获取到资源的拷贝（如果资源类型实现了 <code>Copy</code>），或所有权（资源类型没有实现 <code>Copy</code>）。</p>
<p>Rust 中，本操作符行为可以重载。这也是 Rust 操作符的基本特点。本身没有什么特别的。</p>
<h3 id="强制隐式转换（coercion）"><a href="#强制隐式转换（coercion）" class="headerlink" title="强制隐式转换（coercion）"></a>强制隐式转换（coercion）</h3><p><code>Deref</code> 神奇的地方并不在本身 <code>解引</code> 这个意义上，Rust 的设计者在它之上附加了一个特性：<code>强制隐式转换</code>，这才是它神奇之处。</p>
<p>这种隐式转换的规则为：</p>
<p>一个类型为 <code>T</code> 的对象 <code>t</code>，如果 <code>T: Deref&lt;Target=U&gt;</code>，那么，相关 <code>t</code> 的某个智能指针或引用（比如 <code>&amp;foo</code>）在应用的时候会自动转换成 <code>&amp;U</code>。</p>
<p>粗看这条规则，貌似有点类似于 <code>AsRef</code>，而跟 <code>解引</code> 似乎风马牛不相及。实际里面有些玄妙之处。</p>
<p>Rust 编译器会在做 <code>*v</code> 操作的时候，自动先把 <code>v</code> 做引用归一化操作，即转换成内部通用引用的形式 <code>&amp;v</code>，整个表达式就变成 <code>*&amp;v</code>。这里面有两种情况：</p>
<ol>
<li>把其它类型的指针（比如在库中定义的，<code>Box</code>, <code>Rc</code>, <code>Arc</code>, <code>Cow</code> 等），转成内部标准形式 <code>&amp;v</code>；</li>
<li>把多重 <code>&amp;</code> （比如：<code>&amp;&amp;&amp;&amp;&amp;&amp;&amp;v</code>），简化成 <code>&amp;v</code>（通过插入足够数量的 <code>*</code> 进行解引）。</li>
</ol>
<p>所以，它实际上在解引用之前做了一个引用的归一化操作。</p>
<p>为什么要转呢？ 因为编译器设计的能力是，只能够对 <code>&amp;v</code> 这种引用进行解引用。其它形式的它不认识，所以要做引用归一化操作。</p>
<p>使用引用进行过渡也是为了能够防止不必要的拷贝。</p>
<p>下面举一些例子：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">foo</span>(s: &amp;<span class="hljs-type">str</span>) &#123;<br>    <span class="hljs-comment">// borrow a string for a second</span><br>&#125;<br><br><span class="hljs-comment">// String implements Deref&lt;Target=str&gt;</span><br><span class="hljs-keyword">let</span> <span class="hljs-variable">owned</span> = <span class="hljs-string">&quot;Hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><br><span class="hljs-comment">// therefore, this works:</span><br><span class="hljs-title function_ invoke__">foo</span>(&amp;owned);<br></code></pre></td></tr></table></figure>
<p>因为 <code>String</code> 实现了 <code>Deref&lt;Target=str&gt;</code>。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::rc::Rc;<br><br><span class="hljs-keyword">fn</span> <span class="hljs-title function_">foo</span>(s: &amp;<span class="hljs-type">str</span>) &#123;<br>    <span class="hljs-comment">// borrow a string for a second</span><br>&#125;<br><br><span class="hljs-comment">// String implements Deref&lt;Target=str&gt;</span><br><span class="hljs-keyword">let</span> <span class="hljs-variable">owned</span> = <span class="hljs-string">&quot;Hello&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>();<br><span class="hljs-keyword">let</span> <span class="hljs-variable">counted</span> = Rc::<span class="hljs-title function_ invoke__">new</span>(owned);<br><br><span class="hljs-comment">// therefore, this works:</span><br><span class="hljs-title function_ invoke__">foo</span>(&amp;counted);<br></code></pre></td></tr></table></figure>
<p>因为 <code>Rc&lt;T&gt;</code> 实现了 <code>Deref&lt;Target=T&gt;</code>。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">foo</span>(s: &amp;[<span class="hljs-type">i32</span>]) &#123;<br>    <span class="hljs-comment">// borrow a slice for a second</span><br>&#125;<br><br><span class="hljs-comment">// Vec&lt;T&gt; implements Deref&lt;Target=[T]&gt;</span><br><span class="hljs-keyword">let</span> <span class="hljs-variable">owned</span> = <span class="hljs-built_in">vec!</span>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>];<br><br><span class="hljs-title function_ invoke__">foo</span>(&amp;owned);<br></code></pre></td></tr></table></figure>
<p>因为 <code>Vec&lt;T&gt;</code> 实现了 <code>Deref&lt;Target=[T]&gt;</code>。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Foo</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Foo</span> &#123;<br>    <span class="hljs-keyword">fn</span> <span class="hljs-title function_">foo</span>(&amp;<span class="hljs-keyword">self</span>) &#123; <span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;Foo&quot;</span>); &#125;<br>&#125;<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">f</span> = &amp;&amp;Foo;<br><br>f.<span class="hljs-title function_ invoke__">foo</span>();<br>(&amp;f).<span class="hljs-title function_ invoke__">foo</span>();<br>(&amp;&amp;f).<span class="hljs-title function_ invoke__">foo</span>();<br>(&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;f).<span class="hljs-title function_ invoke__">foo</span>();<br></code></pre></td></tr></table></figure>
<p>上面那几种函数的调用，效果是一样的。</p>
<p><code>coercion</code> 的设计，是 Rust 中仅有的类型隐式转换，设计它的目的，是为了简化程序的书写，让代码不至于过于繁琐。把人从无尽的类型细节中解脱出来，让书写 Rust 代码变成一件快乐的事情。</p>
<h2 id="Cow"><a href="#Cow" class="headerlink" title="Cow"></a><code>Cow</code></h2><p><code>Clone-on-write</code>，即写时克隆。本质上是一个智能指针。</p>
<p>它有两个可选值：</p>
<ul>
<li><code>Borrowed</code>，用于包裹对象的引用（通用引用）；</li>
<li><code>Owned</code>，用于包裹对象的所有者；</li>
</ul>
<p><code>Cow</code> 提供</p>
<ol>
<li>对此对象的不可变访问（比如可直接调用此对象原有的不可变方法）；</li>
<li>如果遇到需要修改此对象，或者需要获得此对象的所有权的情况，<code>Cow</code> 提供方法做克隆处理，并避免多次重复克隆。</li>
</ol>
<p><code>Cow</code> 的设计目的是提高性能（减少复制）同时增加灵活性，因为大部分情况下，业务场景都是读多写少。利用 <code>Cow</code>，可以用统一，规范的形式实现，需要写的时候才做一次对象复制。这样就可能会大大减少复制的次数。</p>
<p>它有以下几个要点需要掌握：</p>
<ol>
<li><code>Cow&lt;T&gt;</code> 能直接调用 <code>T</code> 的不可变方法，因为 <code>Cow</code> 这个枚举，实现了 <code>Deref</code>；</li>
<li>在需要写 <code>T</code>的时候，可以使用 <code>.to_mut()</code> 方法得到一个具有所有权的值的可变借用；<ol>
<li>注意，调用 <code>.to_mut()</code> 不一定会产生克隆；</li>
<li>在已经具有所有权的情况下，调用 <code>.to_mut()</code> 有效，但是不会产生新的克隆；</li>
<li>多次调用 <code>.to_mut()</code> 只会产生一次克隆。</li>
</ol>
</li>
<li>在需要写 <code>T</code> 的时候，可以使用 <code>.into_owned()</code> 创建新的拥有所有权的对象，这个过程往往意味着内存拷贝并创建新对象；<ol>
<li>如果之前 <code>Cow</code> 中的值是借用状态，调用此操作将执行克隆；</li>
<li>本方法，参数是<code>self</code>类型，它会“吃掉”原先的那个对象，调用之后原先的对象的生命周期就截止了，在 <code>Cow</code> 上不能调用多次；</li>
</ol>
</li>
</ol>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p><code>.to_mut()</code> 举例</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::borrow::Cow;<br><br><span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">cow</span>: Cow&lt;[_]&gt; = Cow::<span class="hljs-title function_ invoke__">Owned</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]);<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">hello</span> = cow.<span class="hljs-title function_ invoke__">to_mut</span>();<br><br><span class="hljs-built_in">assert_eq!</span>(hello, &amp;[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]);<br></code></pre></td></tr></table></figure>
<p><code>.into_owned()</code> 举例</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::borrow::Cow;<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">cow</span>: Cow&lt;[_]&gt; = Cow::<span class="hljs-title function_ invoke__">Owned</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]);<br><br><span class="hljs-keyword">let</span> <span class="hljs-variable">hello</span> = cow.<span class="hljs-title function_ invoke__">into_owned</span>();<br><br><span class="hljs-built_in">assert_eq!</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], hello);<br></code></pre></td></tr></table></figure>
<p>综合举例</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::borrow::Cow;<br><br><span class="hljs-keyword">fn</span> <span class="hljs-title function_">abs_all</span>(input: &amp;<span class="hljs-keyword">mut</span> Cow&lt;[<span class="hljs-type">i32</span>]&gt;) &#123;<br>    <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..input.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">v</span> = input[i];<br>        <span class="hljs-keyword">if</span> v &lt; <span class="hljs-number">0</span> &#123;<br>            <span class="hljs-comment">// clones into a vector the first time (if not already owned)</span><br>            input.<span class="hljs-title function_ invoke__">to_mut</span>()[i] = -v;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h3 id="更多的例子"><a href="#更多的例子" class="headerlink" title="更多的例子"></a>更多的例子</h3><p>题目：写一个函数，过滤掉输入的字符串中的所有空格字符，并返回过滤后的字符串。</p>
<p>对这个简单的问题，不用思考，我们都可以很快写出代码：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">fn</span> <span class="hljs-title function_">remove_spaces</span>(input: &amp;<span class="hljs-type">str</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">String</span> &#123;<br>   <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">buf</span> = <span class="hljs-type">String</span>::<span class="hljs-title function_ invoke__">with_capacity</span>(input.<span class="hljs-title function_ invoke__">len</span>());<br><br>   <span class="hljs-keyword">for</span> <span class="hljs-variable">c</span> <span class="hljs-keyword">in</span> input.<span class="hljs-title function_ invoke__">chars</span>() &#123;<br>      <span class="hljs-keyword">if</span> c != <span class="hljs-string">&#x27; &#x27;</span> &#123;<br>         buf.<span class="hljs-title function_ invoke__">push</span>(c);<br>      &#125;<br>   &#125;<br><br>   buf<br>&#125;<br></code></pre></td></tr></table></figure>
<p>设计函数输入参数的时候，我们会停顿一下，这里，用 <code>&amp;str</code> 好呢，还是 <code>String</code> 好呢？思考一番，从性能上考虑，有如下结论：</p>
<ol>
<li>如果使用 <code>String</code> 则外部在调用此函数的时候，<ol>
<li>如果外部的字符串是 <code>&amp;str</code>，那么，它需要做一次克隆，才能调用此函数；</li>
<li>如果外部的字符串是 <code>String</code>，那么，它不需要做克隆，就可以调用此函数。但是，一旦调用后，外部那个字符串的所有权就被 <code>move</code> 到此函数中了，外部的后续代码将无法再使用原字符串。</li>
</ol>
</li>
<li>如果使用 <code>&amp;str</code>，则不存在上述两个问题。但可能会遇到生命周期的问题，需要注意。</li>
</ol>
<p>继续分析上面的例子，我们发现，在函数体内，做了一次新字符串对象的生成和拷贝。</p>
<p>让我们来仔细分析一下业务需求。最坏的情况下，如果字符串中没有空白字符，那最好是直接原样返回。这种情况做这样一次对象的拷贝，完全就是浪费了。</p>
<p>于是我们心想改进这个算法。很快，又遇到了另一个问题，返回值是 <code>String</code> 的嘛，我不论怎样，要把 <code>&amp;str</code> 转换成 <code>String</code> 返回，始终都要经历一次复制。于是我们快要放弃了。</p>
<p>好吧，<code>Cow</code> 君这时出马了。写出了如下代码：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::borrow::Cow;<br><br><span class="hljs-keyword">fn</span> <span class="hljs-title function_">remove_spaces</span>&lt;<span class="hljs-symbol">&#x27;a</span>&gt;(input: &amp;<span class="hljs-symbol">&#x27;a</span> <span class="hljs-type">str</span>) <span class="hljs-punctuation">-&gt;</span> Cow&lt;<span class="hljs-symbol">&#x27;a</span>, <span class="hljs-type">str</span>&gt; &#123;<br>    <span class="hljs-keyword">if</span> input.<span class="hljs-title function_ invoke__">contains</span>(<span class="hljs-string">&#x27; &#x27;</span>) &#123;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">buf</span> = <span class="hljs-type">String</span>::<span class="hljs-title function_ invoke__">with_capacity</span>(input.<span class="hljs-title function_ invoke__">len</span>());<br><br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">c</span> <span class="hljs-keyword">in</span> input.<span class="hljs-title function_ invoke__">chars</span>() &#123;<br>            <span class="hljs-keyword">if</span> c != <span class="hljs-string">&#x27; &#x27;</span> &#123;<br>                buf.<span class="hljs-title function_ invoke__">push</span>(c);<br>            &#125;<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> Cow::<span class="hljs-title function_ invoke__">Owned</span>(buf);<br>    &#125;<br><br>    <span class="hljs-keyword">return</span> Cow::<span class="hljs-title function_ invoke__">Borrowed</span>(input);<br>&#125;<br></code></pre></td></tr></table></figure>
<p>完美解决了业务逻辑与返回值类型冲突的问题。本例可细细品味。</p>
<p>外部程序，拿到这个 <code>Cow</code> 返回值后，按照我们上文描述的 <code>Cow</code> 的特性使用就好了。</p>
<h2 id="Send-和-Sync"><a href="#Send-和-Sync" class="headerlink" title="Send 和 Sync"></a><code>Send</code> 和 <code>Sync</code></h2><p><code>std::marker</code> 模块中，有两个 trait：<code>Send</code> 和 <code>Sync</code>，它们与多线程安全相关。</p>
<p>标记为 <code>marker trait</code> 的 trait，它实际就是一种约定，没有方法的定义，也没有关联元素（associated items）。仅仅是一种约定，实现了它的类型必须满足这种约定。一种类型是否加上这种约定，要么是编译器的行为，要么是人工手动的行为。</p>
<p><code>Send</code> 和 <code>Sync</code> 在大部分情况下（针对 Rust 的基础类型和 std 中的大部分类型），会由编译器自动推导出来。对于不能由编译器自动推导出来的类型，要使它们具有 <code>Send</code> 或 <code>Sync</code> 的约定，可以由人手动实现。实现的时候，必须使用 <code>unsafe</code> 前缀，因为 Rust 默认不信任程序员，由程序员自己控制的东西，统统标记为 <code>unsafe</code>，出了问题（比如，把不是线程安全的对象加上 <code>Sync</code> 约定）由程序员自行负责。</p>
<p>它们的定义如下：</p>
<p>如果 <code>T: Send</code>，那么将 <code>T</code> 传到另一个线程中时（按值传送），不会导致数据竞争或其它不安全情况。</p>
<ol>
<li><code>Send</code> 是对象可以安全发送到另一个执行体中；</li>
<li><code>Send</code> 使被发送对象可以和产生它的线程解耦，防止原线程将此资源释放后，在目标线程中使用出错（use after free）。</li>
</ol>
<p>如果 <code>T: Sync</code>，那么将 <code>&amp;T</code> 传到另一个线程中时，不会导致数据竞争或其它不安全情况。</p>
<ol>
<li><code>Sync</code> 是可以被同时多个执行体访问而不出错；</li>
<li><code>Sync</code> 防止的是竞争；</li>
</ol>
<p>推论：</p>
<ol>
<li><code>T: Sync</code> 意味着 <code>&amp;T: Send</code>；</li>
<li><code>Sync + Copy = Send</code>；</li>
<li>当 <code>T: Send</code> 时，可推导出 <code>&amp;mut T: Send</code>；</li>
<li>当 <code>T: Sync</code> 时，可推导出 <code>&amp;mut T: Sync</code>；</li>
<li>当 <code>&amp;mut T: Send</code> 时，不能推导出 <code>T: Send</code>；</li>
</ol>
<p>（注：<code>T</code>, <code>&amp;T</code>, <code>&amp;mut T</code>，<code>Box&lt;T&gt;</code> 等都是不同的类型）</p>
<p>具体的类型：</p>
<ol>
<li>原始类型（比如： u8, f64），都是 <code>Sync</code>，都是 <code>Copy</code>，因此都是 <code>Send</code>；</li>
<li>只包含原始类型的复合类型，都是 <code>Sync</code>，都是 <code>Copy</code>，因此都是 <code>Send</code>；</li>
<li>当 <code>T: Sync</code>，<code>Box&lt;T&gt;</code>, <code>Vec&lt;T&gt;</code> 等集合类型是 <code>Sync</code>；</li>
<li>具有内部可变性的的指针，不是 <code>Sync</code> 的，比如 <code>Cell</code>, <code>RefCell</code>, <code>UnsafeCell</code>；</li>
<li><code>Rc</code> 不是 <code>Sync</code>。因为只要一做 <code>&amp;Rc&lt;T&gt;</code> 操作，就会克隆一个新引用，它会以非原子性的方式修改引用计数，所以是不安全的；</li>
<li>被 <code>Mutex</code> 和 <code>RWLock</code> 锁住的类型 <code>T: Send</code>，是 <code>Sync</code> 的；</li>
<li>原始指针（<code>*mut</code>, <code>*const</code>）既不是 <code>Send</code> 也不是 <code>Sync</code>；</li>
</ol>
<p>Rust 正是通过这两大武器：<code>所有权和生命周期</code> + <code>Send 和 Sync</code>（本质上为类型系统）来为并发编程提供了安全可靠的基础设施。使得程序员可以放心在其上构建稳健的并发模型。这也正是 Rust 的核心设计观的体现：内核只提供最基础的原语，真正的实现能分离出去就分离出去。并发也是如此。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li><a href="https://github.com/rustcc/RustPrimer">https://github.com/rustcc/RustPrimer</a></li>
</ol>
]]></content>
      <categories>
        <category>rust</category>
      </categories>
      <tags>
        <tag>rust</tag>
      </tags>
  </entry>
  <entry>
    <title>Internlm-03-基于 InternLM 和 LangChain 搭建你的知识库</title>
    <url>/internlm/internlm-03/</url>
    <content><![CDATA[<h1 id="基于-InternLM-和-LangChain-搭建你的知识库"><a href="#基于-InternLM-和-LangChain-搭建你的知识库" class="headerlink" title="基于 InternLM 和 LangChain 搭建你的知识库"></a>基于 InternLM 和 LangChain 搭建你的知识库</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h3 id="InternLM-环境"><a href="#InternLM-环境" class="headerlink" title="InternLM 环境"></a>InternLM 环境</h3><p>开发环境除了 <code>pytorch</code> 等库以外，还需要安装以下库</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 升级pip</span><br>python -m pip install --upgrade pip<br><br>pip install modelscope==1.9.5<br>pip install transformers==4.35.2<br>pip install streamlit==1.24.0<br>pip install sentencepiece==0.1.99<br>pip install accelerate==0.24.1<br></code></pre></td></tr></table></figure>
<h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">import torch<br>from modelscope import snapshot_download, AutoModel, AutoTokenizer<br>import os<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;Shanghai_AI_Laboratory/internlm-chat-7b&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;/root/data/model&#x27;</span>, revision=<span class="hljs-string">&#x27;v1.0.3&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="配置-Langchain"><a href="#配置-Langchain" class="headerlink" title="配置 Langchain"></a>配置 Langchain</h3><p>除了配置大模型的运行环境以外，还需要配置 Langchain 运行环境。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install langchain==0.0.292<br>pip install gradio==4.4.0<br>pip install chromadb==0.4.15<br>pip install sentence-transformers==2.2.2<br>pip install unstructured==0.10.30<br>pip install markdown==3.3.7<br></code></pre></td></tr></table></figure>
<p><img  src="安装依赖.png"  ><span class="image-caption">安装依赖</span></p>
<h3 id="下载-Embedding-模型"><a href="#下载-Embedding-模型" class="headerlink" title="下载 Embedding 模型"></a>下载 Embedding 模型</h3><p>同时，我们需要使用到开源词向量模型 <a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2">Sentence Transformer - paraphrase-multilingual-MiniLM-L12-v2</a>:（我们也可以选用别的开源词向量模型来进行 Embedding，教程中选用这个模型是相对轻量、支持中文且效果较好的，我这里选择使用了更为好用的 bge 系列的 Embedding 模型 <a href="[BAAI/bge-large-zh-v1.5 · Hugging Face](https://huggingface.co/BAAI/bge-large-zh-v1.5">BAAI/bge-large-zh-v1.5</a>)）</p>
<p>首先需要使用 <code>huggingface</code> 官方提供的 <code>huggingface-cli</code> 命令行工具。安装依赖:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install -U huggingface_hub<br></code></pre></td></tr></table></figure>
<p>然后在和 <code>/root/data</code> 目录下新建python文件 <code>download_hf.py</code>，填入以下代码：</p>
<ul>
<li>resume-download：断点续下</li>
<li>local-dir：本地存储路径。（linux环境下需要填写绝对路径）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 下载模型</span><br>os.system(<span class="hljs-string">&#x27;huggingface-cli download --resume-download BAAI/bge-large-zh-v1.5 --local-dir /root/data/model/bge-large-zh-v1.5&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>但是，使用 huggingface 下载可能速度较慢，我们可以使用 huggingface 镜像下载。与使用hugginge face下载相同，只需要填入镜像地址即可。</p>
<p>将 <code>download_hf.py</code> 中的代码修改为以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 设置环境变量</span><br>os.environ[<span class="hljs-string">&#x27;HF_ENDPOINT&#x27;</span>] = <span class="hljs-string">&#x27;https://hf-mirror.com&#x27;</span><br><br><span class="hljs-comment"># 下载模型</span><br>os.system(<span class="hljs-string">&#x27;huggingface-cli download --resume-download BAAI/bge-large-zh-v1.5 --local-dir /root/data/model/bge-large-zh-v1.5&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>然后，在 <code>/root/data</code> 目录下执行该脚本即可自动开始下载：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python download_hf.py<br></code></pre></td></tr></table></figure>
<p><img  src="下载bge.png"  ><span class="image-caption">下载bge模型</span></p>
<h3 id="下载-NLTK-相关资源"><a href="#下载-NLTK-相关资源" class="headerlink" title="下载 NLTK 相关资源"></a>下载 NLTK 相关资源</h3><p>我们在使用开源词向量模型构建开源词向量的时候，需要用到第三方库 <code>nltk</code> 的一些资源。正常情况下，其会自动从互联网上下载，但可能由于网络原因会导致下载中断，此处我们可以从国内仓库镜像地址下载相关资源，保存到服务器上。</p>
<p>我们用以下命令下载 nltk 资源并解压到服务器上：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root<br>git <span class="hljs-built_in">clone</span> https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages<br><span class="hljs-built_in">cd</span> nltk_data<br><span class="hljs-built_in">mv</span> packages/*  ./<br><span class="hljs-built_in">cd</span> tokenizers<br>unzip punkt.zip<br><span class="hljs-built_in">cd</span> ../taggers<br>unzip averaged_perceptron_tagger.zip<br></code></pre></td></tr></table></figure>
<p>之后使用时服务器即会自动使用已有资源，无需再次下载。</p>
<h3 id="下载教程代码"><a href="#下载教程代码" class="headerlink" title="下载教程代码"></a>下载教程代码</h3><p>我们在仓库中同步提供了所有脚本，可以查看该教程文件的同级目录的 <code>demo</code> 文件夹。</p>
<p>建议通过以下目录将仓库 clone 到本地，可以直接在本地运行相关代码：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/data<br>git <span class="hljs-built_in">clone</span> https://github.com/InternLM/tutorial<br></code></pre></td></tr></table></figure>
<p>通过上述命令，可以将本仓库 clone 到本地 <code>root/data/tutorial</code> 目录下，在之后的过程中可以对照仓库中的脚本来完成自己的代码，也可以直接使用仓库中的脚本。</p>
<h2 id="知识库搭建"><a href="#知识库搭建" class="headerlink" title="知识库搭建"></a>知识库搭建</h2><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><p>教程选择了由上海人工智能实验室开源的一系列大模型工具开源仓库作为语料库来源，包括：</p>
<ul>
<li><a href="https://gitee.com/open-compass/opencompass">OpenCompass</a>：面向大模型评测的一站式平台</li>
<li><a href="https://gitee.com/InternLM/lmdeploy">IMDeploy</a>：涵盖了 LLM 任务的全套轻量化、部署和服务解决方案的高效推理工具箱</li>
<li><a href="https://gitee.com/InternLM/xtuner">XTuner</a>：轻量级微调大语言模型的工具库</li>
<li><a href="https://gitee.com/InternLM/InternLM-XComposer">InternLM-XComposer</a>：浦语·灵笔，基于书生·浦语大语言模型研发的视觉-语言大模型</li>
<li><a href="https://gitee.com/InternLM/lagent">Lagent</a>：一个轻量级、开源的基于大语言模型的智能体（agent）框架</li>
<li><a href="https://gitee.com/InternLM/InternLM">InternLM</a>：一个开源的轻量级训练框架，旨在支持大模型训练而无需大量的依赖</li>
</ul>
<p>首先我们需要将上述远程开源仓库 Clone 到本地，可以使用以下命令：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># 进入到数据库盘</span><br>cd <span class="hljs-regexp">/root/</span>data<br><span class="hljs-comment"># clone 上述开源仓库</span><br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/open-compass/</span>opencompass.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>lmdeploy.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>xtuner.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>InternLM-XComposer.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>lagent.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>InternLM.git<br></code></pre></td></tr></table></figure>
<p>接着，为语料处理方便，我们将选用上述仓库中所有的 markdown、txt 文件作为示例语料库。注意，也可以选用其中的代码文件加入到知识库中，但需要针对代码文件格式进行额外处理（因为代码文件对逻辑联系要求较高，且规范性较强，在分割时最好基于代码模块进行分割再加入向量数据库）。</p>
<p>我们首先将上述仓库中所有满足条件的文件路径找出来，我们定义一个函数，该函数将递归指定文件夹路径，返回其中所有满足条件（即后缀名为 .md 或者 .txt 的文件）的文件路径：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_files</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    file_list = []<br>    <span class="hljs-keyword">for</span> filepath, dirnames, filenames <span class="hljs-keyword">in</span> os.walk(dir_path):<br>        <span class="hljs-comment"># os.walk 函数将递归遍历指定文件夹</span><br>        <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> filenames:<br>            <span class="hljs-comment"># 通过后缀名判断文件类型是否满足要求</span><br>            <span class="hljs-keyword">if</span> filename.endswith(<span class="hljs-string">&quot;.md&quot;</span>):<br>                <span class="hljs-comment"># 如果满足要求，将其绝对路径加入到结果列表</span><br>                file_list.append(os.path.join(filepath, filename))<br>            <span class="hljs-keyword">elif</span> filename.endswith(<span class="hljs-string">&quot;.txt&quot;</span>):<br>                file_list.append(os.path.join(filepath, filename))<br>    <span class="hljs-keyword">return</span> file_list<br></code></pre></td></tr></table></figure>
<h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>得到所有目标文件路径之后，我们可以使用 LangChain 提供的 FileLoader 对象来加载目标文件，得到由目标文件解析出的纯文本内容。由于不同类型的文件需要对应不同的 FileLoader，我们判断目标文件类型，并针对性调用对应类型的 FileLoader，同时，调用 FileLoader 对象的 load 方法来得到加载之后的纯文本对象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredFileLoader<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredMarkdownLoader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    <span class="hljs-comment"># 首先调用上文定义的函数得到目标文件路径列表</span><br>    file_lst = get_files(dir_path)<br>    <span class="hljs-comment"># docs 存放加载之后的纯文本对象</span><br>    docs = []<br>    <span class="hljs-comment"># 遍历所有目标文件</span><br>    <span class="hljs-keyword">for</span> one_file <span class="hljs-keyword">in</span> tqdm(file_lst):<br>        file_type = one_file.split(<span class="hljs-string">&#x27;.&#x27;</span>)[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> file_type == <span class="hljs-string">&#x27;md&#x27;</span>:<br>            loader = UnstructuredMarkdownLoader(one_file)<br>        <span class="hljs-keyword">elif</span> file_type == <span class="hljs-string">&#x27;txt&#x27;</span>:<br>            loader = UnstructuredFileLoader(one_file)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 如果是不符合条件的文件，直接跳过</span><br>            <span class="hljs-keyword">continue</span><br>        docs.extend(loader.load())<br>    <span class="hljs-keyword">return</span> docs<br></code></pre></td></tr></table></figure>
<p>使用上文函数，我们得到的 <code>docs</code> 为一个纯文本对象对应的列表。</p>
<h3 id="构建向量数据库"><a href="#构建向量数据库" class="headerlink" title="构建向量数据库"></a>构建向量数据库</h3><p>得到该列表之后，我们就可以将它引入到 LangChain 框架中构建向量数据库。由纯文本对象构建向量数据库，我们需要先对文本进行分块，接着对文本块进行向量化。</p>
<p>LangChain 提供了多种文本分块工具，此处我们使用字符串递归分割器，并选择分块大小为 500，块重叠长度为 150（由于篇幅限制，此处没有展示切割效果，学习者可以自行尝试一下，想要深入学习 LangChain 文本分块可以参考教程 <a href="https://github.com/datawhalechina/prompt-engineering-for-developers/blob/9dbcb48416eb8af9ff9447388838521dc0f9acb0/content/LangChain Chat with Your Data/1.简介 Introduction.md">《LangChain - Chat With Your Data》</a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter<br><br>text_splitter = RecursiveCharacterTextSplitter(<br>    chunk_size=<span class="hljs-number">500</span>, chunk_overlap=<span class="hljs-number">150</span>)<br>split_docs = text_splitter.split_documents(docs)<br></code></pre></td></tr></table></figure>
<p>接着我们选用开源词向量模型 <a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2">Sentence Transformer</a> 来进行文本向量化。LangChain 提供了直接引入 HuggingFace 开源社区中的模型进行向量化的接口：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><br>embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>同时，考虑到 Chroma 是目前最常用的入门数据库，我们选择 Chroma 作为向量数据库，基于上文分块后的文档以及加载的开源向量化模型，将语料加载到指定路径下的向量数据库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><br><span class="hljs-comment"># 定义持久化路径</span><br>persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><span class="hljs-comment"># 加载数据库</span><br>vectordb = Chroma.from_documents(<br>    documents=split_docs,<br>    embedding=embeddings,<br>    persist_directory=persist_directory  <span class="hljs-comment"># 允许我们将persist_directory目录保存到磁盘上</span><br>)<br><span class="hljs-comment"># 将加载的向量数据库持久化到磁盘上</span><br>vectordb.persist()<br></code></pre></td></tr></table></figure>
<h3 id="整体脚本"><a href="#整体脚本" class="headerlink" title="整体脚本"></a>整体脚本</h3><p>将上述代码整合在一起为知识库搭建的脚本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 首先导入所需第三方库</span><br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredFileLoader<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredMarkdownLoader<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 获取文件路径函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_files</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    file_list = []<br>    <span class="hljs-keyword">for</span> filepath, dirnames, filenames <span class="hljs-keyword">in</span> os.walk(dir_path):<br>        <span class="hljs-comment"># os.walk 函数将递归遍历指定文件夹</span><br>        <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> filenames:<br>            <span class="hljs-comment"># 通过后缀名判断文件类型是否满足要求</span><br>            <span class="hljs-keyword">if</span> filename.endswith(<span class="hljs-string">&quot;.md&quot;</span>):<br>                <span class="hljs-comment"># 如果满足要求，将其绝对路径加入到结果列表</span><br>                file_list.append(os.path.join(filepath, filename))<br>            <span class="hljs-keyword">elif</span> filename.endswith(<span class="hljs-string">&quot;.txt&quot;</span>):<br>                file_list.append(os.path.join(filepath, filename))<br>    <span class="hljs-keyword">return</span> file_list<br><br><span class="hljs-comment"># 加载文件函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    <span class="hljs-comment"># 首先调用上文定义的函数得到目标文件路径列表</span><br>    file_lst = get_files(dir_path)<br>    <span class="hljs-comment"># docs 存放加载之后的纯文本对象</span><br>    docs = []<br>    <span class="hljs-comment"># 遍历所有目标文件</span><br>    <span class="hljs-keyword">for</span> one_file <span class="hljs-keyword">in</span> tqdm(file_lst):<br>        file_type = one_file.split(<span class="hljs-string">&#x27;.&#x27;</span>)[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> file_type == <span class="hljs-string">&#x27;md&#x27;</span>:<br>            loader = UnstructuredMarkdownLoader(one_file)<br>        <span class="hljs-keyword">elif</span> file_type == <span class="hljs-string">&#x27;txt&#x27;</span>:<br>            loader = UnstructuredFileLoader(one_file)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 如果是不符合条件的文件，直接跳过</span><br>            <span class="hljs-keyword">continue</span><br>        docs.extend(loader.load())<br>    <span class="hljs-keyword">return</span> docs<br><br><span class="hljs-comment"># 目标文件夹</span><br>tar_dir = [<br>    <span class="hljs-string">&quot;/root/data/InternLM&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/InternLM-XComposer&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/lagent&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/lmdeploy&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/opencompass&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/xtuner&quot;</span><br>]<br><br><span class="hljs-comment"># 加载目标文件</span><br>docs = []<br><span class="hljs-keyword">for</span> dir_path <span class="hljs-keyword">in</span> tar_dir:<br>    docs.extend(get_text(dir_path))<br><br><span class="hljs-comment"># 对文本进行分块</span><br>text_splitter = RecursiveCharacterTextSplitter(<br>    chunk_size=<span class="hljs-number">500</span>, chunk_overlap=<span class="hljs-number">150</span>)<br>split_docs = text_splitter.split_documents(docs)<br><br><span class="hljs-comment"># 加载开源词向量模型</span><br>embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br><br><span class="hljs-comment"># 构建向量数据库</span><br><span class="hljs-comment"># 定义持久化路径</span><br>persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><span class="hljs-comment"># 加载数据库</span><br>vectordb = Chroma.from_documents(<br>    documents=split_docs,<br>    embedding=embeddings,<br>    persist_directory=persist_directory  <span class="hljs-comment"># 允许我们将persist_directory目录保存到磁盘上</span><br>)<br><span class="hljs-comment"># 将加载的向量数据库持久化到磁盘上</span><br>vectordb.persist()<br></code></pre></td></tr></table></figure>
<p>可以在 <code>/root/data</code> 下新建一个 <code>demo</code>目录，将该脚本和后续脚本均放在该目录下运行。运行上述脚本，即可在本地构建已持久化的向量数据库，后续直接导入该数据库即可，无需重复构建。</p>
<h2 id="InternLM-接入-LangChain"><a href="#InternLM-接入-LangChain" class="headerlink" title="InternLM 接入 LangChain"></a>InternLM 接入 LangChain</h2><p>为便捷构建 LLM 应用，我们需要基于本地部署的 InternLM，继承 LangChain 的 LLM 类自定义一个 InternLM LLM 子类，从而实现将 InternLM 接入到 LangChain 框架中。完成 LangChain 的自定义 LLM 子类之后，可以以完全一致的方式调用 LangChain 的接口，而无需考虑底层模型调用的不一致。</p>
<p>基于本地部署的 InternLM 自定义 LLM 类并不复杂，我们只需从 LangChain.llms.base.LLM 类继承一个子类，并重写构造函数与 <code>_call</code> 函数即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.llms.base <span class="hljs-keyword">import</span> LLM<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span>, <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span><br><span class="hljs-keyword">from</span> langchain.callbacks.manager <span class="hljs-keyword">import</span> CallbackManagerForLLMRun<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InternLM_LLM</span>(<span class="hljs-title class_ inherited__">LLM</span>):<br>    <span class="hljs-comment"># 基于本地 InternLM 自定义 LLM 类</span><br>    tokenizer : AutoTokenizer = <span class="hljs-literal">None</span><br>    model: AutoModelForCausalLM = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path :<span class="hljs-built_in">str</span></span>):<br>        <span class="hljs-comment"># model_path: InternLM 模型路径</span><br>        <span class="hljs-comment"># 从本地初始化模型</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;正在从本地加载模型...&quot;</span>)<br>        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=<span class="hljs-literal">True</span>)<br>        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=<span class="hljs-literal">True</span>).to(torch.bfloat16).cuda()<br>        self.model = self.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;完成本地模型的加载&quot;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_call</span>(<span class="hljs-params">self, prompt : <span class="hljs-built_in">str</span>, stop: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                run_manager: <span class="hljs-type">Optional</span>[CallbackManagerForLLMRun] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                **kwargs: <span class="hljs-type">Any</span></span>):<br>        <span class="hljs-comment"># 重写调用函数</span><br>        system_prompt = <span class="hljs-string">&quot;&quot;&quot;You are an AI assistant whose name is InternLM (书生·浦语).</span><br><span class="hljs-string">        - InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span><br><span class="hljs-string">        - InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <br>        messages = [(system_prompt, <span class="hljs-string">&#x27;&#x27;</span>)]<br>        response, history = self.model.chat(self.tokenizer, prompt , history=messages)<br>        <span class="hljs-keyword">return</span> response<br>        <br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_llm_type</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;InternLM&quot;</span><br></code></pre></td></tr></table></figure>
<p>在上述类定义中，重写了构造函数和 <code>_call</code> 函数：对于构造函数，我们在对象实例化的一开始加载本地部署的 InternLM 模型，从而避免每一次调用都需要重新加载模型带来的时间过长；<code>_call</code> 函数是 LLM 类的核心函数，LangChain 会调用该函数来调用 LLM，在该函数中，我们调用已实例化模型的 chat 方法，从而实现对模型的调用并返回调用结果。</p>
<p>在整体项目中，我们将上述代码封装为 LLM.py，后续将直接从该文件中引入自定义的 LLM 类。</p>
<h2 id="构建检索问答链"><a href="#构建检索问答链" class="headerlink" title="构建检索问答链"></a>构建检索问答链</h2><p>LangChain 通过提供检索问答链对象来实现对于 RAG 全流程的封装。所谓检索问答链，即通过一个对象完成检索增强问答（即RAG）的全流程，针对 RAG 的更多概念，我们会在视频内容中讲解，也欢迎读者查阅该教程来进一步了解：<a href="https://github.com/datawhalechina/llm-universe/tree/main">《LLM Universe》</a>。我们可以调用一个 LangChain 提供的 <code>RetrievalQA</code> 对象，通过初始化时填入已构建的数据库和自定义 LLM 作为参数，来简便地完成检索增强问答的全流程，LangChain 会自动完成基于用户提问进行检索、获取相关文档、拼接为合适的 Prompt 并交给 LLM 问答的全部流程。</p>
<h3 id="加载向量数据库"><a href="#加载向量数据库" class="headerlink" title="加载向量数据库"></a>加载向量数据库</h3><p>首先我们需要将上文构建的向量数据库导入进来，我们可以直接通过 Chroma 以及上文定义的词向量模型来加载已构建的数据库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 定义 Embeddings</span><br>embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br><br><span class="hljs-comment"># 向量数据库持久化路径</span><br>persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><br><span class="hljs-comment"># 加载数据库</span><br>vectordb = Chroma(<br>    persist_directory=persist_directory, <br>    embedding_function=embeddings<br>)<br></code></pre></td></tr></table></figure>
<p>上述代码得到的 <code>vectordb</code> 对象即为我们已构建的向量数据库对象，该对象可以针对用户的 <code>query</code> 进行语义向量检索，得到与用户提问相关的知识片段。</p>
<h3 id="实例化自定义-LLM-与-Prompt-Template"><a href="#实例化自定义-LLM-与-Prompt-Template" class="headerlink" title="实例化自定义 LLM 与 Prompt Template"></a>实例化自定义 LLM 与 Prompt Template</h3><p>接着，我们实例化一个基于 InternLM 自定义的 LLM 对象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> LLM <span class="hljs-keyword">import</span> InternLM_LLM<br>llm = InternLM_LLM(model_path = <span class="hljs-string">&quot;/root/data/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span>)<br>llm.predict(<span class="hljs-string">&quot;你是谁&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>构建检索问答链，还需要构建一个 Prompt Template，该 Template 其实基于一个带变量的字符串，在检索之后，LangChain 会将检索到的相关文档片段填入到 Template 的变量中，从而实现带知识的 Prompt 构建。我们可以基于 LangChain 的 Template 基类来实例化这样一个 Template 对象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><br><span class="hljs-comment"># 我们所构造的 Prompt 模板</span><br>template = <span class="hljs-string">&quot;&quot;&quot;使用以下上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。</span><br><span class="hljs-string">问题: &#123;question&#125;</span><br><span class="hljs-string">可参考的上下文：</span><br><span class="hljs-string">···</span><br><span class="hljs-string">&#123;context&#125;</span><br><span class="hljs-string">···</span><br><span class="hljs-string">如果给定的上下文无法让你做出回答，请回答你不知道。</span><br><span class="hljs-string">有用的回答:&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 调用 LangChain 的方法来实例化一个 Template 对象，该对象包含了 context 和 question 两个变量，在实际调用时，这两个变量会被检索到的文档片段和用户提问填充</span><br>QA_CHAIN_PROMPT = PromptTemplate(input_variables=[<span class="hljs-string">&quot;context&quot;</span>,<span class="hljs-string">&quot;question&quot;</span>],template=template)<br></code></pre></td></tr></table></figure>
<h3 id="构建检索问答链-1"><a href="#构建检索问答链-1" class="headerlink" title="构建检索问答链"></a>构建检索问答链</h3><p>最后，可以调用 LangChain 提供的检索问答链构造函数，基于我们的自定义 LLM、Prompt Template 和向量知识库来构建一个基于 InternLM 的检索问答链：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br><br>qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectordb.as_retriever(),return_source_documents=<span class="hljs-literal">True</span>,chain_type_kwargs=&#123;<span class="hljs-string">&quot;prompt&quot;</span>:QA_CHAIN_PROMPT&#125;)<br></code></pre></td></tr></table></figure>
<p>得到的 <code>qa_chain</code> 对象即可以实现我们的核心功能，即基于 InternLM 模型的专业知识库助手。我们可以对比该检索问答链和纯 LLM 的问答效果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 检索问答链回答效果</span><br>question = <span class="hljs-string">&quot;什么是InternLM&quot;</span><br>result = qa_chain(&#123;<span class="hljs-string">&quot;query&quot;</span>: question&#125;)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;检索问答链回答 question 的结果：&quot;</span>)<br><span class="hljs-built_in">print</span>(result[<span class="hljs-string">&quot;result&quot;</span>])<br><br><span class="hljs-comment"># 仅 LLM 回答效果</span><br>result_2 = llm(question)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;大模型回答 question 的结果：&quot;</span>)<br><span class="hljs-built_in">print</span>(result_2)<br></code></pre></td></tr></table></figure>
<h2 id="部署一个-Web-Demo"><a href="#部署一个-Web-Demo" class="headerlink" title="部署一个 Web Demo"></a>部署一个 Web Demo</h2><p>之后我们可以基于 Gradio 框架将其部署到 Web 网页，从而搭建一个小型 Demo，便于测试与使用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入必要的库</span><br><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> LLM <span class="hljs-keyword">import</span> InternLM_LLM<br><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_chain</span>():<br>    <span class="hljs-comment"># 加载问答链</span><br>    <span class="hljs-comment"># 定义 Embeddings</span><br>    embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br><br>    <span class="hljs-comment"># 向量数据库持久化路径</span><br>    persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><br>    <span class="hljs-comment"># 加载数据库</span><br>    vectordb = Chroma(<br>        persist_directory=persist_directory,  <span class="hljs-comment"># 允许我们将persist_directory目录保存到磁盘上</span><br>        embedding_function=embeddings<br>    )<br><br>    llm = InternLM_LLM(model_path = <span class="hljs-string">&quot;/root/data/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span>)<br><br>    template = <span class="hljs-string">&quot;&quot;&quot;使用以下上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。</span><br><span class="hljs-string">    问题: &#123;question&#125;</span><br><span class="hljs-string">    可参考的上下文：</span><br><span class="hljs-string">    ···</span><br><span class="hljs-string">    &#123;context&#125;</span><br><span class="hljs-string">    ···</span><br><span class="hljs-string">    如果给定的上下文无法让你做出回答，请回答你不知道。</span><br><span class="hljs-string">    有用的回答:&quot;&quot;&quot;</span><br><br>    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[<span class="hljs-string">&quot;context&quot;</span>,<span class="hljs-string">&quot;question&quot;</span>],<br>                                    template=template)<br><br>    <span class="hljs-comment"># 运行 chain</span><br>    <span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br><br>    qa_chain = RetrievalQA.from_chain_type(llm,<br>                                        retriever=vectordb.as_retriever(),<br>                                        return_source_documents=<span class="hljs-literal">True</span>,<br>                                        chain_type_kwargs=&#123;<span class="hljs-string">&quot;prompt&quot;</span>:QA_CHAIN_PROMPT&#125;)<br>    <br>    <span class="hljs-keyword">return</span> qa_chain<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_center</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    存储问答 Chain 的对象 </span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.chain = load_chain()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">qa_chain_self_answer</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, chat_history: <span class="hljs-built_in">list</span> = []</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        调用不带历史记录的问答链进行回答</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> question == <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">len</span>(question) &lt; <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, chat_history<br>        <span class="hljs-keyword">try</span>:<br>            chat_history.append(<br>                (question, self.chain(&#123;<span class="hljs-string">&quot;query&quot;</span>: question&#125;)[<span class="hljs-string">&quot;result&quot;</span>]))<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, chat_history<br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            <span class="hljs-keyword">return</span> e, chat_history<br><br><br>model_center = Model_center()<br><br>block = gr.Blocks()<br><span class="hljs-keyword">with</span> block <span class="hljs-keyword">as</span> demo:<br>    <span class="hljs-keyword">with</span> gr.Row(equal_height=<span class="hljs-literal">True</span>):   <br>        <span class="hljs-keyword">with</span> gr.Column(scale=<span class="hljs-number">15</span>):<br>            gr.Markdown(<span class="hljs-string">&quot;&quot;&quot;&lt;h1&gt;&lt;center&gt;InternLM&lt;/center&gt;&lt;/h1&gt;</span><br><span class="hljs-string">                &lt;center&gt;书生浦语&lt;/center&gt;</span><br><span class="hljs-string">                &quot;&quot;&quot;</span>)<br>        <span class="hljs-comment"># gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)</span><br><br>    <span class="hljs-keyword">with</span> gr.Row():<br>        <span class="hljs-keyword">with</span> gr.Column(scale=<span class="hljs-number">4</span>):<br>            chatbot = gr.Chatbot(height=<span class="hljs-number">450</span>, show_copy_button=<span class="hljs-literal">True</span>)<br>            <span class="hljs-comment"># 创建一个文本框组件，用于输入 prompt。</span><br>            msg = gr.Textbox(label=<span class="hljs-string">&quot;Prompt/问题&quot;</span>)<br><br>            <span class="hljs-keyword">with</span> gr.Row():<br>                <span class="hljs-comment"># 创建提交按钮。</span><br>                db_wo_his_btn = gr.Button(<span class="hljs-string">&quot;Chat&quot;</span>)<br>            <span class="hljs-keyword">with</span> gr.Row():<br>                <span class="hljs-comment"># 创建一个清除按钮，用于清除聊天机器人组件的内容。</span><br>                clear = gr.ClearButton(<br>                    components=[chatbot], value=<span class="hljs-string">&quot;Clear console&quot;</span>)<br>                <br>        <span class="hljs-comment"># 设置按钮的点击事件。当点击时，调用上面定义的 qa_chain_self_answer 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。</span><br>        db_wo_his_btn.click(model_center.qa_chain_self_answer, inputs=[<br>                            msg, chatbot], outputs=[msg, chatbot])<br>        <br>    gr.Markdown(<span class="hljs-string">&quot;&quot;&quot;提醒：&lt;br&gt;</span><br><span class="hljs-string">    1. 初始化数据库时间可能较长，请耐心等待。</span><br><span class="hljs-string">    2. 使用中如果出现异常，将会在文本输入框进行展示，请不要惊慌。 &lt;br&gt;</span><br><span class="hljs-string">    &quot;&quot;&quot;</span>)<br><span class="hljs-comment"># threads to consume the request</span><br>gr.close_all()<br><span class="hljs-comment"># 启动新的 Gradio 应用，设置分享功能为 True，并使用环境变量 PORT1 指定服务器端口。</span><br><span class="hljs-comment"># demo.launch(share=True, server_port=int(os.environ[&#x27;PORT1&#x27;]))</span><br><span class="hljs-comment"># 直接启动</span><br>demo.launch()<br></code></pre></td></tr></table></figure>
<p>运行截图如下：</p>
<p><img  src="gradio.png"  ><span class="image-caption">运行gradio</span></p>
<p><img  src="Langchain+InternLM问答.png"  ><span class="image-caption">Langchain+InternLM问答</span></p>
<p>如图，能够正确地回答知识库中的知识。</p>
<h2 id="问题解决以及-Langchain-调试"><a href="#问题解决以及-Langchain-调试" class="headerlink" title="问题解决以及 Langchain 调试"></a>问题解决以及 Langchain 调试</h2><p>我们在遇到奇怪问题的时候，想要调试 Langchain，这个时候可以借助 Langchain 的全局设置设置调试模式，设置方式如下所示：</p>
<p><a href="https://python.langchain.com/docs/guides/debugging">Debugging | 🦜️🔗 Langchain</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.<span class="hljs-built_in">globals</span> <span class="hljs-keyword">import</span> set_verbose <span class="hljs-comment"># 我这里用的 langchain 版本为 0.1.0</span><br><br>set_verbose(<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<p><img  src="langchain-debug.png"  ><span class="image-caption">langchain 的调试输出</span></p>
<h2 id="将应用部署在-OpenXLab-上"><a href="#将应用部署在-OpenXLab-上" class="headerlink" title="将应用部署在 OpenXLab 上"></a>将应用部署在 OpenXLab 上</h2><p><a href="https://openxlab.org.cn/apps/detail/EnableAsync/network-bot">计算机网络问答机器人</a></p>
<h3 id="Sqlite-问题1"><a href="#Sqlite-问题1" class="headerlink" title="Sqlite 问题1"></a>Sqlite 问题<sup><a href="#fn_1" id="reffn_1">1</a></sup></h3><p>OpenXLab 上的 sqlite3 版本低于我们项目用的 Chroma 要求。可参考<a href="https://link.zhihu.com/?target=https%3A//docs.trychroma.com/troubleshooting%23sqlite"> Troubleshooting | Chroma (trychroma.com)</a>，在 <code>requirements.txt</code> 中添加 <code>pysqlite3-binary</code> ，之后加载 sqlite3 库来绕过这个问题。否则就要写脚本在运行时自己安装上更新版本的sqlite3了。下面是修改加载 sqlite3 库的 trick 命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">__import__</span>(<span class="hljs-string">&#x27;pysqlite3&#x27;</span>)<br><span class="hljs-keyword">import</span> sys<br>sys.modules[<span class="hljs-string">&#x27;sqlite3&#x27;</span>] = sys.modules.pop(<span class="hljs-string">&#x27;pysqlite3&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="运行截图"><a href="#运行截图" class="headerlink" title="运行截图"></a>运行截图</h3><p><img  src="openxlab-deploy.png"  ><span class="image-caption">openxlab-deploy</span></p>
<p><img  src="loading.png"  ><span class="image-caption">加载模型</span></p>
<p><img  src="部署.png"  ><span class="image-caption">部署</span></p>
<p><img  src="运行日志.png"  ><span class="image-caption">运行日志</span></p>
<h2 id="参考内容"><a href="#参考内容" class="headerlink" title="参考内容"></a>参考内容</h2><blockquote id="fn_1">
<sup>1</sup>. <a href="https://zhuanlan.zhihu.com/p/676719586">书生・浦语大模型实战营第三课作业(基础+进阶) - 知乎 (zhihu.com)</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>Internlm-04-XTuner 大模型单卡低成本微调实战</title>
    <url>/internlm/internlm-04/</url>
    <content><![CDATA[<h1 id="XTuner-大模型单卡低成本微调实战"><a href="#XTuner-大模型单卡低成本微调实战" class="headerlink" title="XTuner 大模型单卡低成本微调实战"></a>XTuner 大模型单卡低成本微调实战</h1><p>微调前<br><img  src="官方回答.png"  ><span class="image-caption">官方回答</span></p>
<p>微调后<br><img  src="微调后.png"  ><span class="image-caption">微调后.png</span></p>
<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h2><h3 id="1-1-XTuner"><a href="#1-1-XTuner" class="headerlink" title="1.1 XTuner"></a>1.1 XTuner</h3><p>一个大语言模型微调工具箱。由 MMRazor 和 MMDeploy 联合开发。</p>
<h3 id="1-2-支持的开源LLM-2023-11-01"><a href="#1-2-支持的开源LLM-2023-11-01" class="headerlink" title="1.2 支持的开源LLM (2023.11.01)"></a>1.2 支持的开源LLM (2023.11.01)</h3><ul>
<li><a href="https://huggingface.co/internlm/internlm-7b">InternLM</a></li>
<li><a href="https://huggingface.co/meta-llama">Llama，Llama2</a></li>
<li><a href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2</a>，<a href="https://huggingface.co/THUDM/chatglm3-6b-base">ChatGLM3</a></li>
<li><a href="https://huggingface.co/Qwen/Qwen-7B">Qwen</a></li>
<li><a href="https://huggingface.co/baichuan-inc/Baichuan-7B">Baichuan</a>，<a href="https://huggingface.co/baichuan-inc/Baichuan2-7B-Base">Baichuan2</a></li>
<li><a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta">Zephyr</a> </li>
</ul>
<h3 id="1-3-特色"><a href="#1-3-特色" class="headerlink" title="1.3 特色"></a>1.3 特色</h3><ul>
<li><strong>傻瓜化：</strong> 以 配置文件 的形式封装了大部分微调场景，<strong>0基础的非专业人员也能一键开始微调</strong>。</li>
<li><strong>轻量级：</strong> 对于 7B 参数量的LLM，<strong>微调所需的最小显存仅为 8GB</strong></li>
</ul>
<h3 id="1-4-微调原理"><a href="#1-4-微调原理" class="headerlink" title="1.4 微调原理"></a>1.4 微调原理</h3><blockquote>
<p>想象一下，你有一个超大的玩具，现在你想改造这个超大的玩具。但是，<strong>对整个玩具进行全面的改动会非常昂贵</strong>。</p>
</blockquote>
<p>※ 因此，你找到了一种叫 <strong>LoRA</strong> 的方法：<strong>只对玩具中的某些零件进行改动，而不是对整个玩具进行全面改动</strong>。</p>
<p>※ 而 <strong>QLoRA</strong> 是 LoRA 的一种改进</p>
<h2 id="2-快速上手"><a href="#2-快速上手" class="headerlink" title="2 快速上手"></a>2 快速上手</h2><h3 id="2-1-平台"><a href="#2-1-平台" class="headerlink" title="2.1 平台"></a>2.1 平台</h3><p>Ubuntu + Anaconda + CUDA/CUDNN + 8GB nvidia显卡</p>
<h3 id="2-2-安装"><a href="#2-2-安装" class="headerlink" title="2.2 安装"></a>2.2 安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 如果你是在 InternStudio 平台，则从本地 clone 一个已有 pytorch 2.0.1 的环境：</span><br>/root/share/install_conda_env_internlm_base.sh xtuner0.1.9<br><span class="hljs-comment"># 如果你是在其他平台：</span><br>conda create --name xtuner0.1.9 python=3.10 -y<br><br><span class="hljs-comment"># 激活环境</span><br>conda activate xtuner0.1.9<br><span class="hljs-comment"># 进入家目录 （~的意思是 “当前用户的home路径”）</span><br><span class="hljs-built_in">cd</span> ~<br><span class="hljs-comment"># 创建版本文件夹并进入，以跟随本教程</span><br><span class="hljs-built_in">mkdir</span> xtuner019 &amp;&amp; <span class="hljs-built_in">cd</span> xtuner019<br><br><br><span class="hljs-comment"># 拉取 0.1.9 的版本源码</span><br>git <span class="hljs-built_in">clone</span> -b v0.1.9  https://github.com/InternLM/xtuner<br><span class="hljs-comment"># 无法访问github的用户请从 gitee 拉取:</span><br><span class="hljs-comment"># git clone -b v0.1.9 https://gitee.com/Internlm/xtuner</span><br><br><span class="hljs-comment"># 进入源码目录</span><br><span class="hljs-built_in">cd</span> xtuner<br><br><span class="hljs-comment"># 从源码安装 XTuner</span><br>pip install -e <span class="hljs-string">&#x27;.[all]&#x27;</span><br></code></pre></td></tr></table></figure>
<p>安装完后，就开始搞搞准备工作了。（准备在 oasst1 数据集上微调 internlm-7b-chat）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建一个微调 oasst1 数据集的工作路径，进入</span><br><span class="hljs-built_in">mkdir</span> ~/ft-oasst1 &amp;&amp; <span class="hljs-built_in">cd</span> ~/ft-oasst1<br></code></pre></td></tr></table></figure>
<h3 id="2-3-微调"><a href="#2-3-微调" class="headerlink" title="2.3 微调"></a>2.3 微调</h3><h4 id="2-3-1-准备配置文件"><a href="#2-3-1-准备配置文件" class="headerlink" title="2.3.1 准备配置文件"></a>2.3.1 准备配置文件</h4><p>XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-comment"># 列出所有内置配置</span><br>xtuner list-cfg<br></code></pre></td></tr></table></figure>
<blockquote>
<p>假如显示bash: xtuner: command not found的话可以考虑在终端输入 export PATH=$PATH:’/root/.local/bin’</p>
</blockquote>
<p><img  src="cfg-list.png"  ><span class="image-caption">部分配置文件展示</span></p>
<p>拷贝一个配置文件到当前目录：<br><code># xtuner copy-cfg $&#123;CONFIG_NAME&#125; $&#123;SAVE_PATH&#125;</code></p>
<p>在本案例中即：（注意最后有个英文句号，代表复制到当前路径）<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-built_in">cd</span> ~/ft-oasst1<br>xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .<br></code></pre></td></tr></table></figure></p>
<p>配置文件名的解释：</p>
<blockquote>
<p>xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型名</th>
<th>internlm_chat_7b</th>
</tr>
</thead>
<tbody>
<tr>
<td>使用算法</td>
<td>qlora</td>
</tr>
<tr>
<td>数据集</td>
<td>oasst1</td>
</tr>
<tr>
<td>把数据集跑几次</td>
<td>跑3次：e3 (epoch 3 )</td>
</tr>
</tbody>
</table>
</div>
<p>*无 chat比如 <code>internlm-7b</code> 代表是基座(base)模型</p>
<h4 id="2-3-2-模型下载"><a href="#2-3-2-模型下载" class="headerlink" title="2.3.2 模型下载"></a>2.3.2 模型下载</h4><blockquote>
<p>由于下载模型很慢，用教学平台的同学可以直接复制模型。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-built_in">cp</span> -r /root/share/temp/model_repos/internlm-chat-7b ~/ft-oasst1/<br></code></pre></td></tr></table></figure>
<blockquote>
<p>以下是自己下载模型的步骤。</p>
</blockquote>
<p>不用 xtuner 默认的<code>从 huggingface 拉取模型</code>，而是提前从 ModelScope 下载模型到本地</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-comment"># 创建一个目录，放模型文件，防止散落一地</span><br><span class="hljs-built_in">mkdir</span> ~/ft-oasst1/internlm-chat-7b<br><br><span class="hljs-comment"># 装一下拉取模型文件要用的库</span><br>pip install modelscope<br><br><span class="hljs-comment"># 从 modelscope 下载下载模型文件</span><br><span class="hljs-built_in">cd</span> ~/ft-oasst1<br>apt install git git-lfs -y<br>git lfs install<br>git lfs <span class="hljs-built_in">clone</span> https://modelscope.cn/Shanghai_AI_Laboratory/internlm-chat-7b.git -b v1.0.3<br></code></pre></td></tr></table></figure>
<h4 id="2-3-3-数据集下载"><a href="#2-3-3-数据集下载" class="headerlink" title="2.3.3 数据集下载"></a>2.3.3 数据集下载</h4><blockquote>
<p><a href="https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/main">https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/main</a></p>
</blockquote>
<p>由于 huggingface 网络问题，咱们已经给大家提前下载好了，复制到正确位置即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~/ft-oasst1<br><span class="hljs-comment"># ...-guanaco 后面有个空格和英文句号啊</span><br><span class="hljs-built_in">cp</span> -r /root/share/temp/datasets/openassistant-guanaco .<br></code></pre></td></tr></table></figure>
<p>此时，当前路径的文件应该长这样：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">|-- internlm-chat-7b<br>|   |-- README.md<br>|   |-- config.json<br>|   |-- configuration.json<br>|   |-- configuration_internlm.py<br>|   |-- generation_config.json<br>|   |-- modeling_internlm.py<br>|   |-- pytorch_model-00001-of-00008.bin<br>|   |-- pytorch_model-00002-of-00008.bin<br>|   |-- pytorch_model-00003-of-00008.bin<br>|   |-- pytorch_model-00004-of-00008.bin<br>|   |-- pytorch_model-00005-of-00008.bin<br>|   |-- pytorch_model-00006-of-00008.bin<br>|   |-- pytorch_model-00007-of-00008.bin<br>|   |-- pytorch_model-00008-of-00008.bin<br>|   |-- pytorch_model.bin.index.json<br>|   |-- special_tokens_map.json<br>|   |-- tokenization_internlm.py<br>|   |-- tokenizer.model<br>|   `-- tokenizer_config.json<br>|-- internlm_chat_7b_qlora_oasst1_e3_copy.py<br>`-- openassistant-guanaco<br>    |-- openassistant_best_replies_eval.jsonl<br>    `-- openassistant_best_replies_train.jsonl<br></code></pre></td></tr></table></figure>
<h4 id="2-3-4-修改配置文件"><a href="#2-3-4-修改配置文件" class="headerlink" title="2.3.4 修改配置文件"></a>2.3.4 修改配置文件</h4><p>修改其中的模型和数据集为 本地路径</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~/ft-oasst1<br>vim internlm_chat_7b_qlora_oasst1_e3_copy.py<br></code></pre></td></tr></table></figure>
<blockquote>
<p>在vim界面完成修改后，请输入:wq退出。假如认为改错了可以用:q!退出且不保存。当然我们也可以考虑打开python文件直接修改，但注意修改完后需要按下Ctrl+S进行保存。</p>
</blockquote>
<p>减号代表要删除的行，加号代表要增加的行。<br><figure class="highlight diff"><table><tr><td class="code"><pre><code class="hljs diff"># 修改模型为本地路径<br><span class="hljs-deletion">- pretrained_model_name_or_path = &#x27;internlm/internlm-chat-7b&#x27;</span><br><span class="hljs-addition">+ pretrained_model_name_or_path = &#x27;./internlm-chat-7b&#x27;</span><br><br># 修改训练数据集为本地路径<br><span class="hljs-deletion">- data_path = &#x27;timdettmers/openassistant-guanaco&#x27;</span><br><span class="hljs-addition">+ data_path = &#x27;./openassistant-guanaco&#x27;</span><br></code></pre></td></tr></table></figure></p>
<p><strong>常用超参</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数名</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>data_path</strong></td>
<td>数据路径或 HuggingFace 仓库名</td>
</tr>
<tr>
<td>max_length</td>
<td>单条数据最大 Token 数，超过则截断</td>
</tr>
<tr>
<td>pack_to_max_length</td>
<td>是否将多条短数据拼接到 max_length，提高 GPU 利用率</td>
</tr>
<tr>
<td>accumulative_counts</td>
<td>梯度累积，每多少次 backward 更新一次参数</td>
</tr>
<tr>
<td>evaluation_inputs</td>
<td>训练过程中，会根据给定的问题进行推理，便于观测训练状态</td>
</tr>
<tr>
<td>evaluation_freq</td>
<td>Evaluation 的评测间隔 iter 数</td>
</tr>
<tr>
<td>……</td>
<td>……</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>如果想把显卡的现存吃满，充分利用显卡资源，可以将 <code>max_length</code> 和 <code>batch_size</code> 这两个参数调大。</p>
</blockquote>
<h4 id="2-3-5-开始微调"><a href="#2-3-5-开始微调" class="headerlink" title="2.3.5 开始微调"></a>2.3.5 开始微调</h4><p><strong>训练：</strong></p>
<p>xtuner train ${CONFIG_NAME_OR_PATH}</p>
<p><strong>也可以增加 deepspeed 进行训练加速：</strong></p>
<p>xtuner train ${CONFIG_NAME_OR_PATH} —deepspeed deepspeed_zero2</p>
<p>例如，我们可以利用 QLoRA 算法在 oasst1 数据集上微调 InternLM-7B：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-comment"># 单卡</span><br><span class="hljs-comment">## 用刚才改好的config文件训练</span><br>xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py<br><br><span class="hljs-comment"># 多卡</span><br>NPROC_PER_NODE=<span class="hljs-variable">$&#123;GPU_NUM&#125;</span> xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py<br><br><span class="hljs-comment"># 若要开启 deepspeed 加速，增加 --deepspeed deepspeed_zero2 即可</span><br></code></pre></td></tr></table></figure>
<blockquote>
<p>微调得到的 PTH 模型文件和其他杂七杂八的文件都默认在当前的 <code>./work_dirs</code> 中。</p>
</blockquote>
<p><img  src="train.png"  ><span class="image-caption">训练截图</span></p>
<p>跑完训练后，当前路径应该长这样：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash">|-- internlm-chat-7b<br>|-- internlm_chat_7b_qlora_oasst1_e3_copy.py<br>|-- openassistant-guanaco<br>|   |-- openassistant_best_replies_eval.jsonl<br>|   `-- openassistant_best_replies_train.jsonl<br>`-- work_dirs<br>    `-- internlm_chat_7b_qlora_oasst1_e3_copy<br>        |-- 20231101_152923<br>        |   |-- 20231101_152923.<span class="hljs-built_in">log</span><br>        |   `-- vis_data<br>        |       |-- 20231101_152923.json<br>        |       |-- config.py<br>        |       `-- scalars.json<br>        |-- epoch_1.pth<br>        |-- epoch_2.pth<br>        |-- epoch_3.pth<br>        |-- internlm_chat_7b_qlora_oasst1_e3_copy.py<br>        `-- last_checkpoint<br></code></pre></td></tr></table></figure>
<h4 id="2-3-6-将得到的-PTH-模型转换为-HuggingFace-模型，即：生成-Adapter-文件夹"><a href="#2-3-6-将得到的-PTH-模型转换为-HuggingFace-模型，即：生成-Adapter-文件夹" class="headerlink" title="2.3.6 将得到的 PTH 模型转换为 HuggingFace 模型，即：生成 Adapter 文件夹"></a>2.3.6 将得到的 PTH 模型转换为 HuggingFace 模型，<strong>即：生成 Adapter 文件夹</strong></h4><p><code>xtuner convert pth_to_hf $&#123;CONFIG_NAME_OR_PATH&#125; $&#123;PTH_file_dir&#125; $&#123;SAVE_PATH&#125;</code></p>
<p>在本示例中，为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> hf<br><span class="hljs-built_in">export</span> MKL_SERVICE_FORCE_INTEL=1<br><br>xtuner convert pth_to_hf ./internlm_chat_7b_qlora_oasst1_e3_copy.py ./work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_1.pth ./hf<br></code></pre></td></tr></table></figure><br>此时，路径中应该长这样：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash">|-- internlm-chat-7b<br>|-- internlm_chat_7b_qlora_oasst1_e3_copy.py<br>|-- openassistant-guanaco<br>|   |-- openassistant_best_replies_eval.jsonl<br>|   `-- openassistant_best_replies_train.jsonl<br>|-- hf<br>|   |-- README.md<br>|   |-- adapter_config.json<br>|   |-- adapter_model.bin<br>|   `-- xtuner_config.py<br>`-- work_dirs<br>    `-- internlm_chat_7b_qlora_oasst1_e3_copy<br>        |-- 20231101_152923<br>        |   |-- 20231101_152923.<span class="hljs-built_in">log</span><br>        |   `-- vis_data<br>        |       |-- 20231101_152923.json<br>        |       |-- config.py<br>        |       `-- scalars.json<br>        |-- epoch_1.pth<br>        |-- epoch_2.pth<br>        |-- epoch_3.pth<br>        |-- internlm_chat_7b_qlora_oasst1_e3_copy.py<br>        `-- last_checkpoint<br></code></pre></td></tr></table></figure>
<p><span style="color: red;"><strong>此时，hf 文件夹即为我们平时所理解的所谓 “LoRA 模型文件”</strong></span></p>
<blockquote>
<p>可以简单理解：LoRA 模型文件 = Adapter</p>
</blockquote>
<h3 id="2-4-部署与测试"><a href="#2-4-部署与测试" class="headerlink" title="2.4 部署与测试"></a>2.4 部署与测试</h3><h4 id="2-4-1-将-HuggingFace-adapter-合并到大语言模型："><a href="#2-4-1-将-HuggingFace-adapter-合并到大语言模型：" class="headerlink" title="2.4.1 将 HuggingFace adapter 合并到大语言模型："></a>2.4.1 将 HuggingFace adapter 合并到大语言模型：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash">xtuner convert merge ./internlm-chat-7b ./hf ./merged --max-shard-size 2GB<br><span class="hljs-comment"># xtuner convert merge \</span><br><span class="hljs-comment">#     $&#123;NAME_OR_PATH_TO_LLM&#125; \</span><br><span class="hljs-comment">#     $&#123;NAME_OR_PATH_TO_ADAPTER&#125; \</span><br><span class="hljs-comment">#     $&#123;SAVE_PATH&#125; \</span><br><span class="hljs-comment">#     --max-shard-size 2GB</span><br></code></pre></td></tr></table></figure>
<h4 id="2-4-2-与合并后的模型对话："><a href="#2-4-2-与合并后的模型对话：" class="headerlink" title="2.4.2 与合并后的模型对话："></a>2.4.2 与合并后的模型对话：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-comment"># 加载 Adapter 模型对话（Float 16）</span><br>xtuner chat ./merged --prompt-template internlm_chat<br><br><span class="hljs-comment"># 4 bit 量化加载</span><br><span class="hljs-comment"># xtuner chat ./merged --bits 4 --prompt-template internlm_chat</span><br></code></pre></td></tr></table></figure>
<h4 id="2-4-3-Demo"><a href="#2-4-3-Demo" class="headerlink" title="2.4.3 Demo"></a>2.4.3 Demo</h4><ul>
<li>修改 <code>cli_demo.py</code> 中的模型路径<figure class="highlight diff"><table><tr><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">- model_name_or_path = &quot;/root/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span><br><span class="hljs-addition">+ model_name_or_path = &quot;merged&quot;</span><br></code></pre></td></tr></table></figure></li>
<li>运行 <code>cli_demo.py</code> 以目测微调效果<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python ./cli_demo.py<br></code></pre></td></tr></table></figure>
</li>
</ul>
<p><strong><code>xtuner chat</code></strong> <strong>的启动参数</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>启动参数</th>
<th>干哈滴</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>—prompt-template</strong></td>
<td>指定对话模板</td>
</tr>
<tr>
<td>—system</td>
<td>指定SYSTEM文本</td>
</tr>
<tr>
<td>—system-template</td>
<td>指定SYSTEM模板</td>
</tr>
<tr>
<td>-<strong>-bits</strong></td>
<td>LLM位数</td>
</tr>
<tr>
<td>—bot-name</td>
<td>bot名称</td>
</tr>
<tr>
<td>—with-plugins</td>
<td>指定要使用的插件</td>
</tr>
<tr>
<td><strong>—no-streamer</strong></td>
<td>是否启用流式传输</td>
</tr>
<tr>
<td><strong>—lagent</strong></td>
<td>是否使用lagent</td>
</tr>
<tr>
<td>—command-stop-word</td>
<td>命令停止词</td>
</tr>
<tr>
<td>—answer-stop-word</td>
<td>回答停止词</td>
</tr>
<tr>
<td>—offload-folder</td>
<td>存放模型权重的文件夹（或者已经卸载模型权重的文件夹）</td>
</tr>
<tr>
<td>—max-new-tokens</td>
<td>生成文本中允许的最大 <code>token</code> 数量</td>
</tr>
<tr>
<td><strong>—temperature</strong></td>
<td>温度值</td>
</tr>
<tr>
<td>—top-k</td>
<td>保留用于顶k筛选的最高概率词汇标记数</td>
</tr>
<tr>
<td>—top-p</td>
<td>如果设置为小于1的浮点数，仅保留概率相加高于 <code>top_p</code> 的最小一组最有可能的标记</td>
</tr>
<tr>
<td>—seed</td>
<td>用于可重现文本生成的随机种子</td>
</tr>
</tbody>
</table>
</div>
<h2 id="3-自定义微调"><a href="#3-自定义微调" class="headerlink" title="3 自定义微调"></a>3 自定义微调</h2><blockquote>
<p>以 <strong><a href="https://github.com/abachaa/Medication_QA_MedInfo2019">Medication QA</a></strong> <strong>数据集</strong>为例</p>
</blockquote>
<h3 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h3><h4 id="3-1-1-场景需求"><a href="#3-1-1-场景需求" class="headerlink" title="3.1.1 场景需求"></a>3.1.1 <strong>场景需求</strong></h4><p>   基于 InternLM-chat-7B 模型，用 MedQA 数据集进行微调，将其往<code>医学问答</code>领域对齐。</p>
<h4 id="3-1-2-真实数据预览"><a href="#3-1-2-真实数据预览" class="headerlink" title="3.1.2 真实数据预览"></a>3.1.2 <strong>真实数据预览</strong></h4><div class="table-container">
<table>
<thead>
<tr>
<th>问题</th>
<th>答案</th>
</tr>
</thead>
<tbody>
<tr>
<td>What are ketorolac eye drops?（什么是酮咯酸滴眼液？）</td>
<td>Ophthalmic   ketorolac is used to treat itchy eyes caused by allergies. It also is used to   treat swelling and redness (inflammation) that can occur after cataract   surgery. Ketorolac is in a class of medications called nonsteroidal   anti-inflammatory drugs (NSAIDs). It works by stopping the release of   substances that cause allergy symptoms and inflammation.</td>
</tr>
<tr>
<td>What medicines raise blood sugar? （什么药物会升高血糖？）</td>
<td>Some   medicines for conditions other than diabetes can raise your blood sugar   level. This is a concern when you have diabetes. Make sure every doctor you   see knows about all of the medicines, vitamins, or herbal supplements you   take. This means anything you take with or without a prescription. Examples include:     Barbiturates.     Thiazide diuretics.     Corticosteroids.     Birth control pills (oral contraceptives) and progesterone.     Catecholamines.     Decongestants that contain beta-adrenergic agents, such as pseudoephedrine.     The B vitamin niacin. The risk of high blood sugar from niacin lowers after you have taken it for a few months. The antipsychotic medicine olanzapine (Zyprexa).</td>
</tr>
</tbody>
</table>
</div>
<h3 id="3-2-数据准备"><a href="#3-2-数据准备" class="headerlink" title="3.2 数据准备"></a>3.2 数据准备</h3><blockquote>
<p><strong>以</strong> <strong><a href="https://github.com/abachaa/Medication_QA_MedInfo2019">Medication QA</a></strong> <strong>数据集为例</strong></p>
</blockquote>
<p><strong>原格式：(.xlsx)</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>问题</strong></th>
<th>药物类型</th>
<th>问题类型</th>
<th><strong>回答</strong></th>
<th>主题</th>
<th>URL</th>
</tr>
</thead>
<tbody>
<tr>
<td>aaa</td>
<td>bbb</td>
<td>ccc</td>
<td>ddd</td>
<td>eee</td>
<td>fff</td>
</tr>
</tbody>
</table>
</div>
<h4 id="3-2-1-将数据转为-XTuner-的数据格式"><a href="#3-2-1-将数据转为-XTuner-的数据格式" class="headerlink" title="3.2.1 将数据转为 XTuner 的数据格式"></a>3.2.1 将数据转为 XTuner 的数据格式</h4><p><strong>目标格式：(.jsonL)</strong></p>
<figure class="highlight json"><table><tr><td class="code"><pre><code class="hljs JSON"><span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;conversation&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;system&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;input&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;output&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;conversation&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;system&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;input&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;output&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;xxx&quot;</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure>
<p>通过 pytho n脚本：将 <code>.xlsx</code> 中的 问题 和 回答 两列 提取出来，再放入 <code>.jsonL</code> 文件的每个 conversation 的 input 和 output 中。</p>
<blockquote>
<p>这一步的 python 脚本可以请 ChatGPT 来完成。</p>
</blockquote>
<figure class="highlight text"><table><tr><td class="code"><pre><code class="hljs text">Write a python file for me. using openpyxl. input file name is MedQA2019.xlsx<br>Step1: The input file is .xlsx. Exact the column A and column D in the sheet named &quot;DrugQA&quot; .<br>Step2: Put each value in column A into each &quot;input&quot; of each &quot;conversation&quot;. Put each value in column D into each &quot;output&quot; of each &quot;conversation&quot;.<br>Step3: The output file is .jsonL. It looks like:<br>[&#123;<br>    &quot;conversation&quot;:[<br>        &#123;<br>            &quot;system&quot;: &quot;xxx&quot;,<br>            &quot;input&quot;: &quot;xxx&quot;,<br>            &quot;output&quot;: &quot;xxx&quot;<br>        &#125;<br>    ]<br>&#125;,<br>&#123;<br>    &quot;conversation&quot;:[<br>        &#123;<br>            &quot;system&quot;: &quot;xxx&quot;,<br>            &quot;input&quot;: &quot;xxx&quot;,<br>            &quot;output&quot;: &quot;xxx&quot;<br>        &#125;<br>    ]<br>&#125;]<br>Step4: All &quot;system&quot; value changes to &quot;You are a professional, highly experienced doctor professor. You always provide accurate, comprehensive, and detailed answers based on the patients&#x27; questions.&quot;<br></code></pre></td></tr></table></figure>
<blockquote>
<p>ChatGPT 生成的 python 代码见本仓库的 <a href="./xlsx2jsonl.py">xlsx2jsonl.py</a></p>
</blockquote>
<p>执行 python 脚本，获得格式化后的数据集：<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python xlsx2jsonl.py<br></code></pre></td></tr></table></figure></p>
<p>此时，当然也可以对数据进行训练集和测试集的分割，同样可以让 ChatGPT 写 python 代码。当然如果你没有严格的科研需求、不在乎“训练集泄露”的问题，也可以不做训练集与测试集的分割。</p>
<h4 id="3-2-2-划分训练集和测试集"><a href="#3-2-2-划分训练集和测试集" class="headerlink" title="3.2.2 划分训练集和测试集"></a>3.2.2 划分训练集和测试集</h4><figure class="highlight text"><table><tr><td class="code"><pre><code class="hljs text">my .jsonL file looks like:<br>[&#123;<br>    &quot;conversation&quot;:[<br>        &#123;<br>            &quot;system&quot;: &quot;xxx&quot;,<br>            &quot;input&quot;: &quot;xxx&quot;,<br>            &quot;output&quot;: &quot;xxx&quot;<br>        &#125;<br>    ]<br>&#125;,<br>&#123;<br>    &quot;conversation&quot;:[<br>        &#123;<br>            &quot;system&quot;: &quot;xxx&quot;,<br>            &quot;input&quot;: &quot;xxx&quot;,<br>            &quot;output&quot;: &quot;xxx&quot;<br>        &#125;<br>    ]<br>&#125;]<br>Step1, read the .jsonL file.<br>Step2, count the amount of the &quot;conversation&quot; elements.<br>Step3, randomly split all &quot;conversation&quot; elements by 7:3. Targeted structure is same as the input.<br>Step4, save the 7/10 part as train.jsonl. save the 3/10 part as test.jsonl<br></code></pre></td></tr></table></figure>
<p>生成的python代码见 <a href="./split2train_and_test.py">split2train_and_test.py</a></p>
<h3 id="3-3-开始自定义微调"><a href="#3-3-开始自定义微调" class="headerlink" title="3.3 开始自定义微调"></a>3.3 开始自定义微调</h3><p>此时，我们重新建一个文件夹来玩“微调自定义数据集”<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> ~/ft-medqa &amp;&amp; <span class="hljs-built_in">cd</span> ~/ft-medqa<br></code></pre></td></tr></table></figure></p>
<p>把前面下载好的internlm-chat-7b模型文件夹拷贝过来。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> -r ~/ft-oasst1/internlm-chat-7b .<br></code></pre></td></tr></table></figure>
<p>别忘了把自定义数据集，即几个 <code>.jsonL</code>，也传到服务器上。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/InternLM/tutorial<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> ~/tutorial/xtuner/MedQA2019-structured-train.jsonl .<br></code></pre></td></tr></table></figure>
<h4 id="3-3-1-准备配置文件"><a href="#3-3-1-准备配置文件" class="headerlink" title="3.3.1 准备配置文件"></a>3.3.1 准备配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 复制配置文件到当前目录</span><br>xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .<br><span class="hljs-comment"># 改个文件名</span><br><span class="hljs-built_in">mv</span> internlm_chat_7b_qlora_oasst1_e3_copy.py internlm_chat_7b_qlora_medqa2019_e3.py<br><br><span class="hljs-comment"># 修改配置文件内容</span><br>vim internlm_chat_7b_qlora_medqa2019_e3.py<br></code></pre></td></tr></table></figure>
<p>减号代表要删除的行，加号代表要增加的行。<br><figure class="highlight diff"><table><tr><td class="code"><pre><code class="hljs diff"># 修改import部分<br><span class="hljs-deletion">- from xtuner.dataset.map_fns import oasst1_map_fn, template_map_fn_factory</span><br><span class="hljs-addition">+ from xtuner.dataset.map_fns import template_map_fn_factory</span><br><br># 修改模型为本地路径<br><span class="hljs-deletion">- pretrained_model_name_or_path = &#x27;internlm/internlm-chat-7b&#x27;</span><br><span class="hljs-addition">+ pretrained_model_name_or_path = &#x27;./internlm-chat-7b&#x27;</span><br><br># 修改训练数据为 MedQA2019-structured-train.jsonl 路径<br><span class="hljs-deletion">- data_path = &#x27;timdettmers/openassistant-guanaco&#x27;</span><br><span class="hljs-addition">+ data_path = &#x27;MedQA2019-structured-train.jsonl&#x27;</span><br><br># 修改 train_dataset 对象<br>train_dataset = dict(<br>    type=process_hf_dataset,<br><span class="hljs-deletion">-   dataset=dict(type=load_dataset, path=data_path),</span><br><span class="hljs-addition">+   dataset=dict(type=load_dataset, path=&#x27;json&#x27;, data_files=dict(train=data_path)),</span><br>    tokenizer=tokenizer,<br>    max_length=max_length,<br><span class="hljs-deletion">-   dataset_map_fn=alpaca_map_fn,</span><br><span class="hljs-addition">+   dataset_map_fn=None,</span><br>    template_map_fn=dict(<br>        type=template_map_fn_factory, template=prompt_template),<br>    remove_unused_columns=True,<br>    shuffle_before_pack=True,<br>    pack_to_max_length=pack_to_max_length)<br></code></pre></td></tr></table></figure></p>
<h4 id="3-3-2-XTuner！启动！"><a href="#3-3-2-XTuner！启动！" class="headerlink" title="3.3.2 XTuner！启动！"></a>3.3.2 <strong>XTuner！启动！</strong></h4><p><img  src="imgs/ysqd.png"  ><span class="image-caption">tH8udZzECYl5are.png</span></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">xtuner train internlm_chat_7b_qlora_medqa2019_e3.py --deepspeed deepspeed_zero2<br></code></pre></td></tr></table></figure>
<h4 id="3-3-3-pth-转-huggingface"><a href="#3-3-3-pth-转-huggingface" class="headerlink" title="3.3.3 pth 转 huggingface"></a>3.3.3 pth 转 huggingface</h4><p>同前述，这里不赘述了。<a href="#236-将得到的-pth-模型转换为-huggingface-模型即生成adapter文件夹">将得到的-pth-模型转换为-huggingface-模型即生成adapter文件夹</a>  </p>
<h4 id="3-3-4-部署与测试"><a href="#3-3-4-部署与测试" class="headerlink" title="3.3.4 部署与测试"></a>3.3.4 部署与测试</h4><p>同前述。<a href="#24-部署与测试">部署与测试</a></p>
<h2 id="4-用-MS-Agent-数据集-赋予-LLM-以-Agent-能力"><a href="#4-用-MS-Agent-数据集-赋予-LLM-以-Agent-能力" class="headerlink" title="4 用 MS-Agent 数据集 赋予 LLM 以 Agent 能力"></a>4 用 MS-Agent 数据集 赋予 LLM 以 Agent 能力</h2><h3 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h3><p>MSAgent 数据集每条样本包含一个对话列表（conversations），其里面包含了 system、user、assistant 三种字段。其中：</p>
<ul>
<li><p>system: 表示给模型前置的人设输入，其中有告诉模型如何调用插件以及生成请求</p>
</li>
<li><p>user: 表示用户的输入 prompt，分为两种，通用生成的prompt和调用插件需求的 prompt</p>
</li>
<li><p>assistant: 为模型的回复。其中会包括插件调用代码和执行代码，调用代码是要 LLM 生成的，而执行代码是调用服务来生成结果的</p>
</li>
</ul>
<p>一条调用网页搜索插件查询“上海明天天气”的数据样本示例如下图所示：<br><img  src="imgs/msagent_data.png"  ><span class="image-caption">BlgfEqpiRFO5G6L.png</span></p>
<h3 id="4-2-微调步骤"><a href="#4-2-微调步骤" class="headerlink" title="4.2 微调步骤"></a>4.2 微调步骤</h3><h4 id="4-2-1-准备工作"><a href="#4-2-1-准备工作" class="headerlink" title="4.2.1 准备工作"></a>4.2.1 准备工作</h4><blockquote>
<p>xtuner 是从国内的 ModelScope 平台下载 MS-Agent 数据集，因此不用提前手动下载数据集文件。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 准备工作</span><br><span class="hljs-built_in">mkdir</span> ~/ft-msagent &amp;&amp; <span class="hljs-built_in">cd</span> ~/ft-msagent<br><span class="hljs-built_in">cp</span> -r ~/ft-oasst1/internlm-chat-7b .<br><br><span class="hljs-comment"># 查看配置文件</span><br>xtuner list-cfg | grep msagent<br><br><span class="hljs-comment"># 复制配置文件到当前目录</span><br>xtuner copy-cfg internlm_7b_qlora_msagent_react_e3_gpu8 .<br><br><span class="hljs-comment"># 修改配置文件中的模型为本地路径</span><br>vim ./internlm_7b_qlora_msagent_react_e3_gpu8_copy.py <br></code></pre></td></tr></table></figure>
<figure class="highlight diff"><table><tr><td class="code"><pre><code class="hljs diff"><span class="hljs-deletion">- pretrained_model_name_or_path = &#x27;internlm/internlm-chat-7b&#x27;</span><br><span class="hljs-addition">+ pretrained_model_name_or_path = &#x27;./internlm-chat-7b&#x27;</span><br></code></pre></td></tr></table></figure>
<h4 id="4-2-2-开始微调"><a href="#4-2-2-开始微调" class="headerlink" title="4.2.2 开始微调"></a>4.2.2 开始微调</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash">xtuner train ./internlm_7b_qlora_msagent_react_e3_gpu8_copy.py --deepspeed deepspeed_zero2<br></code></pre></td></tr></table></figure>
<h3 id="4-3-直接使用"><a href="#4-3-直接使用" class="headerlink" title="4.3 直接使用"></a>4.3 直接使用</h3><blockquote>
<p>由于 msagent 的训练非常费时，大家如果想尽快把这个教程跟完，可以直接从 modelScope 拉取咱们已经微调好了的 Adapter。如下演示。</p>
</blockquote>
<h4 id="4-3-1-下载-Adapter"><a href="#4-3-1-下载-Adapter" class="headerlink" title="4.3.1 下载 Adapter"></a>4.3.1 下载 Adapter</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs Bash"><span class="hljs-built_in">cd</span> ~/ft-msagent<br>apt install git git-lfs<br>git lfs install<br>git lfs <span class="hljs-built_in">clone</span> https://www.modelscope.cn/xtuner/internlm-7b-qlora-msagent-react.git<br></code></pre></td></tr></table></figure>
<p>OK，现在目录应该长这样：</p>
<ul>
<li>internlm_7b_qlora_msagent_react_e3_gpu8_copy.py</li>
<li>internlm-7b-qlora-msagent-react</li>
<li>internlm-chat-7b</li>
<li>work_dir（可有可无）</li>
</ul>
<p>有了这个在 msagent 上训练得到的Adapter，模型现在已经有 agent 能力了！就可以加 —lagent 以调用来自 lagent 的代理功能了！</p>
<h4 id="4-3-2-添加-serper-环境变量"><a href="#4-3-2-添加-serper-环境变量" class="headerlink" title="4.3.2 添加 serper 环境变量"></a>4.3.2 添加 serper 环境变量</h4><blockquote>
<p><strong>开始 chat 之前，还要加个 serper 的环境变量：</strong></p>
<p>去 serper.dev 免费注册一个账号，生成自己的 api key。这个东西是用来给 lagent 去获取 google 搜索的结果的。等于是 serper.dev 帮你去访问 google，而不是从你自己本地去访问 google 了。</p>
</blockquote>
<p><img  src="imgs/serper.png"  ><span class="image-caption">kDSdpQrhHfTWYsc.png</span></p>
<p>添加 serper api key 到环境变量：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> SERPER_API_KEY=abcdefg<br></code></pre></td></tr></table></figure>
<h4 id="4-3-3-xtuner-agent，启动！"><a href="#4-3-3-xtuner-agent，启动！" class="headerlink" title="4.3.3 xtuner + agent，启动！"></a>4.3.3 xtuner + agent，启动！</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">xtuner chat ./internlm-chat-7b --adapter internlm-7b-qlora-msagent-react --lagent<br></code></pre></td></tr></table></figure>
<h2 id="5-注意事项"><a href="#5-注意事项" class="headerlink" title="5 注意事项"></a>5 注意事项</h2><p>本教程使用 xtuner 0.1.9 版本<br>若需要跟着本教程一步一步完成，建议严格遵循本教程的步骤！</p>
<p>若出现莫名其妙报错，请尝试更换为以下包的版本：（如果有报错再检查，没报错不用看）<br><figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">torch</span>                         <span class="hljs-number">2</span>.<span class="hljs-number">1</span>.<span class="hljs-number">1</span><br><span class="hljs-attribute">transformers</span>                  <span class="hljs-number">4</span>.<span class="hljs-number">34</span>.<span class="hljs-number">0</span><br><span class="hljs-attribute">transformers</span>-stream-generator <span class="hljs-number">0</span>.<span class="hljs-number">0</span>.<span class="hljs-number">4</span><br></code></pre></td></tr></table></figure><br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install torch==2.1.1<br>pip install transformers==4.34.0<br>pip install transformers-stream-generator=0.0.4<br></code></pre></td></tr></table></figure><br>CUDA 相关：（如果有报错再检查，没报错不用看）<br><figure class="highlight apache"><table><tr><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">NVIDIA</span>-SMI <span class="hljs-number">535</span>.<span class="hljs-number">54</span>.<span class="hljs-number">03</span>              <br><span class="hljs-attribute">Driver</span> Version: <span class="hljs-number">535</span>.<span class="hljs-number">54</span>.<span class="hljs-number">03</span>    <br><span class="hljs-attribute">CUDA</span> Version: <span class="hljs-number">12</span>.<span class="hljs-number">2</span><br><br><span class="hljs-attribute">nvidia</span>-cuda-cupti-cu12        <span class="hljs-number">12</span>.<span class="hljs-number">1</span>.<span class="hljs-number">105</span><br><span class="hljs-attribute">nvidia</span>-cuda-nvrtc-cu12        <span class="hljs-number">12</span>.<span class="hljs-number">1</span>.<span class="hljs-number">105</span><br><span class="hljs-attribute">nvidia</span>-cuda-runtime-cu12      <span class="hljs-number">12</span>.<span class="hljs-number">1</span>.<span class="hljs-number">105</span><br></code></pre></td></tr></table></figure></p>
<h2 id="6-作业"><a href="#6-作业" class="headerlink" title="6 作业"></a>6 作业</h2><h3 id="1-概述-1"><a href="#1-概述-1" class="headerlink" title="1 概述"></a>1 概述</h3><p>目标：通过微调，让模型成为我们的小助手</p>
<p>方式：使用 XTuner 进行微调</p>
<p><strong>微调前</strong><br><img  src="官方回答.png"  ><span class="image-caption">官方回答</span></p>
<p><strong>微调后</strong><br><img  src="微调后.png"  ><span class="image-caption">微调后.png</span></p>
<h3 id="2-实操"><a href="#2-实操" class="headerlink" title="2 实操"></a>2 实操</h3><h4 id="微调环境准备"><a href="#微调环境准备" class="headerlink" title="微调环境准备"></a>微调环境准备</h4><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># InternStudio 平台中，从本地 clone 一个已有 pytorch 2.0.1 的环境（后续均在该环境执行，若为其他环境可作为参考）</span><br><span class="hljs-comment"># 进入环境后首先 bash</span><br><span class="hljs-comment"># 进入环境后首先 bash</span><br><span class="hljs-comment"># 进入环境后首先 bash</span><br>bash<br>conda create --name personal_assistant --<span class="hljs-built_in">clone</span>=/root/share/conda_envs/internlm-base<br><span class="hljs-comment"># 如果在其他平台：</span><br><span class="hljs-comment"># conda create --name personal_assistant python=3.10 -y</span><br><br><span class="hljs-comment"># 激活环境</span><br>conda activate personal_assistant<br><span class="hljs-comment"># 进入家目录 （~的意思是 “当前用户的home路径”）</span><br><span class="hljs-built_in">cd</span> ~<br><span class="hljs-comment"># 创建版本文件夹并进入，以跟随本教程</span><br><span class="hljs-comment"># personal_assistant用于存放本教程所使用的东西</span><br><span class="hljs-built_in">mkdir</span> /root/personal_assistant &amp;&amp; <span class="hljs-built_in">cd</span> /root/personal_assistant<br><span class="hljs-built_in">mkdir</span> /root/personal_assistant/xtuner019 &amp;&amp; <span class="hljs-built_in">cd</span> /root/personal_assistant/xtuner019<br><br><span class="hljs-comment"># 拉取 0.1.9 的版本源码</span><br>git <span class="hljs-built_in">clone</span> -b v0.1.9  https://github.com/InternLM/xtuner<br><span class="hljs-comment"># 无法访问github的用户请从 gitee 拉取:</span><br><span class="hljs-comment"># git clone -b v0.1.9 https://gitee.com/Internlm/xtuner</span><br><br><span class="hljs-comment"># 进入源码目录</span><br><span class="hljs-built_in">cd</span> xtuner<br><br><span class="hljs-comment"># 从源码安装 XTuner</span><br>pip install -e <span class="hljs-string">&#x27;.[all]&#x27;</span><br></code></pre></td></tr></table></figure>
<h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>创建<code>data</code>文件夹用于存放用于训练的数据集</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p /root/personal_assistant/data &amp;&amp; <span class="hljs-built_in">cd</span> /root/personal_assistant/data<br></code></pre></td></tr></table></figure>
<p>在<code>data</code>目录下创建一个json文件<code>personal_assistant.json</code>作为本次微调所使用的数据集。json中内容可参考下方(复制粘贴n次做数据增广，数据量小无法有效微调，下面仅用于展示格式，下面也有生成脚本)</p>
<p>其中<code>conversation</code>表示一次对话的内容，<code>input</code>为输入，即用户会问的问题，<code>output</code>为输出，即想要模型回答的答案。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;conversation&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>            <span class="hljs-punctuation">&#123;</span><br>                <span class="hljs-attr">&quot;input&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;请介绍一下你自己&quot;</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-attr">&quot;output&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;我是不要葱姜蒜大佬的小助手，内在是上海AI实验室书生·浦语的7B大模型哦&quot;</span><br>            <span class="hljs-punctuation">&#125;</span><br>        <span class="hljs-punctuation">]</span><br>    <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;conversation&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>            <span class="hljs-punctuation">&#123;</span><br>                <span class="hljs-attr">&quot;input&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;请做一下自我介绍&quot;</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-attr">&quot;output&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;我是不要葱姜蒜大佬的小助手，内在是上海AI实验室书生·浦语的7B大模型哦&quot;</span><br>            <span class="hljs-punctuation">&#125;</span><br>        <span class="hljs-punctuation">]</span><br>    <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure>
<p>以下是一个python脚本，用于生成数据集。在<code>data</code>目录下新建一个generate_data.py文件，将以下代码复制进去，然后运行该脚本即可生成数据集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><br><span class="hljs-comment"># 输入你的名字</span><br>name = <span class="hljs-string">&#x27;Shengshenlan&#x27;</span><br><span class="hljs-comment"># 重复次数</span><br>n = <span class="hljs-number">10000</span><br><br>data = [<br>    &#123;<br>        <span class="hljs-string">&quot;conversation&quot;</span>: [<br>            &#123;<br>                <span class="hljs-string">&quot;input&quot;</span>: <span class="hljs-string">&quot;请做一下自我介绍&quot;</span>,<br>                <span class="hljs-string">&quot;output&quot;</span>: <span class="hljs-string">&quot;我是&#123;&#125;的小助手，内在是上海AI实验室书生·浦语的7B大模型哦&quot;</span>.<span class="hljs-built_in">format</span>(name)<br>            &#125;<br>        ]<br>    &#125;<br>]<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>    data.append(data[<span class="hljs-number">0</span>])<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;personal_assistant.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    json.dump(data, f, ensure_ascii=<span class="hljs-literal">False</span>, indent=<span class="hljs-number">4</span>)<br><br></code></pre></td></tr></table></figure>
<h4 id="配置准备"><a href="#配置准备" class="headerlink" title="配置准备"></a>配置准备</h4><p>下载模型<code>InternLM-chat-7B</code></p>
<p><a href="https://studio.intern-ai.org.cn/">InternStudio</a> 平台的 <code>share</code> 目录下已经为我们准备了全系列的 <code>InternLM</code> 模型，可以使用如下命令复制<code>internlm-chat-7b</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p /root/personal_assistant/model/Shanghai_AI_Laboratory<br><span class="hljs-built_in">cp</span> -r /root/share/temp/model_repos/internlm-chat-7b /root/personal_assistant/model/Shanghai_AI_Laboratory<br></code></pre></td></tr></table></figure>
<p>XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 列出所有内置配置</span><br>xtuner list-cfg<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#创建用于存放配置的文件夹config并进入</span><br><span class="hljs-built_in">mkdir</span> /root/personal_assistant/config &amp;&amp; <span class="hljs-built_in">cd</span> /root/personal_assistant/config<br></code></pre></td></tr></table></figure>
<p>拷贝一个配置文件到当前目录：<code>xtuner copy-cfg $&#123;CONFIG_NAME&#125; $&#123;SAVE_PATH&#125;</code><br>在本例中：（注意最后有个英文句号，代表复制到当前路径）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .<br></code></pre></td></tr></table></figure>
<p>修改拷贝后的文件internlm_chat_7b_qlora_oasst1_e3_copy.py，修改下述位置：<br>(这是一份修改好的文件<a href="./internlm_chat_7b_qlora_oasst1_e3_copy.py">internlm_chat_7b_qlora_oasst1_e3_copy.py</a>)<br><img  src="修改配置.png"  ><span class="image-caption">修改配置</span></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># PART 1 中</span><br><span class="hljs-comment"># 预训练模型存放的位置</span><br>pretrained_model_name_or_path = <span class="hljs-string">&#x27;/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b&#x27;</span><br><br><span class="hljs-comment"># 微调数据存放的位置</span><br>data_path = <span class="hljs-string">&#x27;/root/personal_assistant/data/personal_assistant.json&#x27;</span><br><br><span class="hljs-comment"># 训练中最大的文本长度</span><br>max_length = 512<br><br><span class="hljs-comment"># 每一批训练样本的大小</span><br>batch_size = 2<br><br><span class="hljs-comment"># 最大训练轮数</span><br>max_epochs = 3<br><br><span class="hljs-comment"># 验证的频率</span><br>evaluation_freq = 90<br><br><span class="hljs-comment"># 用于评估输出内容的问题（用于评估的问题尽量与数据集的question保持一致）</span><br>evaluation_inputs = [ <span class="hljs-string">&#x27;请介绍一下你自己&#x27;</span>, <span class="hljs-string">&#x27;请做一下自我介绍&#x27;</span> ]<br><br><br><span class="hljs-comment"># PART 3 中</span><br>dataset=dict(<span class="hljs-built_in">type</span>=load_dataset, path=<span class="hljs-string">&#x27;json&#x27;</span>, data_files=dict(train=data_path))<br>dataset_map_fn=None<br></code></pre></td></tr></table></figure>
<h4 id="微调启动"><a href="#微调启动" class="headerlink" title="微调启动"></a>微调启动</h4><p>用<code>xtuner train</code>命令启动训练、</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">xtuner train /root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py<br></code></pre></td></tr></table></figure>
<p><img  src="训练过程.png"  ><span class="image-caption">训练数据样例</span></p>
<blockquote>
<p>会在训练完成后，输出用于验证的Sample output</p>
<h4 id="微调后参数转换-合并"><a href="#微调后参数转换-合并" class="headerlink" title="微调后参数转换/合并"></a>微调后参数转换/合并</h4></blockquote>
<p>训练后的pth格式参数转Hugging Face格式</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建用于存放Hugging Face格式参数的hf文件夹</span><br><span class="hljs-built_in">mkdir</span> /root/personal_assistant/config/work_dirs/hf<br><br><span class="hljs-built_in">export</span> MKL_SERVICE_FORCE_INTEL=1<br><br><span class="hljs-comment"># 配置文件存放的位置</span><br><span class="hljs-built_in">export</span> CONFIG_NAME_OR_PATH=/root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py<br><br><span class="hljs-comment"># 模型训练后得到的pth格式参数存放的位置</span><br><span class="hljs-built_in">export</span> PTH=/root/personal_assistant/config/work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_3.pth<br><br><span class="hljs-comment"># pth文件转换为Hugging Face格式后参数存放的位置</span><br><span class="hljs-built_in">export</span> SAVE_PATH=/root/personal_assistant/config/work_dirs/hf<br><br><span class="hljs-comment"># 执行参数转换</span><br>xtuner convert pth_to_hf <span class="hljs-variable">$CONFIG_NAME_OR_PATH</span> <span class="hljs-variable">$PTH</span> <span class="hljs-variable">$SAVE_PATH</span><br></code></pre></td></tr></table></figure>
<p>Merge模型参数<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> MKL_SERVICE_FORCE_INTEL=1<br><span class="hljs-built_in">export</span> MKL_THREADING_LAYER=<span class="hljs-string">&#x27;GNU&#x27;</span><br><br><span class="hljs-comment"># 原始模型参数存放的位置</span><br><span class="hljs-built_in">export</span> NAME_OR_PATH_TO_LLM=/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b<br><br><span class="hljs-comment"># Hugging Face格式参数存放的位置</span><br><span class="hljs-built_in">export</span> NAME_OR_PATH_TO_ADAPTER=/root/personal_assistant/config/work_dirs/hf<br><br><span class="hljs-comment"># 最终Merge后的参数存放的位置</span><br><span class="hljs-built_in">mkdir</span> /root/personal_assistant/config/work_dirs/hf_merge<br><span class="hljs-built_in">export</span> SAVE_PATH=/root/personal_assistant/config/work_dirs/hf_merge<br><br><span class="hljs-comment"># 执行参数Merge</span><br>xtuner convert merge \<br>    <span class="hljs-variable">$NAME_OR_PATH_TO_LLM</span> \<br>    <span class="hljs-variable">$NAME_OR_PATH_TO_ADAPTER</span> \<br>    <span class="hljs-variable">$SAVE_PATH</span> \<br>    --max-shard-size 2GB<br></code></pre></td></tr></table></figure></p>
<h4 id="网页DEMO"><a href="#网页DEMO" class="headerlink" title="网页DEMO"></a>网页DEMO</h4><p>安装网页Demo所需依赖</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install streamlit==1.24.0<br></code></pre></td></tr></table></figure>
<p>下载 InternLM 项目代码</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建code文件夹用于存放InternLM项目代码</span><br><span class="hljs-built_in">mkdir</span> /root/personal_assistant/code &amp;&amp; <span class="hljs-built_in">cd</span> /root/personal_assistant/code<br>git <span class="hljs-built_in">clone</span> https://github.com/InternLM/InternLM.git<br></code></pre></td></tr></table></figure>
<p>将 <code>/root/code/InternLM/web_demo.py</code> 中 29 行和 33 行的模型路径更换为Merge后存放参数的路径 <code>/root/personal_assistant/config/work_dirs/hf_merge</code><br>运行 <code>/root/personal_assistant/code/InternLM</code> 目录下的 <code>web_demo.py</code> 文件，之后将端口映射到本地。在本地浏览器输入 <code>http://127.0.0.1:6006</code> 即可。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><code class="hljs stylus">streamlit run /root/personal_assistant/code/InternLM/web_demo<span class="hljs-selector-class">.py</span> <span class="hljs-attr">--server</span><span class="hljs-selector-class">.address</span> <span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span> <span class="hljs-attr">--server</span><span class="hljs-selector-class">.port</span> <span class="hljs-number">6006</span><br></code></pre></td></tr></table></figure>
<p>注意：要在浏览器打开 <code>http://127.0.0.1:6006</code> 页面后，模型才会加载。<br>在加载完模型之后，就可以与微调后的 InternLM-Chat-7B 进行对话了</p>
<h3 id="3-效果"><a href="#3-效果" class="headerlink" title="3 效果"></a>3 效果</h3><p>微调前<br><img  src="官方回答.png"  ><span class="image-caption">官方回答</span></p>
<p>微调后<br><img  src="微调后.png"  ><span class="image-caption">微调后.png</span></p>
<h2 id="7-进阶作业"><a href="#7-进阶作业" class="headerlink" title="7 进阶作业"></a>7 进阶作业</h2><h3 id="1-模型上传"><a href="#1-模型上传" class="headerlink" title="1 模型上传"></a>1 模型上传</h3><p><img  src="model-upload.png"  ><span class="image-caption">model-upload.png</span></p>
<h3 id="2-修改启动文件"><a href="#2-修改启动文件" class="headerlink" title="2 修改启动文件"></a>2 修改启动文件</h3><p>接下来需要修改启动文件以下载模型以及合并 lora 层，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> openxlab.model <span class="hljs-keyword">import</span> download<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Download</span>():<br>    download(model_repo=<span class="hljs-string">&#x27;OpenLMLab/InternLM-chat-7b&#x27;</span>,output=<span class="hljs-string">&#x27;/home/xlab-app-center/InternLM-chat-7b&#x27;</span>)<br>    download(model_repo=<span class="hljs-string">&#x27;EnableAsync/openxlab-assistant&#x27;</span>,output=<span class="hljs-string">&quot;/home/xlab-app-center/hf&quot;</span>)<br><br>Download()<br>os.system(<span class="hljs-string">&#x27;echo $PWD&#x27;</span>)<br>os.system(<span class="hljs-string">&#x27;ls&#x27;</span>)<br><br>os.system(<span class="hljs-string">&#x27;xtuner convert merge /home/xlab-app-center/InternLM-chat-7b /home/xlab-app-center/hf /home/xlab-app-center/hf-merge --max-shard-size 2GB&#x27;</span>)<br>os.system(<span class="hljs-string">&#x27;streamlit run /home/xlab-app-center/InternLM/web_demo.py --server.address=0.0.0.0 --server.port 7860&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="3-构建并运行"><a href="#3-构建并运行" class="headerlink" title="3 构建并运行"></a>3 构建并运行</h3><p><img  src="./build.png"  ><span class="image-caption">构建及运行</span></p>
<p>Github 地址如下：</p>
<p><a href="https://github.com/EnableAsync/openxlab-assistant">EnableAsync/openxlab-assistant (github.com)</a></p>
<p>运行地址如下：</p>
<p><a href="https://openxlab.org.cn/apps/detail/EnableAsync/openxlab-assistant">应用中心-OpenXLab-小卡的助手</a></p>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>Internlm-06-使用 OpenCompass 对大模型进行评测</title>
    <url>/internlm/internlm-06/</url>
    <content><![CDATA[<h1 id="使用-OpenCompass-对大模型进行评测"><a href="#使用-OpenCompass-对大模型进行评测" class="headerlink" title="使用 OpenCompass 对大模型进行评测"></a>使用 OpenCompass 对大模型进行评测</h1><h2 id="大模型评测概要"><a href="#大模型评测概要" class="headerlink" title="大模型评测概要"></a>大模型评测概要</h2><h3 id="人工智能技术的发展和主要模型的演变"><a href="#人工智能技术的发展和主要模型的演变" class="headerlink" title="人工智能技术的发展和主要模型的演变"></a>人工智能技术的发展和主要模型的演变</h3><ul>
<li><strong>OpenAI GPT系列：</strong><ul>
<li>2018年：发布第一代GPT模型，开启自然语言模型生成式预训练。</li>
<li>随后：发布GPT-2和GPT-3模型。</li>
</ul>
</li>
<li><strong>谷歌的预训练模型：</strong><ul>
<li>探索不同的大规模预训练模型，如T5, Flan等。</li>
</ul>
</li>
<li><strong>OpenAI的ChatGPT和GPT-4：</strong><ul>
<li>2022年11月：发布ChatGPT，展示问答、逻辑推理和内容创作能力。</li>
<li>2023年4月：发布GPT-4，引入多模态能力，拓展语言模型能力。</li>
</ul>
</li>
</ul>
<h3 id="大模型的国际竞争和应用"><a href="#大模型的国际竞争和应用" class="headerlink" title="大模型的国际竞争和应用"></a>大模型的国际竞争和应用</h3><ul>
<li><strong>OpenAI和微软的集成：</strong><ul>
<li>将ChatGPT和GPT-4集成进搜索引擎和Office办公套件，推出New Bing和Office Copilot。</li>
</ul>
</li>
<li><strong>谷歌的Bard：</strong><ul>
<li>基于PaLM和PaLM-2模型，与OpenAI和微软竞争。</li>
</ul>
</li>
<li><strong>中国企业和高校的发展：</strong><ul>
<li>百度、阿里、华为、商汤、讯飞等发布国产大模型。</li>
<li>清华、复旦等高校发布GLM, MOSS等模型。</li>
</ul>
</li>
</ul>
<h3 id="大模型评测的国际和国内进展"><a href="#大模型评测的国际和国内进展" class="headerlink" title="大模型评测的国际和国内进展"></a>大模型评测的国际和国内进展</h3><ul>
<li><strong>国际评测框架和数据集：</strong><ul>
<li>斯坦福大学的HELM评测框架。</li>
<li>纽约大学与谷歌、Meta的SuperGLUE评测集。</li>
<li>加州大学伯克利分校的MMLU测试集。</li>
<li>谷歌的Big-Bench评测集。</li>
</ul>
</li>
<li><strong>中国的评测数据集：</strong><ul>
<li>如CLUE, CUGE等，评测中文语言模型能力。</li>
</ul>
</li>
</ul>
<h3 id="面临的挑战和OpenCompass的提议"><a href="#面临的挑战和OpenCompass的提议" class="headerlink" title="面临的挑战和OpenCompass的提议"></a>面临的挑战和OpenCompass的提议</h3><ul>
<li><strong>当前挑战：</strong><ul>
<li>大模型应用场景广泛，但评测方案往往缺乏系统化。</li>
</ul>
</li>
<li><strong>OpenCompass的提议：</strong><ul>
<li>设计全面、高效、可拓展的评测方案。</li>
<li>提供分布式自动化评测系统，支持全面系统的能力评估。</li>
</ul>
</li>
</ul>
<h1 id="OpenCompass介绍"><a href="#OpenCompass介绍" class="headerlink" title="OpenCompass介绍"></a>OpenCompass介绍</h1><h2 id="评测对象"><a href="#评测对象" class="headerlink" title="评测对象"></a>评测对象</h2><p>本算法库的主要评测对象为语言大模型与多模态大模型。我们以语言大模型为例介绍评测的具体模型类型。</p>
<ul>
<li><p><strong>基座模型</strong>：一般是经过海量的文本数据以自监督学习的方式进行训练获得的模型（如OpenAI的GPT-3，Meta的LLaMA），往往具有强大的文字续写能力。</p>
</li>
<li><p><strong>对话模型</strong>：一般是在的基座模型的基础上，经过指令微调或人类偏好对齐获得的模型（如OpenAI的ChatGPT、上海人工智能实验室的书生·浦语），能理解人类指令，具有较强的对话能力。</p>
</li>
</ul>
<h2 id="工具架构"><a href="#工具架构" class="headerlink" title="工具架构"></a>工具架构</h2><p><img  src="工具架构.png"  ><span class="image-caption">工具架构</span></p>
<h3 id="大模型评测的层级结构"><a href="#大模型评测的层级结构" class="headerlink" title="大模型评测的层级结构"></a>大模型评测的层级结构</h3><ul>
<li><p>模型层</p>
<ul>
<li>重点评测对象：<ul>
<li>基座模型</li>
<li>对话模型</li>
</ul>
</li>
</ul>
</li>
<li><p>能力层</p>
<ul>
<li><p>通用能力：</p>
<ul>
<li>语言</li>
<li>知识</li>
<li>理解</li>
<li>推理</li>
<li>安全</li>
</ul>
</li>
<li><p>特色能力：</p>
<ul>
<li>长文本处理</li>
<li>编码能力</li>
<li>工具使用</li>
<li>知识增强</li>
</ul>
</li>
</ul>
</li>
<li><p>方法层</p>
<ul>
<li><p>客观评测：</p>
<ul>
<li>评估模型在确定答案任务（如选择题、填空、封闭式问答）上的能力。</li>
</ul>
</li>
<li><p>主观评测：</p>
<ul>
<li>评估用户对模型回复的真实满意度。</li>
<li>方法包括：<ul>
<li>基于模型辅助的主观评测</li>
<li>基于人类反馈的主观评测</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>工具层</p>
<ul>
<li>自动化评测支持：<ul>
<li>分布式评测技术</li>
<li>提示词工程</li>
<li>对接评测数据库</li>
<li>评测榜单发布</li>
<li>评测报告生成</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="评测方法"><a href="#评测方法" class="headerlink" title="评测方法"></a>评测方法</h2><h3 id="客观评测"><a href="#客观评测" class="headerlink" title="客观评测"></a>客观评测</h3><h4 id="评测客观问题的方法"><a href="#评测客观问题的方法" class="headerlink" title="评测客观问题的方法"></a>评测客观问题的方法</h4><ul>
<li><strong>定量比较：</strong><ul>
<li>使用定量指标比较模型输出与标准答案的差异。</li>
<li>根据差异结果衡量模型性能。</li>
</ul>
</li>
<li><p><strong>输入输出规范：</strong></p>
<ul>
<li>在评测阶段规范模型的输入和输出。</li>
<li>尽量减少噪声输出，以便更客观地评价模型能力。</li>
</ul>
<h4 id="模型能力的激发与引导"><a href="#模型能力的激发与引导" class="headerlink" title="模型能力的激发与引导"></a>模型能力的激发与引导</h4></li>
<li><p>提示词工程（Prompt Engineering）：</p>
<ul>
<li>使用特定提示词引导模型输出。</li>
</ul>
</li>
<li><p>语境学习（In-Context Learning）：</p>
<ul>
<li>利用上下文环境提升模型的输出质量。</li>
</ul>
<h4 id="客观评测的具体实践"><a href="#客观评测的具体实践" class="headerlink" title="客观评测的具体实践"></a>客观评测的具体实践</h4></li>
<li><p><strong>判别式评测：</strong></p>
<ul>
<li>结合问题和候选答案。</li>
<li>计算困惑度（perplexity），选择困惑度最小的答案。</li>
</ul>
</li>
<li><strong>生成式评测：</strong><ul>
<li>用于生成类任务（如语言翻译、程序生成、逻辑分析）。</li>
<li>使用问题作为输入，留白答案区域由模型补全。</li>
<li>对模型输出进行后处理，确保满足数据集要求。</li>
</ul>
</li>
</ul>
<h3 id="主观评测"><a href="#主观评测" class="headerlink" title="主观评测"></a>主观评测</h3><h4 id="主观评测的重要性"><a href="#主观评测的重要性" class="headerlink" title="主观评测的重要性"></a>主观评测的重要性</h4><ul>
<li>场景和能力多样性：<ul>
<li>语言表达丰富多变，很多场景和能力难以通过客观指标评测。</li>
</ul>
</li>
<li>模型安全和语言能力：<ul>
<li>需要依赖人的主观感受进行评测，以更真实地反映模型能力。</li>
</ul>
</li>
</ul>
<h4 id="OpenCompass的主观评测方案"><a href="#OpenCompass的主观评测方案" class="headerlink" title="OpenCompass的主观评测方案"></a>OpenCompass的主观评测方案</h4><ul>
<li>评测实施：<ul>
<li>使用受试者的主观判断对大语言模型进行评测。</li>
<li>构建主观测试问题集，对比不同模型的回复。</li>
</ul>
</li>
<li>成本与效率：<ul>
<li>高成本的人类主观评测。</li>
<li>结合使用性能优异的大语言模型进行主观打分。</li>
</ul>
</li>
</ul>
<h4 id="主观评测的具体实践"><a href="#主观评测的具体实践" class="headerlink" title="主观评测的具体实践"></a>主观评测的具体实践</h4><ul>
<li>单模型回复满意度统计：<ul>
<li>对单一模型的回复进行满意度评分。</li>
</ul>
</li>
<li>多模型满意度比较：<ul>
<li>比较不同模型回复的满意度。</li>
</ul>
</li>
</ul>
<h1 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h1><p><img  src="opencompass流程.png"  ><span class="image-caption">opencompass 评判流程</span></p>
<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>在 OpenCompass 中评估一个模型通常包括以下几个阶段：<strong>配置</strong> -&gt; <strong>推理</strong> -&gt; <strong>评估</strong> -&gt; <strong>可视化</strong>。</p>
<p><strong>配置</strong>：这是整个工作流的起点。您需要配置整个评估过程，选择要评估的模型和数据集。此外，还可以选择评估策略、计算后端等，并定义显示结果的方式。</p>
<p><strong>推理与评估</strong>：在这个阶段，OpenCompass 将会开始对模型和数据集进行并行推理和评估。<strong>推理</strong>阶段主要是让模型从数据集产生输出，而<strong>评估</strong>阶段则是衡量这些输出与标准答案的匹配程度。这两个过程会被拆分为多个同时运行的“任务”以提高效率，但请注意，如果计算资源有限，这种策略可能会使评测变得更慢。</p>
<p><strong>可视化</strong>：评估完成后，OpenCompass 将结果整理成易读的表格，并将其保存为 CSV 和 TXT 文件。你也可以激活飞书状态上报功能，此后可以在飞书客户端中及时获得评测状态报告。</p>
<p>接下来，我们将展示 OpenCompass 的基础用法，展示书生浦语在 <a href="https://cevalbenchmark.com/index.html#home">C-Eval</a> 基准任务上的评估。它们的配置文件可以在 <a href="https://github.com/open-compass/opencompass/blob/main/configs/eval_demo.py">configs/eval_demo.py</a> 中找到。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="面向GPU的环境安装"><a href="#面向GPU的环境安装" class="headerlink" title="面向GPU的环境安装"></a>面向GPU的环境安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">conda create --name opencompass --<span class="hljs-built_in">clone</span>=/root/share/conda_envs/internlm-base<br>conda activate opencompass<br>git <span class="hljs-built_in">clone</span> https://github.com/open-compass/opencompass<br><span class="hljs-built_in">cd</span> opencompass<br>pip install -e .<br></code></pre></td></tr></table></figure>
<p>有部分第三方功能,如代码能力基准测试 Humaneval 以及 Llama格式的模型评测,可能需要额外步骤才能正常运行，如需评测，详细步骤请参考<a href="https://opencompass.readthedocs.io/zh_CN/latest/get_started/installation.html">安装指南</a>。</p>
<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 解压评测数据集到 data/ 处</span><br><span class="hljs-built_in">cp</span> /share/temp/datasets/OpenCompassData-core-20231110.zip /root/opencompass/<br>unzip OpenCompassData-core-20231110.zip<br><br><span class="hljs-comment"># 将会在opencompass下看到data文件夹</span><br></code></pre></td></tr></table></figure>
<h3 id="查看支持的数据集和模型"><a href="#查看支持的数据集和模型" class="headerlink" title="查看支持的数据集和模型"></a>查看支持的数据集和模型</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 列出所有跟 internlm 及 ceval 相关的配置</span><br>python tools/list_configs.py internlm ceval<br></code></pre></td></tr></table></figure>
<p>将会看到</p>
<figure class="highlight text"><table><tr><td class="code"><pre><code class="hljs text">+--------------------------+--------------------------------------------------------+<br>| Model                    | Config Path                                            |<br>|--------------------------+--------------------------------------------------------|<br>| hf_internlm_20b          | configs/models/hf_internlm/hf_internlm_20b.py          |<br>| hf_internlm_7b           | configs/models/hf_internlm/hf_internlm_7b.py           |<br>| hf_internlm_chat_20b     | configs/models/hf_internlm/hf_internlm_chat_20b.py     |<br>| hf_internlm_chat_7b      | configs/models/hf_internlm/hf_internlm_chat_7b.py      |<br>| hf_internlm_chat_7b_8k   | configs/models/hf_internlm/hf_internlm_chat_7b_8k.py   |<br>| hf_internlm_chat_7b_v1_1 | configs/models/hf_internlm/hf_internlm_chat_7b_v1_1.py |<br>| internlm_7b              | configs/models/internlm/internlm_7b.py                 |<br>| ms_internlm_chat_7b_8k   | configs/models/ms_internlm/ms_internlm_chat_7b_8k.py   |<br>+--------------------------+--------------------------------------------------------+<br>+----------------------------+------------------------------------------------------+<br>| Dataset                    | Config Path                                          |<br>|----------------------------+------------------------------------------------------|<br>| ceval_clean_ppl            | configs/datasets/ceval/ceval_clean_ppl.py            |<br>| ceval_gen                  | configs/datasets/ceval/ceval_gen.py                  |<br>| ceval_gen_2daf24           | configs/datasets/ceval/ceval_gen_2daf24.py           |<br>| ceval_gen_5f30c7           | configs/datasets/ceval/ceval_gen_5f30c7.py           |<br>| ceval_ppl                  | configs/datasets/ceval/ceval_ppl.py                  |<br>| ceval_ppl_578f8d           | configs/datasets/ceval/ceval_ppl_578f8d.py           |<br>| ceval_ppl_93e5ce           | configs/datasets/ceval/ceval_ppl_93e5ce.py           |<br>| ceval_zero_shot_gen_bd40ef | configs/datasets/ceval/ceval_zero_shot_gen_bd40ef.py |<br>+----------------------------+------------------------------------------------------+<br></code></pre></td></tr></table></figure>
<h3 id="启动评测"><a href="#启动评测" class="headerlink" title="启动评测"></a>启动评测</h3><p>确保按照上述步骤正确安装 OpenCompass 并准备好数据集后，可以通过以下命令评测 InternLM-Chat-7B 模型在 C-Eval 数据集上的性能。由于 OpenCompass 默认并行启动评估过程，我们可以在第一次运行时以 <code>--debug</code> 模式启动评估，并检查是否存在问题。在 <code>--debug</code> 模式下，任务将按顺序执行，并实时打印输出。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py --datasets ceval_gen --hf-path /share/temp/model_repos/internlm-chat-7b/ --tokenizer-path /share/temp/model_repos/internlm-chat-7b/ --tokenizer-kwargs padding_side=<span class="hljs-string">&#x27;left&#x27;</span> truncation=<span class="hljs-string">&#x27;left&#x27;</span> trust_remote_code=True --model-kwargs trust_remote_code=True device_map=<span class="hljs-string">&#x27;auto&#x27;</span> --max-seq-len 2048 --max-out-len 16 --batch-size 4 --num-gpus 1 --debug<br></code></pre></td></tr></table></figure>
<p>命令解析<br><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">--datasets ceval_gen \<br>--hf-path /share/temp/model_repos/internlm-chat-7b/ \  <span class="hljs-comment"># HuggingFace 模型路径</span><br>--tokenizer-path /share/temp/model_repos/internlm-chat-7b/ \  <span class="hljs-comment"># HuggingFace tokenizer 路径（如果与模型路径相同，可以省略）</span><br>--tokenizer-kwargs padding_side=<span class="hljs-string">&#x27;left&#x27;</span> truncation=<span class="hljs-string">&#x27;left&#x27;</span> trust_remote_code=True \  <span class="hljs-comment"># 构建 tokenizer 的参数</span><br>--model-kwargs device_map=<span class="hljs-string">&#x27;auto&#x27;</span> trust_remote_code=True \  <span class="hljs-comment"># 构建模型的参数</span><br>--max-seq-len 2048 \  <span class="hljs-comment"># 模型可以接受的最大序列长度</span><br>--max-out-len 16 \  <span class="hljs-comment"># 生成的最大 token 数</span><br>--batch-size 2  \  <span class="hljs-comment"># 批量大小</span><br>--num-gpus 1  <span class="hljs-comment"># 运行模型所需的 GPU 数量</span><br>--debug<br></code></pre></td></tr></table></figure></p>
<p>如果一切正常，您应该看到屏幕上显示 “Starting inference process”：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">[2024-01-12 18:23:55,076] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [INFO] Starting inference process...<br></code></pre></td></tr></table></figure>
<p>评测完成后，将会看到：<br><figure class="highlight maxima"><table><tr><td class="code"><pre><code class="hljs maxima"><br>dataset                                         version    metric         mode      opencompass.models.huggingface.HuggingFace_model_repos_internlm-chat-7b<br>----------------------------------------------  ---------  -------------  ------  -------------------------------------------------------------------------<br>ceval-computer_network                          db9ce2     accuracy       gen                                                                         <span class="hljs-number">31.58</span><br>ceval-operating_system                          1c2571     accuracy       gen                                                                         <span class="hljs-number">36.84</span><br>ceval-computer_architecture                     a74dad     accuracy       gen                                                                         <span class="hljs-number">28.57</span><br>ceval-college_programming                       4ca32a     accuracy       gen                                                                         <span class="hljs-number">32.43</span><br>ceval-college_physics                           963fa8     accuracy       gen                                                                         <span class="hljs-number">26.32</span><br>ceval-college_chemistry                         e78857     accuracy       gen                                                                         <span class="hljs-number">16.67</span><br>ceval-advanced_mathematics                      ce03e2     accuracy       gen                                                                         <span class="hljs-number">21.05</span><br>ceval-probability_and_statistics                <span class="hljs-number">65e812</span>     accuracy       gen                                                                         <span class="hljs-number">38.89</span><br>ceval-discrete_mathematics                      e894ae     accuracy       gen                                                                         <span class="hljs-number">18.75</span><br>ceval-electrical_engineer                       ae42b9     accuracy       gen                                                                         <span class="hljs-number">35.14</span><br>ceval-metrology_engineer                        ee34ea     accuracy       gen                                                                         <span class="hljs-number">50</span><br>ceval-high_school_mathematics                   1dc5bf     accuracy       gen                                                                         <span class="hljs-number">22.22</span><br>ceval-high_school_physics                       adf25f     accuracy       gen                                                                         <span class="hljs-number">31.58</span><br>ceval-high_school_chemistry                     2ed27f     accuracy       gen                                                                         <span class="hljs-number">15.79</span><br>ceval-high_school_biology                       8e2b9a     accuracy       gen                                                                         <span class="hljs-number">36.84</span><br>ceval-middle_school_mathematics                 bee8d5     accuracy       gen                                                                         <span class="hljs-number">26.32</span><br>ceval-middle_school_biology                     86817c     accuracy       gen                                                                         <span class="hljs-number">61.9</span><br>ceval-middle_school_physics                     8accf6     accuracy       gen                                                                         <span class="hljs-number">63.16</span><br>ceval-middle_school_chemistry                   167a15     accuracy       gen                                                                         <span class="hljs-number">60</span><br>ceval-veterinary_medicine                       b4e08d     accuracy       gen                                                                         <span class="hljs-number">47.83</span><br>ceval-college_economics                         f3f4e6     accuracy       gen                                                                         <span class="hljs-number">41.82</span><br>ceval-business_administration                   c1614e     accuracy       gen                                                                         <span class="hljs-number">33.33</span><br>ceval-marxism                                   cf874c     accuracy       gen                                                                         <span class="hljs-number">68.42</span><br>ceval-mao_zedong_thought                        51c7a4     accuracy       gen                                                                         <span class="hljs-number">70.83</span><br>ceval-education_science                         591fee     accuracy       gen                                                                         <span class="hljs-number">58.62</span><br>ceval-teacher_qualification                     4e4ced     accuracy       gen                                                                         <span class="hljs-number">70.45</span><br>ceval-high_school_politics                      5c0de2     accuracy       gen                                                                         <span class="hljs-number">26.32</span><br>ceval-high_school_geography                     <span class="hljs-number">865461</span>     accuracy       gen                                                                         <span class="hljs-number">47.37</span><br>ceval-middle_school_politics                    5be3e7     accuracy       gen                                                                         <span class="hljs-number">52.38</span><br>ceval-middle_school_geography                   8a63be     accuracy       gen                                                                         <span class="hljs-number">58.33</span><br>ceval-modern_chinese_history                    fc01af     accuracy       gen                                                                         <span class="hljs-number">73.91</span><br>ceval-ideological_and_moral_cultivation         a2aa4a     accuracy       gen                                                                         <span class="hljs-number">63.16</span><br>ceval-logic                                     f5b022     accuracy       gen                                                                         <span class="hljs-number">31.82</span><br>ceval-law                                       a110a1     accuracy       gen                                                                         <span class="hljs-number">25</span><br>ceval-chinese_language_and_literature           <span class="hljs-number">0f8b68</span>     accuracy       gen                                                                         <span class="hljs-number">30.43</span><br>ceval-art_studies                               2a1300     accuracy       gen                                                                         <span class="hljs-number">60.61</span><br>ceval-professional_tour_guide                   4e673e     accuracy       gen                                                                         <span class="hljs-number">62.07</span><br>ceval-legal_professional                        ce8787     accuracy       gen                                                                         <span class="hljs-number">39.13</span><br>ceval-high_school_chinese                       <span class="hljs-number">315705</span>     accuracy       gen                                                                         <span class="hljs-number">63.16</span><br>ceval-high_school_history                       7eb30a     accuracy       gen                                                                         <span class="hljs-number">70</span><br>ceval-middle_school_history                     48ab4a     accuracy       gen                                                                         <span class="hljs-number">59.09</span><br>ceval-civil_servant                             87d061     accuracy       gen                                                                         <span class="hljs-number">53.19</span><br>ceval-sports_science                            70f27b     accuracy       gen                                                                         <span class="hljs-number">52.63</span><br>ceval-plant_protection                          8941f9     accuracy       gen                                                                         <span class="hljs-number">59.09</span><br>ceval-basic_medicine                            c409d6     accuracy       gen                                                                         <span class="hljs-number">47.37</span><br>ceval-clinical_medicine                         49e82d     accuracy       gen                                                                         <span class="hljs-number">40.91</span><br>ceval-urban_and_rural_planner                   <span class="hljs-number">95b885</span>     accuracy       gen                                                                         <span class="hljs-number">45.65</span><br>ceval-accountant                                <span class="hljs-number">002837</span>     accuracy       gen                                                                         <span class="hljs-number">26.53</span><br>ceval-fire_engineer                             bc23f5     accuracy       gen                                                                         <span class="hljs-number">22.58</span><br>ceval-environmental_impact_assessment_engineer  c64e2d     accuracy       gen                                                                         <span class="hljs-number">64.52</span><br>ceval-tax_accountant                            3a5e3c     accuracy       gen                                                                         <span class="hljs-number">34.69</span><br>ceval-physician                                 6e277d     accuracy       gen                                                                         <span class="hljs-number">40.82</span><br>ceval-stem                                      -          naive_average  gen                                                                         <span class="hljs-number">35.09</span><br>ceval-social-science                            -          naive_average  gen                                                                         <span class="hljs-number">52.79</span><br>ceval-humanities                                -          naive_average  gen                                                                         <span class="hljs-number">52.58</span><br>ceval-other                                     -          naive_average  gen                                                                         <span class="hljs-number">44.36</span><br>ceval-hard                                      -          naive_average  gen                                                                         <span class="hljs-number">23.91</span><br>ceval                                           -          naive_average  gen                                                                         <span class="hljs-number">44.16</span><br></code></pre></td></tr></table></figure></p>
<p>有关 <code>run.py</code> 支持的所有与 HuggingFace 相关的参数，请阅读 <a href="https://opencompass.readthedocs.io/zh-cn/latest/user_guides/experimentation.html#id2">评测任务发起</a></p>
<p>除了通过命令行配置实验外，OpenCompass 还允许用户在配置文件中编写实验的完整配置，并通过 <code>run.py</code> 直接运行它。配置文件是以 Python 格式组织的，并且必须包括 <code>datasets</code> 和 <code>models</code> 字段。</p>
<p>示例测试配置在 <a href="https://github.com/open-compass/opencompass/blob/main/configs/eval_demo.py">configs/eval_demo.py</a> 中。此配置通过 <a href="../user_guides/config.md#继承机制">继承机制</a> 引入所需的数据集和模型配置，并以所需格式组合 <code>datasets</code> 和 <code>models</code> 字段。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mmengine.config <span class="hljs-keyword">import</span> read_base<br><br><span class="hljs-keyword">with</span> read_base():<br>    <span class="hljs-keyword">from</span> .datasets.siqa.siqa_gen <span class="hljs-keyword">import</span> siqa_datasets<br>    <span class="hljs-keyword">from</span> .datasets.winograd.winograd_ppl <span class="hljs-keyword">import</span> winograd_datasets<br>    <span class="hljs-keyword">from</span> .models.opt.hf_opt_125m <span class="hljs-keyword">import</span> opt125m<br>    <span class="hljs-keyword">from</span> .models.opt.hf_opt_350m <span class="hljs-keyword">import</span> opt350m<br><br>datasets = [*siqa_datasets, *winograd_datasets]<br>models = [opt125m, opt350m]<br></code></pre></td></tr></table></figure>
<p>运行任务时，我们只需将配置文件的路径传递给 <code>run.py</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py configs/eval_demo.py<br></code></pre></td></tr></table></figure>
<p>OpenCompass 提供了一系列预定义的模型配置，位于 <code>configs/models</code> 下。以下是与 <a href="https://github.com/open-compass/opencompass/blob/main/configs/models/opt/hf_opt_350m.py">opt-350m</a>（<code>configs/models/opt/hf_opt_350m.py</code>）相关的配置片段：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用 `HuggingFaceCausalLM` 评估由 HuggingFace 的 `AutoModelForCausalLM` 支持的模型</span><br><span class="hljs-keyword">from</span> opencompass.models <span class="hljs-keyword">import</span> HuggingFaceCausalLM<br><br><span class="hljs-comment"># OPT-350M</span><br>opt350m = <span class="hljs-built_in">dict</span>(<br>       <span class="hljs-built_in">type</span>=HuggingFaceCausalLM,<br>       <span class="hljs-comment"># `HuggingFaceCausalLM` 的初始化参数</span><br>       path=<span class="hljs-string">&#x27;facebook/opt-350m&#x27;</span>,<br>       tokenizer_path=<span class="hljs-string">&#x27;facebook/opt-350m&#x27;</span>,<br>       tokenizer_kwargs=<span class="hljs-built_in">dict</span>(<br>           padding_side=<span class="hljs-string">&#x27;left&#x27;</span>,<br>           truncation_side=<span class="hljs-string">&#x27;left&#x27;</span>,<br>           proxies=<span class="hljs-literal">None</span>,<br>           trust_remote_code=<span class="hljs-literal">True</span>),<br>       model_kwargs=<span class="hljs-built_in">dict</span>(device_map=<span class="hljs-string">&#x27;auto&#x27;</span>),<br>       <span class="hljs-comment"># 下面是所有模型的共同参数，不特定于 HuggingFaceCausalLM</span><br>       abbr=<span class="hljs-string">&#x27;opt350m&#x27;</span>,               <span class="hljs-comment"># 结果显示的模型缩写</span><br>       max_seq_len=<span class="hljs-number">2048</span>,             <span class="hljs-comment"># 整个序列的最大长度</span><br>       max_out_len=<span class="hljs-number">100</span>,              <span class="hljs-comment"># 生成的最大 token 数</span><br>       batch_size=<span class="hljs-number">64</span>,                <span class="hljs-comment"># 批量大小</span><br>       run_cfg=<span class="hljs-built_in">dict</span>(num_gpus=<span class="hljs-number">1</span>),     <span class="hljs-comment"># 该模型所需的 GPU 数量</span><br>    )<br></code></pre></td></tr></table></figure>
<p>使用配置时，我们可以通过命令行参数 <code>--models</code> 指定相关文件，或使用继承机制将模型配置导入到配置文件中的 <code>models</code> 列表中。</p>
<p>与模型类似，数据集的配置文件也提供在 <code>configs/datasets</code> 下。用户可以在命令行中使用 <code>--datasets</code>，或通过继承在配置文件中导入相关配置</p>
<p>下面是来自 <code>configs/eval_demo.py</code> 的与数据集相关的配置片段：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mmengine.config <span class="hljs-keyword">import</span> read_base  <span class="hljs-comment"># 使用 mmengine.read_base() 读取基本配置</span><br><br><span class="hljs-keyword">with</span> read_base():<br>    <span class="hljs-comment"># 直接从预设的数据集配置中读取所需的数据集配置</span><br>    <span class="hljs-keyword">from</span> .datasets.winograd.winograd_ppl <span class="hljs-keyword">import</span> winograd_datasets  <span class="hljs-comment"># 读取 Winograd 配置，基于 PPL（困惑度）进行评估</span><br>    <span class="hljs-keyword">from</span> .datasets.siqa.siqa_gen <span class="hljs-keyword">import</span> siqa_datasets  <span class="hljs-comment"># 读取 SIQA 配置，基于生成进行评估</span><br><br>datasets = [*siqa_datasets, *winograd_datasets]       <span class="hljs-comment"># 最终的配置需要包含所需的评估数据集列表 &#x27;datasets&#x27;</span><br></code></pre></td></tr></table></figure>
<p>数据集配置通常有两种类型：’ppl’ 和 ‘gen’，分别指示使用的评估方法。其中 <code>ppl</code> 表示辨别性评估，<code>gen</code> 表示生成性评估。</p>
<p>此外，<a href="https://github.com/open-compass/opencompass/blob/main/configs/datasets/collections">configs/datasets/collections</a> 收录了各种数据集集合，方便进行综合评估。OpenCompass 通常使用 <a href="https://github.com/open-compass/opencompass/blob/main/configs/datasets/collections/base_medium.py"><code>base_medium.py</code></a> 进行全面的模型测试。要复制结果，只需导入该文件，例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py --models hf_llama_7b --datasets base_medium<br></code></pre></td></tr></table></figure>
<p>OpenCompass 通常假定运行环境网络是可用的。如果您遇到网络问题或希望在离线环境中运行 OpenCompass，请参阅 <a href="https://opencompass.readthedocs.io/zh-cn/latest/get_started/faq.html">FAQ - 网络 - Q1</a> 寻求解决方案。</p>
<h2 id="可视化评估结果"><a href="#可视化评估结果" class="headerlink" title="可视化评估结果"></a>可视化评估结果</h2><p>评估完成后，评估结果表格将打印如下：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><code class="hljs text">dataset    version    metric    mode      opt350m    opt125m<br>---------  ---------  --------  ------  ---------  ---------<br>siqa       e78df3     accuracy  gen         21.55      12.44<br>winograd   b6c7ed     accuracy  ppl         51.23      49.82<br></code></pre></td></tr></table></figure>
<p>所有运行输出将定向到 <code>outputs/demo/</code> 目录，结构如下：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><code class="hljs text">outputs/default/<br>├── 20200220_120000<br>├── 20230220_183030     # 每个实验一个文件夹<br>│   ├── configs         # 用于记录的已转储的配置文件。如果在同一个实验文件夹中重新运行了不同的实验，可能会保留多个配置<br>│   ├── logs            # 推理和评估阶段的日志文件<br>│   │   ├── eval<br>│   │   └── infer<br>│   ├── predictions   # 每个任务的推理结果<br>│   ├── results       # 每个任务的评估结果<br>│   └── summary       # 单个实验的汇总评估结果<br>├── ...<br></code></pre></td></tr></table></figure>
<p>打印评测结果的过程可被进一步定制化，用于输出一些数据集的平均分 (例如 MMLU, C-Eval 等)。</p>
<p>关于评测结果输出的更多介绍可阅读 <a href="../user_guides/summarizer.md">结果展示</a>。</p>
<h2 id="更多教程"><a href="#更多教程" class="headerlink" title="更多教程"></a>更多教程</h2><p>想要更多了解 OpenCompass, 可以点击下列链接学习。</p>
<ul>
<li><a href="https://opencompass.readthedocs.io/zh-cn/latest/">https://opencompass.readthedocs.io/zh-cn/latest/</a></li>
</ul>
<h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><h3 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建环境</span><br>conda create --name opencompass --<span class="hljs-built_in">clone</span>=/root/share/conda_envs/internlm-base<br>conda activate opencompass<br><span class="hljs-comment"># 使用镜像 clone</span><br>git <span class="hljs-built_in">clone</span> https://mirror.ghproxy.com/https://github.com/open-compass/opencompass<br><span class="hljs-built_in">cd</span> opencompass<br>pip install -e .<br></code></pre></td></tr></table></figure>
<h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cp</span> /share/temp/datasets/OpenCompassData-core-20231110.zip /root/opencompass/<br><span class="hljs-built_in">cd</span> /root/opencompass/<br>unzip OpenCompassData-core-20231110.zip<br></code></pre></td></tr></table></figure>
<p><img  src="unzip.png"  ><span class="image-caption">解压数据</span></p>
<h3 id="查看支持的数据集和模型-1"><a href="#查看支持的数据集和模型-1" class="headerlink" title="查看支持的数据集和模型"></a>查看支持的数据集和模型</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python tools/list_configs.py internlm ceval<br></code></pre></td></tr></table></figure>
<p><img  src="list.png"  ><span class="image-caption">列出所有跟 internlm 及 ceval 相关的配置</span></p>
<h3 id="启动评测-1"><a href="#启动评测-1" class="headerlink" title="启动评测"></a>启动评测</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py \<br>--datasets ceval_gen \<br>--hf-path /share/model_repos/internlm2-chat-7b/ \  <span class="hljs-comment"># HuggingFace 模型路径</span><br>--tokenizer-path /share/model_repos/internlm2-chat-7b/ \  <span class="hljs-comment"># 注意这里是 internlm2</span><br>--tokenizer-kwargs padding_side=<span class="hljs-string">&#x27;left&#x27;</span> truncation=<span class="hljs-string">&#x27;left&#x27;</span> trust_remote_code=True \  <span class="hljs-comment"># 构建 tokenizer 的参数</span><br>--model-kwargs device_map=<span class="hljs-string">&#x27;auto&#x27;</span> trust_remote_code=True \  <span class="hljs-comment"># 构建模型的参数</span><br>--max-seq-len 2048 \  <span class="hljs-comment"># 模型可以接受的最大序列长度</span><br>--max-out-len 16 \  <span class="hljs-comment"># 生成的最大 token 数</span><br>--batch-size 2  \  <span class="hljs-comment"># 批量大小</span><br>--num-gpus 1 \ <span class="hljs-comment"># 运行模型所需的 GPU 数量</span><br>--debug<br></code></pre></td></tr></table></figure>
<p>便于复制版：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py \<br>--datasets ceval_gen \<br>--hf-path /share/model_repos/internlm2-chat-7b/ \<br>--tokenizer-path /share/model_repos/internlm2-chat-7b/ \<br>--tokenizer-kwargs padding_side=<span class="hljs-string">&#x27;left&#x27;</span> truncation=<span class="hljs-string">&#x27;left&#x27;</span> trust_remote_code=True \<br>--model-kwargs device_map=<span class="hljs-string">&#x27;auto&#x27;</span> trust_remote_code=True \<br>--max-seq-len 2048 \<br>--max-out-len 16 \<br>--batch-size 2  \<br>--num-gpus 1 \<br>--debug<br></code></pre></td></tr></table></figure>
<p>发现显存不够用，尝试改小 batch size 为 1。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py \<br>--datasets ceval_gen \<br>--hf-path /share/model_repos/internlm2-chat-7b/ \<br>--tokenizer-path /share/model_repos/internlm2-chat-7b/ \<br>--tokenizer-kwargs padding_side=<span class="hljs-string">&#x27;left&#x27;</span> truncation=<span class="hljs-string">&#x27;left&#x27;</span> trust_remote_code=True \<br>--model-kwargs device_map=<span class="hljs-string">&#x27;auto&#x27;</span> trust_remote_code=True \<br>--max-seq-len 2048 \<br>--max-out-len 16 \<br>--batch-size 1  \<br>--num-gpus 1 \<br>--debug<br></code></pre></td></tr></table></figure>
<p><img  src="run.png"  ><span class="image-caption">运行截图</span></p>
<p><img  src="result.png"  ><span class="image-caption">评测结果</span></p>
<h2 id="进阶作业"><a href="#进阶作业" class="headerlink" title="进阶作业"></a>进阶作业</h2><p>安装 lmdeploy，这一步是必须的，否则无法加载 TurboMind 模型</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install lmdeploy==0.2.0<br></code></pre></td></tr></table></figure>
<p>编写 config 文件如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mmengine.config <span class="hljs-keyword">import</span> read_base<br><span class="hljs-keyword">from</span> opencompass.models.turbomind <span class="hljs-keyword">import</span> TurboMindModel<br><br><span class="hljs-keyword">with</span> read_base():<br>    <span class="hljs-comment"># choose a list of datasets</span><br>    <span class="hljs-keyword">from</span> .datasets.ceval.ceval_gen_5f30c7 <span class="hljs-keyword">import</span> ceval_datasets<br><br><br>datasets = [*ceval_datasets]<br><br>internlm_meta_template = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">round</span>=[<br>    <span class="hljs-built_in">dict</span>(role=<span class="hljs-string">&#x27;HUMAN&#x27;</span>, begin=<span class="hljs-string">&#x27;&lt;|User|&gt;:&#x27;</span>, end=<span class="hljs-string">&#x27;\n&#x27;</span>),<br>    <span class="hljs-built_in">dict</span>(role=<span class="hljs-string">&#x27;BOT&#x27;</span>, begin=<span class="hljs-string">&#x27;&lt;|Bot|&gt;:&#x27;</span>, end=<span class="hljs-string">&#x27;&lt;eoa&gt;\n&#x27;</span>, generate=<span class="hljs-literal">True</span>),<br>],<br>                              eos_token_id=<span class="hljs-number">103028</span>)<br><br><span class="hljs-comment"># config for internlm-chat-7b</span><br>internlm_chat_7b = <span class="hljs-built_in">dict</span>(<br>    <span class="hljs-built_in">type</span>=TurboMindModel,<br>    abbr=<span class="hljs-string">&#x27;internlm-chat-7b&#x27;</span>,<br>    path=<span class="hljs-string">&#x27;/root/workspace_quant_awq4&#x27;</span>, <span class="hljs-comment"># 这里的 path 是上一节课中的 awq 模型</span><br>    engine_config=<span class="hljs-built_in">dict</span>(session_len=<span class="hljs-number">2048</span>,<br>                       max_batch_size=<span class="hljs-number">32</span>,<br>                       rope_scaling_factor=<span class="hljs-number">1.0</span>),<br>    gen_config=<span class="hljs-built_in">dict</span>(top_k=<span class="hljs-number">1</span>,<br>                    top_p=<span class="hljs-number">0.8</span>,<br>                    temperature=<span class="hljs-number">1.0</span>,<br>                    max_new_tokens=<span class="hljs-number">100</span>),<br>    max_out_len=<span class="hljs-number">100</span>,<br>    max_seq_len=<span class="hljs-number">1024</span>,<br>    batch_size=<span class="hljs-number">2</span>,<br>    concurrency=<span class="hljs-number">32</span>,<br>    meta_template=internlm_meta_template,<br>    run_cfg=<span class="hljs-built_in">dict</span>(num_gpus=<span class="hljs-number">1</span>, num_procs=<span class="hljs-number">1</span>),<br>)<br><br>models = [internlm_chat_7b]<br></code></pre></td></tr></table></figure>
<p>运行评测：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py configs/eval_internlm_my_deploy.py --debug<br></code></pre></td></tr></table></figure>
<p><img  src="awq.png"  ><span class="image-caption">加载量化后的模型</span></p>
<p><img  src="result-internlm-awq.png"  ><span class="image-caption">评判 internlm-awq</span></p>
<p>可见 internlm-AWQ 在 ceval 上的得分并不如 internlm2。</p>
<h3 id="使用-lmdeploy-0-2-0-转换-internlm2-为-awq-模型并进行评测"><a href="#使用-lmdeploy-0-2-0-转换-internlm2-为-awq-模型并进行评测" class="headerlink" title="使用 lmdeploy 0.2.0 转换 internlm2 为 awq 模型并进行评测"></a>使用 lmdeploy 0.2.0 转换 internlm2 为 awq 模型并进行评测</h3><p>使用 lmdeploy 0.2 的时候与 0.1 版本进行 AWQ 量化的方式略有不同，同时要从 huggingface 上下载测试数据集，所以国内可以使用镜像：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> HF_ENDPOINT=https://hf-mirror.com<br>lmdeploy lite auto_awq /root/share/model_repos/internlm2-chat-7b  --work-dir internlm2-chat-7b-4bit<br></code></pre></td></tr></table></figure>
<p>之后对模型进行转化：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy convert  internlm2-chat-7b ./internlm2-chat-7b-4bit/ --model-format awq --group-size 128  --dst-path  ./workspace_awq_internlm2<br></code></pre></td></tr></table></figure>
<p><img  src="convert.png"  ><span class="image-caption">转换模型</span></p>
<p>之后编写新的 config.py</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mmengine.config <span class="hljs-keyword">import</span> read_base<br><span class="hljs-keyword">from</span> opencompass.models.turbomind <span class="hljs-keyword">import</span> TurboMindModel<br><br><span class="hljs-keyword">with</span> read_base():<br>    <span class="hljs-comment"># choose a list of datasets</span><br>    <span class="hljs-keyword">from</span> .datasets.ceval.ceval_gen_5f30c7 <span class="hljs-keyword">import</span> ceval_datasets<br><br><br>datasets = [*ceval_datasets]<br><br>internlm_meta_template = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">round</span>=[<br>    <span class="hljs-built_in">dict</span>(role=<span class="hljs-string">&#x27;HUMAN&#x27;</span>, begin=<span class="hljs-string">&#x27;&lt;|User|&gt;:&#x27;</span>, end=<span class="hljs-string">&#x27;\n&#x27;</span>),<br>    <span class="hljs-built_in">dict</span>(role=<span class="hljs-string">&#x27;BOT&#x27;</span>, begin=<span class="hljs-string">&#x27;&lt;|Bot|&gt;:&#x27;</span>, end=<span class="hljs-string">&#x27;&lt;eoa&gt;\n&#x27;</span>, generate=<span class="hljs-literal">True</span>),<br>],<br>                              eos_token_id=<span class="hljs-number">103028</span>)<br><br><span class="hljs-comment"># config for internlm2-chat-7b-awq</span><br>internlm2_chat_7b = <span class="hljs-built_in">dict</span>(<br>    <span class="hljs-built_in">type</span>=TurboMindModel,<br>    abbr=<span class="hljs-string">&#x27;internlm-chat-7b&#x27;</span>,<br>    path=<span class="hljs-string">&#x27;/root/workspace_awq_internlm2&#x27;</span>,<br>    engine_config=<span class="hljs-built_in">dict</span>(session_len=<span class="hljs-number">2048</span>,<br>                       max_batch_size=<span class="hljs-number">32</span>,<br>                       rope_scaling_factor=<span class="hljs-number">1.0</span>),<br>    gen_config=<span class="hljs-built_in">dict</span>(top_k=<span class="hljs-number">1</span>,<br>                    top_p=<span class="hljs-number">0.8</span>,<br>                    temperature=<span class="hljs-number">1.0</span>,<br>                    max_new_tokens=<span class="hljs-number">100</span>),<br>    max_out_len=<span class="hljs-number">100</span>,<br>    max_seq_len=<span class="hljs-number">1024</span>,<br>    batch_size=<span class="hljs-number">2</span>,<br>    concurrency=<span class="hljs-number">32</span>,<br>    meta_template=internlm_meta_template,<br>    run_cfg=<span class="hljs-built_in">dict</span>(num_gpus=<span class="hljs-number">1</span>, num_procs=<span class="hljs-number">1</span>),<br>)<br><br>models = [internlm2_chat_7b]<br></code></pre></td></tr></table></figure>
<p>进行评测：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">python run.py configs/eval_internlm2_my_deploy.py --debug<br></code></pre></td></tr></table></figure>
<p><img  src="awq-result.png"  ><span class="image-caption">AWQ 量化评测</span></p>
<p>能够发现 AWQ 量化后的模型在 ceval 数据集上的得分比原模型要好。精度不仅没有明显下降，相反在不少任务上还有一定的提升。可能得原因是，量化会导致一定的误差，有时候这种误差可能会减少模型对训练数据的拟合，从而提高泛化性能。量化可以被视为引入轻微噪声的正则化方法。或者，也有可能量化后的模型正好对某些数据集具有更好的性能。</p>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
  <entry>
    <title>算法整理</title>
    <url>/uncategorized/leetcode/</url>
    <content><![CDATA[<h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><h2 id="rust-中-dbg-超时"><a href="#rust-中-dbg-超时" class="headerlink" title="rust 中 dbg! 超时"></a><code>rust</code> 中 <code>dbg!</code> 超时</h2><p>在 <code>rust</code> 中使用 <code>dbg!</code> 的时候，在题目判定时，可能会因为 <code>dbg!</code> 超时，提交代码的时候要去掉 <code>dbg!</code></p>
<h1 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h1><h2 id="双指针"><a href="#双指针" class="headerlink" title="双指针"></a>双指针</h2><p>第 27、977 题就是经典的双指针题目。</p>
<h2 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h2><p>注意，使用滑动窗口的时候，for 循环代表滑动窗口的结尾，否则又会陷入两个 for 的困境。</p>
<h1 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h1><p>链表对于有插入、交换或者删除的操作的时候，一般加一个虚拟头节点更好处理。</p>
<h2 id="双指针-1"><a href="#双指针-1" class="headerlink" title="双指针"></a>双指针</h2><p>经典的一个 pre 指针，一个 cur 指针：可以解决反转链表、交换节点等问题。<br>还有一个 fast 指针，一个 slow 指针：可以解决删除第 n 个元素的问题。</p>
<h1 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h1><h2 id="最长公共子串"><a href="#最长公共子串" class="headerlink" title="最长公共子串"></a>最长公共子串</h2><p>状态转移方程如下：</p>
<script type="math/tex; mode=display">
d p[i][j]=\left\{\begin{array}{l}
d p[i-1][j-1]+1, \text { 当且仅当 } x[i]=y[j] \\
0, \text { 当 } x[i] \ne y[j]
\end{array}\right.</script><p>按照上面方程实现的算法时间复杂度为 $O(n^2)$，空间复杂度为 $O(n^2)$。</p>
<p><img  src="../leetcode/d6f0b0e17ed6e13f5c042d172b1ddca782cb6aba589f5fcfea8944831614502f-image.png"  ><span class="image-caption">image.png</span></p>
<p>注意到，更新 $dp[i][j]$ 只需要上一列，即 $dp[i-1]$ 列，所以可以将空间复杂度降低为 $O(n)$，但是需要注意因为使用的是相同的数组列，所以字符串不相等时需要设置 $dp[j] = 0$，同时要注意从后向前更新数组，因为如果从前向后更新，那么当前的 $dp[j]$ 使用的是当前列刚刚更新过的数据，而我们需要的是上一列的数据，所以可以从后向前更新数据避免这个问题。</p>
<p>rust 代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">dp</span> = <span class="hljs-built_in">vec!</span>[<span class="hljs-number">0</span>; s2.<span class="hljs-title function_ invoke__">len</span>()];<br><span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..s1.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>    <span class="hljs-comment">// 逆序迭代是因为更新a[i][j]需要a[i-1][j-1]</span><br>    <span class="hljs-comment">// 现在是一个数组，所以 a[j] 是原来的 a[i][j]，而我们需要的是 a[i-1][j]</span><br>    <span class="hljs-comment">// 所以从后向前迭代，a[j] 是原来的 a[i-1][j]</span><br>    <span class="hljs-keyword">for</span> <span class="hljs-variable">j</span> <span class="hljs-keyword">in</span> (<span class="hljs-number">0</span>..s2.<span class="hljs-title function_ invoke__">len</span>()).<span class="hljs-title function_ invoke__">s2</span>() &#123;<br>        <span class="hljs-keyword">if</span> s[i] == s2[j] &#123;<br>            <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> || j == <span class="hljs-number">0</span> &#123;<br>                dp[j] = <span class="hljs-number">1</span>;<br>            &#125; <span class="hljs-keyword">else</span> &#123;<br>                dp[j] = dp[j - <span class="hljs-number">1</span>] + <span class="hljs-number">1</span>;<br>            &#125;<br>            <span class="hljs-keyword">if</span> dp[j] &gt; max_len &#123;<br>                <span class="hljs-keyword">let</span> <span class="hljs-variable">before_s2</span> = s2.<span class="hljs-title function_ invoke__">len</span>() - <span class="hljs-number">1</span> - j;<br>                <span class="hljs-keyword">if</span> before_s2 + dp[j] - <span class="hljs-number">1</span> == i &#123;<br>                    max_len = dp[j];<br>                    max_end = i;<br>                &#125;<br>            &#125;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">// 与之前不同，之前使用的是不同的列，所以不需要置0</span><br>            dp[j] = <span class="hljs-number">0</span>;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h2 id="最长回文子串"><a href="#最长回文子串" class="headerlink" title="最长回文子串"></a>最长回文子串</h2><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>将字符串倒置之后求最长公共子串（状态转移方程与最长公共子串相同），并判断是否为回文子串，这里回文子串「由倒置字符串推出的原字符串末尾下标」与「i」应该相等。</p>
<p>代码中 <code>longest_palindrome1</code> 的求最长公共子串空间复杂度为 $O(n^2)$，<code>longest_palindrome2</code> 的求最长公共子串空间复杂度为 $O(n)$。</p>
<p>代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Solution</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">longest_palindrome1</span>(s: <span class="hljs-type">String</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">String</span> &#123;<br>        <span class="hljs-keyword">if</span> s.<span class="hljs-title function_ invoke__">len</span>() &lt;= <span class="hljs-number">1</span> &#123;<br>            <span class="hljs-keyword">return</span> s;<br>        &#125;<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">rev</span>: <span class="hljs-type">String</span> = s.<span class="hljs-title function_ invoke__">chars</span>().<span class="hljs-title function_ invoke__">rev</span>().<span class="hljs-title function_ invoke__">collect</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">rev</span> = rev.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = s.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">dp</span> = <span class="hljs-built_in">vec!</span>[<span class="hljs-built_in">vec!</span>[<span class="hljs-number">0</span>; rev.<span class="hljs-title function_ invoke__">len</span>()]; s.<span class="hljs-title function_ invoke__">len</span>()];<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">max_len</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">max_end</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..s.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>            <span class="hljs-keyword">for</span> <span class="hljs-variable">j</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..rev.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>                <span class="hljs-keyword">if</span> s[i] == rev[j] &#123;<br>                    <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> || j == <span class="hljs-number">0</span> &#123;<br>                        dp[i][j] = <span class="hljs-number">1</span>;<br>                    &#125; <span class="hljs-keyword">else</span> &#123;<br>                        dp[i][j] = dp[i - <span class="hljs-number">1</span>][j - <span class="hljs-number">1</span>] + <span class="hljs-number">1</span>;<br>                    &#125;<br>                &#125;<br>                <span class="hljs-keyword">if</span> dp[i][j] &gt; max_len &#123;<br>                    <span class="hljs-comment">// 如果是回文串，那么「由倒置字符串推出的原字符串末尾下标」与「i」应该相等</span><br>                    <span class="hljs-comment">// 其中，倒置字符串的 rev.len() - 1 - j，也就是倒置之前的开始下标，减一是因为长度比下标多一</span><br>                    <span class="hljs-comment">// 再加上 dp[i][j] - 1，就是原字符串的末尾下标。abc，a的下标为0，长度为3，0+3为3，但是最大下标为2，所以需要减一</span><br>                    <span class="hljs-keyword">let</span> <span class="hljs-variable">before_rev</span> = rev.<span class="hljs-title function_ invoke__">len</span>() - <span class="hljs-number">1</span> - j;<br>                    <span class="hljs-keyword">if</span> before_rev + dp[i][j] - <span class="hljs-number">1</span> == i &#123;<br>                        max_len = dp[i][j];<br>                        max_end = i;<br>                    &#125;<br>                &#125;<br>            &#125;<br>        &#125;<br>        std::<span class="hljs-type">str</span>::<span class="hljs-title function_ invoke__">from_utf8</span>(&amp;s[max_end + <span class="hljs-number">1</span> - max_len..max_end + <span class="hljs-number">1</span>])<br>            .<span class="hljs-title function_ invoke__">unwrap</span>()<br>            .<span class="hljs-title function_ invoke__">to_string</span>()<br>    &#125;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">longest_palindrome2</span>(s: <span class="hljs-type">String</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">String</span> &#123;<br>        <span class="hljs-keyword">if</span> s.<span class="hljs-title function_ invoke__">len</span>() &lt; <span class="hljs-number">1</span> &#123;<br>            <span class="hljs-keyword">return</span> s;<br>        &#125;<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">rev</span>: <span class="hljs-type">String</span> = s.<span class="hljs-title function_ invoke__">chars</span>().<span class="hljs-title function_ invoke__">rev</span>().<span class="hljs-title function_ invoke__">collect</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">s</span> = s.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">rev</span> = rev.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">max_len</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">max_end</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">dp</span> = <span class="hljs-built_in">vec!</span>[<span class="hljs-number">0</span>; rev.<span class="hljs-title function_ invoke__">len</span>()];<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..s.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>            <span class="hljs-comment">// 逆序迭代是因为更新a[i][j]需要a[i-1][j-1]</span><br>            <span class="hljs-comment">// 现在是一个数组，所以 a[j] 是原来的 a[i][j]，而我们需要的是 a[i-1][j]</span><br>            <span class="hljs-comment">// 所以从后向前迭代，a[j] 是原来的 a[i-1][j]</span><br>            <span class="hljs-keyword">for</span> <span class="hljs-variable">j</span> <span class="hljs-keyword">in</span> (<span class="hljs-number">0</span>..rev.<span class="hljs-title function_ invoke__">len</span>()).<span class="hljs-title function_ invoke__">rev</span>() &#123;<br>                <span class="hljs-keyword">if</span> s[i] == rev[j] &#123;<br>                    <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> || j == <span class="hljs-number">0</span> &#123;<br>                        dp[j] = <span class="hljs-number">1</span>;<br>                    &#125; <span class="hljs-keyword">else</span> &#123;<br>                        dp[j] = dp[j - <span class="hljs-number">1</span>] + <span class="hljs-number">1</span>;<br>                    &#125;<br>                    <span class="hljs-keyword">if</span> dp[j] &gt; max_len &#123;<br>                        <span class="hljs-keyword">let</span> <span class="hljs-variable">before_rev</span> = rev.<span class="hljs-title function_ invoke__">len</span>() - <span class="hljs-number">1</span> - j;<br>                        <span class="hljs-keyword">if</span> before_rev + dp[j] - <span class="hljs-number">1</span> == i &#123;<br>                            max_len = dp[j];<br>                            max_end = i;<br>                        &#125;<br>                    &#125;<br>                &#125; <span class="hljs-keyword">else</span> &#123;<br>                    <span class="hljs-comment">// 与之前不同，之前使用的是不同的列，所以不需要置0</span><br>                    dp[j] = <span class="hljs-number">0</span>;<br>                &#125;<br>            &#125;<br>        &#125;<br>        std::<span class="hljs-type">str</span>::<span class="hljs-title function_ invoke__">from_utf8</span>(&amp;s[max_end + <span class="hljs-number">1</span> - max_len..max_end + <span class="hljs-number">1</span>])<br>            .<span class="hljs-title function_ invoke__">unwrap</span>()<br>            .<span class="hljs-title function_ invoke__">to_string</span>()<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h3 id="中心拓展算法"><a href="#中心拓展算法" class="headerlink" title="中心拓展算法"></a>中心拓展算法</h3><p>为了避免在之后的叙述中出现歧义，这里我们指出什么是“朴素算法”。</p>
<p>该算法通过下述方式工作：对每个中心位置 $i$ 在比较一对对应字符后，只要可能，该算法便尝试将答案加 $1$。</p>
<p>该算法是比较慢的：它只能在 $O(n^2)$ 的时间内计算答案。</p>
<p>该算法的实现如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// C++ Version</span><br><span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">d1</span><span class="hljs-params">(n)</span>, <span class="hljs-title">d2</span><span class="hljs-params">(n)</span></span>;<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) &#123;<br>  d1[i] = <span class="hljs-number">1</span>;<br>  <span class="hljs-keyword">while</span> (<span class="hljs-number">0</span> &lt;= i - d1[i] &amp;&amp; i + d1[i] &lt; n &amp;&amp; s[i - d1[i]] == s[i + d1[i]]) &#123;<br>    d1[i]++;<br>  &#125;<br><br>  d2[i] = <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">while</span> (<span class="hljs-number">0</span> &lt;= i - d2[i] - <span class="hljs-number">1</span> &amp;&amp; i + d2[i] &lt; n &amp;&amp;<br>         s[i - d2[i] - <span class="hljs-number">1</span>] == s[i + d2[i]]) &#123;<br>    d2[i]++;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Python Version</span><br>d1 = [<span class="hljs-number">0</span>] * n<br>d2 = [<span class="hljs-number">0</span>] * n<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n):<br>    d1[i] = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-number">0</span> &lt;= i - d1[i] <span class="hljs-keyword">and</span> i + d1[i] &lt; n <span class="hljs-keyword">and</span> s[i - d1[i]] == s[i + d1[i]]:<br>        d1[i] += <span class="hljs-number">1</span><br><br>    d2[i] = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-number">0</span> &lt;= i - d2[i] - <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> i + d2[i] &lt; n <span class="hljs-keyword">and</span> s[i - d2[i] - <span class="hljs-number">1</span>] == s[i + d2[i]]:<br>        d2[i] += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure>
<h3 id="Manacher-算法12"><a href="#Manacher-算法12" class="headerlink" title="Manacher 算法12"></a>Manacher 算法<sup><a href="#fn_1" id="reffn_1">1</a></sup><sup><a href="#fn_2" id="reffn_2">2</a></sup></h3><p>Manacher 算法是对中心拓展算法的优化，为了快速计算，我们维护已找到的最靠右的子回文串的 <strong>边界 $(l, r)$</strong>（即具有最大 $r$ 值的回文串，其中 $l$ 和 $r$ 分别为该回文串左右边界的位置）。初始时，我们置 $l = 0$ 和 $r = -1$（<em>-1</em>需区别于倒序索引位置，这里可为任意负数，仅为了循环初始时方便）。</p>
<p>现在假设我们要对下一个 $i$ 计算 $P[i]$，而之前所有 $P[]$ 中的值已计算完毕。我们将通过下列方式计算：</p>
<ul>
<li><p>如果 $i$ 位于当前子回文串之外，即 $i &gt; r$，那么我们调用朴素算法。</p>
<p>因此我们将连续地增加 $d_1[i]$，同时在每一步中检查当前的子串 $[i - P[i] \dots i +  P[i]]$（$P[i]$ 表示半径长度，下同）是否为一个回文串。如果我们找到了第一处对应字符不同，又或者碰到了 $s$  的边界，则算法停止。在两种情况下我们均已计算完 $P[i]$。此后，仍需记得更新 $(l, r)$。</p>
</li>
<li><p>现在考虑 $i \le r$ 的情况。我们将尝试从已计算过的 $P[]$ 的值中获取一些信息。首先在子回文串  $(l, r)$ 中反转位置 $i$，即我们得到 $j = l + (r - i)$。现在来考察值 $P[j]$。因为位置 $j$ 同位置  $i$ 对称，我们 <strong>几乎总是</strong> 可以置 $P[i] = P[j]$。</p>
<p>存在 <strong>棘手的情况</strong>，主要有以下：</p>
<ul>
<li><p>超出了 $r$</p>
<p><img  src="../leetcode/b0d52a5f30747e55ef09b3c7b7cfc23026e37040edc41f387263e8f8a0ba8f49-image.png"  ><span class="image-caption">图转自 LeetCode</span></p>
<p>当我们要求 $P [ i ]$ 的时候，$P [mirror] = 7$，而此时 $P [ i ]$ 并不等于 $7$，为什么呢，因为我们从 $i$ 开始往后数 $7$ 个，等于 $22$，已经超过了最右的 $r$，此时不能利用对称性了，但我们一定可以扩展到 $r$ 的，所以 $P [ i ]$ 至少等于 $r - i = 20 - 15 = 5$，会不会更大呢，我们只需要比较 $T [ r+1 ]$ 和 $T [ r+1 ]$ 关于 $i$ 的对称点就行了，就像中心扩展法一样一个个扩展。</p>
</li>
<li><p>$P[i]$ 遇到了原字符串的左边界</p>
<p><img  src="../leetcode/714e6f768e67304fb7162ecac3ae85fcf23ad82a21456e8ca55ac2c8cfd2609e-image.png"  ><span class="image-caption">image.png</span></p>
<p>此时$P [ i_{mirror} ] = 1$，但是 $P [ i ]$ 赋值成 1 是不正确的，出现这种情况的原因是 $P [ i_{mirror} ]$ 在扩展的时候首先是 “#” == “#”，之后遇到了 “^” 和另一个字符比较，也就是到了边界，才终止循环的。而 $P [ i ]$ 并没有遇到边界，所以我们可以继续通过中心扩展法一步一步向两边扩展就行了。</p>
</li>
<li><p>$i = r$</p>
<p>此时我们先把 P [ i ] 赋值为 0，然后通过中心扩展法一步一步扩展就行了。</p>
</li>
</ul>
<p>考虑 $r$ 的更新</p>
<p>就这样一步一步的求出每个 $P [ i ]$，当求出的 $P [ i ]$ 的右边界大于当前的 $r$ 时，我们就需要更新 $r$ 为当前的回文串了。</p>
</li>
</ul>
<h2 id="最长公共子序列（LCS）"><a href="#最长公共子序列（LCS）" class="headerlink" title="最长公共子序列（LCS）"></a>最长公共子序列（LCS）</h2><h3 id="动态规划-1"><a href="#动态规划-1" class="headerlink" title="动态规划"></a>动态规划</h3><p>状态转移方程如下：</p>
<script type="math/tex; mode=display">
d p[i][j]=\left\{\begin{array}{ll}
d p[i-1][j-1]+1, & t e x t_{1}[i-1]=t e x t_{2}[j-1] \\
\max (d p[i-1][j], d p[i][j-1]), & t e x t_{1}[i-1] \neq t e x t_{2}[j-1]
\end{array}\right.</script><p>LCS 对应的状态转移方程与最长公共子串不同之处在于：</p>
<ul>
<li>最长公共子串要求字符串连续，所以下一个状态只能由上一个对应的字符串得到。</li>
<li>LCS 不要求字符串连续，所以可以前后移动，就有了第二个式子。</li>
</ul>
<p>知道状态定义之后，我们开始写状态转移方程。</p>
<ul>
<li><p>当 $text_1[i - 1] = text_2[j - 1]$ 时，说明两个子字符串的最后一位相等，所以最长公共子序列又增加了 1，所以 $dp[i][j] = dp[i - 1][j - 1] + 1$；举个例子，比如对于 <code>ac</code> 和 <code>bc</code> 而言，他们的最长公共子序列的长度等于 <code>a</code> 和 <code>b</code> 的最长公共子序列长度 $0 + 1 = 1$。</p>
</li>
<li><p>当 $text_1[i - 1] \ne text_2[j - 1]$ 时，说明两个子字符串的最后一位不相等，那么此时的状态 $dp[i][j]$ 应该是 $dp[i - 1][j]$ 和 $dp[i][j - 1]$ 的最大值。举个例子，比如对于 <code>ace</code> 和 <code>bc</code> 而言，他们的最长公共子序列的长度等于</p>
<p> ① <code>ace</code> 和 <code>b</code> 的最长公共子序列长度 <code>0</code> 与</p>
<p>② <code>ac</code> 和 <code>bc</code> 的最长公共子序列长度 <code>1</code> 的最大值，即 <code>1</code>。</p>
</li>
</ul>
<p>代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Solution</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">longest_common_subsequence</span>(text1: <span class="hljs-type">String</span>, text2: <span class="hljs-type">String</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">i32</span> &#123;<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">text1</span> = text1.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">text2</span> = text2.<span class="hljs-title function_ invoke__">as_bytes</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">m</span> = text1.<span class="hljs-title function_ invoke__">len</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">n</span> = text2.<span class="hljs-title function_ invoke__">len</span>();<br>        <span class="hljs-comment">// dp[i][j] 代表 text1[0..i] 与 text2[0..j] 的最大子序列，注意不包括第 i 和第 j 个字符</span><br>        <span class="hljs-comment">// 同理，dp 数组要循环到 m 与 n 才结束</span><br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">dp</span> = <span class="hljs-built_in">vec!</span>[<span class="hljs-built_in">vec!</span>[<span class="hljs-number">0</span>; n + <span class="hljs-number">1</span>]; m + <span class="hljs-number">1</span>];<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>..=m &#123;<br>            <span class="hljs-keyword">for</span> <span class="hljs-variable">j</span> <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>..=n &#123;<br>                <span class="hljs-comment">// 这里要注意，比较的是第 i-1 与第 j-1 个字符</span><br>                <span class="hljs-keyword">if</span> text1[i - <span class="hljs-number">1</span>] == text2[j - <span class="hljs-number">1</span>] &#123;<br>                    dp[i][j] = dp[i - <span class="hljs-number">1</span>][j - <span class="hljs-number">1</span>] + <span class="hljs-number">1</span>;<br>                &#125; <span class="hljs-keyword">else</span> &#123;<br>                    dp[i][j] = std::cmp::<span class="hljs-title function_ invoke__">max</span>(dp[i][j - <span class="hljs-number">1</span>], dp[i - <span class="hljs-number">1</span>][j]);<br>                &#125;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> dp[m][n];<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h1 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h1><h2 id="寻找两个正序数组的中位数3"><a href="#寻找两个正序数组的中位数3" class="headerlink" title="寻找两个正序数组的中位数3"></a>寻找两个正序数组的中位数<sup><a href="#fn_3" id="reffn_3">3</a></sup></h2><p>中位数定义：将一个集合划分为两个长度相等的子集，其中一个子集中的元素总是大于另一个子集中的元素。</p>
<pre><code>      left_part          |         right_part
A[0], A[1], ..., A[i-1]  |  A[i], A[i+1], ..., A[m-1]
B[0], B[1], ..., B[j-1]  |  B[j], B[j+1], ..., B[n-1]
</code></pre><p>根据中位数的定义，我们需要找到以上的划分（设两个数组总长度为偶数）使得</p>
<ul>
<li>$\text{len}(left_part) = \text{len}(right_part)$</li>
<li>$\max(left_part)=\max(right_part)$</li>
</ul>
<p>此时的中位数为：</p>
<script type="math/tex; mode=display">\text{median} = \frac{\max(left\_part)+\min(right\_part)}{2}</script><p>所以现在的问题关键在于寻找这样一个划分。要寻找这样一个划分需要根据这个划分满足的两个条件：</p>
<ul>
<li>左边元素共有 $i + j$ 个，右边元素共有 $(m-i)+(n-j)$ 个，所以由第一个式子可以得到 $i+j=(m-i)+(n-j)$。变形得到 $i+j=\frac{m+n}{2}$。假设 $m &lt; n$，即 B 数组长于 A 数组，则 $i\in[0,m]$，有 $j = \frac{m+n}{2}-i$ 且 $j \in [0,n]$，所以只要知道 $i$ 的值，那么 $j$ 的值也是确定的。</li>
<li>在 $(0, m)$ 中找到 $i$，满足 $A[i-1] \le B[j]$ 且 $A[i] \ge B[j-1]$ 。</li>
</ul>
<p>注意到第一个条件中，当 $i$ 增大的时候，$j$ 会减小以此来保证左右两部分的元素个数相同。同时 A、B 数组都是单调不递减的，所以一定存在一个最大的 $i$ 满足 $A[i-1] \le B[j]$。（当 $i$ 取 $i+1$ 时 $A[i] &gt; B[j-1]$）</p>
<p>所以问题转化为：找一个最大的 $i$ 使得 $A[i-1] \le B[j]$。</p>
<p>对于这个问题，我们容易枚举 $i$，同时 A、B 都是单调递增的，所以我们还能知道枚举出的 $i$ 是不是满足条件（$A[i-1] \le B[j]$），并从中找出满足条件的最大 $i$ 值即可。</p>
<p>对于两个数组总长度为奇数的情况，可以使得 $j = \lfloor \frac{m+n+1}{2}-i \rfloor$。</p>
<p>代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-meta">#[warn(dead_code)]</span><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Solution</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">find_median_sorted_arrays</span>(nums1: <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">i32</span>&gt;, nums2: <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">i32</span>&gt;) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">f64</span> &#123;<br>        <span class="hljs-keyword">if</span> nums1.<span class="hljs-title function_ invoke__">len</span>() &gt; nums2.<span class="hljs-title function_ invoke__">len</span>() &#123;<br>            <span class="hljs-keyword">return</span> Solution::<span class="hljs-title function_ invoke__">find_median_sorted_arrays</span>(nums2, nums1);<br>        &#125;<br>        <span class="hljs-comment">// m &lt; n</span><br>        <span class="hljs-keyword">let</span> (m, n) = (nums1.<span class="hljs-title function_ invoke__">len</span>(), nums2.<span class="hljs-title function_ invoke__">len</span>());<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">left</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">right</span> = m;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">pos</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">median1</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">median2</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">while</span> left &lt;= right &#123;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">i</span> = (left + right) / <span class="hljs-number">2</span>;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">j</span> = (m + n + <span class="hljs-number">1</span>) / <span class="hljs-number">2</span> - i;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">nums_im1</span> = <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> &#123; -<span class="hljs-number">0x3f3f3f3f</span> &#125; <span class="hljs-keyword">else</span> &#123; nums1[i - <span class="hljs-number">1</span>] &#125;;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">nums_i</span> = <span class="hljs-keyword">if</span> i == m &#123; <span class="hljs-number">0x3f3f3f3f</span> &#125; <span class="hljs-keyword">else</span> &#123; nums1[i] &#125;;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">nums_jm1</span> = <span class="hljs-keyword">if</span> j == <span class="hljs-number">0</span> &#123; -<span class="hljs-number">0x3f3f3f3f</span> &#125; <span class="hljs-keyword">else</span> &#123; nums2[j - <span class="hljs-number">1</span>] &#125;;<br>            <span class="hljs-keyword">let</span> <span class="hljs-variable">nums_j</span> = <span class="hljs-keyword">if</span> j == n &#123; <span class="hljs-number">0x3f3f3f3f</span> &#125; <span class="hljs-keyword">else</span> &#123; nums2[j] &#125;;<br>            <span class="hljs-keyword">if</span> nums_im1 &lt;= nums_j &#123;<br>                median1 = std::cmp::<span class="hljs-title function_ invoke__">max</span>(nums_im1, nums_jm1);<br>                median2 = std::cmp::<span class="hljs-title function_ invoke__">min</span>(nums_i, nums_j);<br>                left = i + <span class="hljs-number">1</span>;<br>            &#125; <span class="hljs-keyword">else</span> &#123;<br>                right = i - <span class="hljs-number">1</span>;<br>            &#125;<br>        &#125;<br>        <span class="hljs-title function_ invoke__">if</span> (m + n) &amp; <span class="hljs-number">1</span> == <span class="hljs-number">0</span> &#123;<br>            (median1 + median2) <span class="hljs-keyword">as</span> <span class="hljs-type">f64</span> / <span class="hljs-number">2.0</span><br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            median1 <span class="hljs-keyword">as</span> <span class="hljs-type">f64</span><br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h2 id="三数之和"><a href="#三数之和" class="headerlink" title="三数之和"></a>三数之和</h2><h3 id="朴素算法"><a href="#朴素算法" class="headerlink" title="朴素算法"></a>朴素算法</h3><p>排序之后三重循环，判断三个数之和是否为 $0$，时间复杂度 $O(n^3)$。</p>
<p>排序的目的是为了容易地去除重复数字，因为排序之后只需要判断当前和前一个元素是否相等就可以知道是否是重复数字。</p>
<h3 id="排序后双指针"><a href="#排序后双指针" class="headerlink" title="排序后双指针"></a>排序后双指针</h3><p>注意到排序之后整个数组是单调非递减的，我们需要 $a+b+c=0$，当固定了 $a$ 和 $b$ 的时候，$c$ 从大到小地判断是否有 $a+b+c=0$ 即可。看似是最外层对应 $a$ 的循环嵌套对应 $b$ 的循环，并在其中加上了 $c$ 递减的循环，但是实际上注意到当 $b$ 与 $c$ 是同一个元素时，如果仍然不满足 $a+b+c=0$，那么 $c$ 继续向左减小就与之前的数字重复了，所以对于每一次 $b$ 中的循环，最多运行 $n$ 次，外边再嵌套 $a$ 的循环，时间复杂度为 $O(n^2)$。</p>
<p>代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-meta">#[warn(dead_code)]</span><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Solution</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">three_sum</span>(<span class="hljs-keyword">mut</span> nums: <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">i32</span>&gt;) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">Vec</span>&lt;<span class="hljs-type">i32</span>&gt;&gt; &#123;<br>        nums.<span class="hljs-title function_ invoke__">sort</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">len</span> = nums.<span class="hljs-title function_ invoke__">len</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">ans</span> = <span class="hljs-type">Vec</span>::<span class="hljs-title function_ invoke__">new</span>();<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..len &#123;<br>            <span class="hljs-comment">// 防止取到相同的数字</span><br>            <span class="hljs-keyword">if</span> i &gt; <span class="hljs-number">0</span> &amp;&amp; nums[i - <span class="hljs-number">1</span>] == nums[i] &#123;<br>                <span class="hljs-keyword">continue</span>;<br>            &#125;<br>            <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">third</span> = len - <span class="hljs-number">1</span>;<br>            <span class="hljs-comment">// 注意这里开始位置是 i+1，目的是为了不与 a 取重</span><br>            <span class="hljs-keyword">for</span> <span class="hljs-variable">j</span> <span class="hljs-keyword">in</span> i + <span class="hljs-number">1</span>..len &#123;<br>                <span class="hljs-comment">// 注意这里判定条件是 j &gt; i+1 否则会取不到与 a 相同的数字</span><br>                <span class="hljs-keyword">if</span> j &gt; i + <span class="hljs-number">1</span> &amp;&amp; nums[j - <span class="hljs-number">1</span>] == nums[j] &#123;<br>                    <span class="hljs-keyword">continue</span>;<br>                &#125;<br>                <span class="hljs-keyword">while</span> j &lt; third &amp;&amp; nums[i] + nums[j] + nums[third] &gt; <span class="hljs-number">0</span> &#123;<br>                    third = third - <span class="hljs-number">1</span>;<br>                &#125;<br>                <span class="hljs-keyword">if</span> j == third &#123;<br>                    <span class="hljs-keyword">break</span>;<br>                &#125;<br>                <span class="hljs-keyword">if</span> nums[i] + nums[j] + nums[third] == <span class="hljs-number">0</span> &#123;<br>                    ans.<span class="hljs-title function_ invoke__">push</span>(<span class="hljs-built_in">vec!</span>[nums[i], nums[j], nums[third]]);<br>                &#125;<br>            &#125;<br>        &#125;<br>        ans<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h2 id="盛最多水的容器4"><a href="#盛最多水的容器4" class="headerlink" title="盛最多水的容器4"></a>盛最多水的容器<sup><a href="#fn_4" id="reffn_4">4</a></sup></h2><script type="math/tex; mode=display">
area = (right - left) * \min (height[left], height[right])</script><p>由上面的公式可以知道，面积由两部分共同决定：</p>
<ul>
<li>宽度</li>
<li>高度</li>
</ul>
<p>所以考虑尽可能地增加宽度和高度。假设左指针指向的数为 $x$，右指针指向的数为 $y$，假设 $x &lt; y$，距离为 $t$，接下来进行具体分析：</p>
<ol>
<li>水量 $ area = \min(x, y) <em> t = x </em> t $，当左指针不变的时候，右指针无论在哪都不会影响容器的水量了，水量是固定的 $x*t$。</li>
<li>所以考虑左指针向右移动，这样才有可能取到更大的水量。</li>
<li>同理左指针指向的数大于右指针指向的数的时候，左移右指针才有可能取到更大的水量。</li>
<li>重复以上步骤就可以得到最大水量。</li>
</ol>
<p>总时间复杂度为 $O(n)$。</p>
<p>注解：</p>
<ul>
<li>对于双指针问题，两个指针的初始位置不一定都在最左或者最右，要灵活地设置指针位置。</li>
</ul>
<h2 id="最接近三数之和"><a href="#最接近三数之和" class="headerlink" title="最接近三数之和"></a>最接近三数之和</h2><p>与「盛最多水的容器」和「三数之和」类似，代码如下：</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-meta">#[warn(dead_code)]</span><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Solution</span>;<br><br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">three_sum_closest</span>(<span class="hljs-keyword">mut</span> nums: <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">i32</span>&gt;, target: <span class="hljs-type">i32</span>) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">i32</span> &#123;<br>        nums.<span class="hljs-title function_ invoke__">sort</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-variable">len</span> = nums.<span class="hljs-title function_ invoke__">len</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">ans</span> = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">diff</span> = <span class="hljs-number">0x3f3f3f3f</span>;<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">i</span> <span class="hljs-keyword">in</span> <span class="hljs-number">0</span>..len &#123;<br>            <span class="hljs-keyword">if</span> i &gt; <span class="hljs-number">0</span> &amp;&amp; nums[i] == nums[i - <span class="hljs-number">1</span>] &#123;<br>                <span class="hljs-keyword">continue</span>;<br>            &#125;<br>            <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">j</span> = i + <span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">k</span> = len - <span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">while</span> j &lt; k &#123;<br>                <span class="hljs-comment">//dbg!((i, j , k));</span><br>                <span class="hljs-keyword">let</span> <span class="hljs-variable">sum</span> = nums[i] + nums[j] + nums[k];<br>                <span class="hljs-keyword">if</span> sum == target &#123;<br>                    <span class="hljs-keyword">return</span> sum;<br>                &#125;<br>                <span class="hljs-keyword">let</span> <span class="hljs-variable">tmp</span> = (sum - target).<span class="hljs-title function_ invoke__">abs</span>();<br>                <span class="hljs-keyword">if</span> tmp &lt; diff &#123;<br>                    diff = tmp;<br>                    ans = sum;<br>                &#125;<br>                <span class="hljs-keyword">if</span> sum &gt; target &#123;<br>                    <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">k0</span> = k - <span class="hljs-number">1</span>;<br>                    <span class="hljs-keyword">while</span> j &lt; k0 &amp;&amp; nums[k0] == nums[k] &#123;<br>                        k0 = k0 - <span class="hljs-number">1</span>;<br>                    &#125;<br>                    k = k0;<br>                &#125; <span class="hljs-keyword">else</span> &#123;<br>                    <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">j0</span> = j + <span class="hljs-number">1</span>;<br>                    <span class="hljs-keyword">while</span> j0 &lt; k &amp;&amp; nums[j0] == nums[j] &#123;<br>                        j0 = j0 + <span class="hljs-number">1</span>;<br>                    &#125;<br>                    j = j0;<br>                &#125;<br>            &#125;<br>        &#125;<br>        ans<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="合并K个升序链表"><a href="#合并K个升序链表" class="headerlink" title="合并K个升序链表"></a>合并K个升序链表</h2><p>使用优先队列即可。</p>
<figure class="highlight rust"><table><tr><td class="code"><pre><code class="hljs rust"><span class="hljs-keyword">use</span> std::&#123;cmp::Reverse, collections::BinaryHeap&#125;;<br><span class="hljs-keyword">impl</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">pub</span> <span class="hljs-keyword">fn</span> <span class="hljs-title function_">merge_k_lists</span>(lists: <span class="hljs-type">Vec</span>&lt;<span class="hljs-type">Option</span>&lt;<span class="hljs-type">Box</span>&lt;ListNode&gt;&gt;&gt;) <span class="hljs-punctuation">-&gt;</span> <span class="hljs-type">Option</span>&lt;<span class="hljs-type">Box</span>&lt;ListNode&gt;&gt; &#123;<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">priority_queue</span> = BinaryHeap::<span class="hljs-title function_ invoke__">new</span>();<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">ret</span> = <span class="hljs-type">Box</span>::<span class="hljs-title function_ invoke__">new</span>(ListNode::<span class="hljs-title function_ invoke__">new</span>(<span class="hljs-number">0</span>));<br>        <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">ptr</span> = &amp;<span class="hljs-keyword">mut</span> ret;<br>        <span class="hljs-keyword">for</span> <span class="hljs-variable">list</span> <span class="hljs-keyword">in</span> lists &#123;<br>            <span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">plist</span> = &amp;list;<br>            <span class="hljs-keyword">while</span> <span class="hljs-keyword">let</span> <span class="hljs-variable">Some</span>(node) = plist &#123;<br>                priority_queue.<span class="hljs-title function_ invoke__">push</span>(<span class="hljs-title function_ invoke__">Reverse</span>(node.val));<br>                plist = &amp;node.next;<br>            &#125;<br>        &#125;<br><br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">let</span> <span class="hljs-variable">Some</span>(<span class="hljs-title function_ invoke__">Reverse</span>(node)) = priority_queue.<span class="hljs-title function_ invoke__">pop</span>() &#123;<br>            ptr.next = <span class="hljs-title function_ invoke__">Some</span>(<span class="hljs-type">Box</span>::<span class="hljs-title function_ invoke__">new</span>(ListNode::<span class="hljs-title function_ invoke__">new</span>(node)));<br>            ptr = ptr.next.<span class="hljs-title function_ invoke__">as_mut</span>().<span class="hljs-title function_ invoke__">unwrap</span>();<br>        &#125;<br>        ret.next<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote id="fn_1">
<sup>1</sup>. <a href="https://leetcode-cn.com/problems/longest-palindromic-substring/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-bao-gu">https://leetcode-cn.com/problems/longest-palindromic-substring/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-bao-gu</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. <a href="https://oi-wiki.org/string/manacher/">https://oi-wiki.org/string/manacher/</a><a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. <a href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/solution/xun-zhao-liang-ge-you-xu-shu-zu-de-zhong-wei-s-114/">https://leetcode-cn.com/problems/median-of-two-sorted-arrays/solution/xun-zhao-liang-ge-you-xu-shu-zu-de-zhong-wei-s-114/</a><a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. <a href="https://leetcode-cn.com/problems/container-with-most-water/solution/sheng-zui-duo-shui-de-rong-qi-by-leetcode-solution/">https://leetcode-cn.com/problems/container-with-most-water/solution/sheng-zui-duo-shui-de-rong-qi-by-leetcode-solution/</a><a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a>
</blockquote>
]]></content>
      <tags>
        <tag>data structure, algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Internlm-05-LMDeploy 的量化和部署</title>
    <url>/internlm/internlm-05/</url>
    <content><![CDATA[<h1 id="LMDeploy-的量化和部署"><a href="#LMDeploy-的量化和部署" class="headerlink" title="LMDeploy 的量化和部署"></a>LMDeploy 的量化和部署</h1><h2 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1 环境配置"></a>1 环境配置</h2><p>这里 <code>/share/conda_envs</code> 目录下的环境是官方未大家准备好的基础环境，因为该目录是共享只读的，而我们后面需要在此基础上安装新的软件包，所以需要复制到我们自己的 conda 环境（该环境下我们是可写的）。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">conda create -n lmdeploy --<span class="hljs-built_in">clone</span> /share/conda_envs/internlm-base<br></code></pre></td></tr></table></figure>
<ul>
<li>如果clone操作过慢，可采用如下操作:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">/root/share/install_conda_env_internlm_base.sh lmdeploy<br></code></pre></td></tr></table></figure>
<p>我们取 <code>CONDA_ENV_NAME</code> 为 <code>lmdeploy</code>，复制完成后，可以在本地查看环境。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">conda <span class="hljs-built_in">env</span> list<br></code></pre></td></tr></table></figure>
<p>结果如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># conda environments:</span><br><span class="hljs-comment">#</span><br>base                  *  /root/.conda<br>lmdeploy                 /root/.conda/envs/lmdeploy<br></code></pre></td></tr></table></figure>
<p>然后激活环境。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">conda activate lmdeploy<br></code></pre></td></tr></table></figure>
<p>如果是在 InternStudio 开发环境，需要先运行下面的命令，否则会报错。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 解决 ModuleNotFoundError: No module named &#x27;packaging&#x27; 问题</span><br>pip install packaging<br><span class="hljs-comment"># 使用 flash_attn 的预编译包解决安装过慢问题</span><br>pip install /root/share/wheels/flash_attn-2.4.2+cu118torch2.0cxx11abiTRUE-cp310-cp310-linux_x86_64.whl<br></code></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">pip install <span class="hljs-string">&#x27;lmdeploy[all]==v0.1.0&#x27;</span><br></code></pre></td></tr></table></figure>
<p>由于默认安装的是 runtime 依赖包，但是我们这里还需要部署和量化，所以，这里选择 <code>[all]</code>。然后可以再检查一下 lmdeploy 包，如下图所示。</p>
<p><img  src="env.png"  ><span class="image-caption">安装 lmdeploy 成功</span></p>
<p>基础环境到这里就配置好了。</p>
<h2 id="2-服务部署"><a href="#2-服务部署" class="headerlink" title="2 服务部署"></a>2 服务部署</h2><p>这一部分主要涉及本地推理和部署。我们先看一张图。</p>
<p><img  src="lmdeploy.drawio.png"  ><span class="image-caption">服务架构图</span></p>
<p>lmdeploy 从架构上把整个服务流程分成下面几个模块。</p>
<ul>
<li>模型推理/服务。主要提供模型本身的推理，一般来说可以和具体业务解耦，专注模型推理本身性能的优化。可以以模块、API等多种方式提供。</li>
<li>Client。可以理解为前端，与用户交互的地方。</li>
<li>API Server。一般作为前端的后端，提供与产品和服务相关的数据和功能支持。</li>
</ul>
<p>值得说明的是，以上的划分是一个相对完整的模型，但在实际中这并不是绝对的。比如可以把“模型推理”和“API Server”合并，有的甚至是三个流程打包在一起提供服务。</p>
<p>接下来，我们看一下 lmdeploy 提供的部署功能。</p>
<h3 id="2-1-模型转换"><a href="#2-1-模型转换" class="headerlink" title="2.1 模型转换"></a>2.1 模型转换</h3><p>使用 TurboMind 推理模型需要先将模型转化为 TurboMind 的格式，目前支持在线转换和离线转换两种形式。在线转换可以直接加载 Huggingface 模型，离线转换需需要先保存模型再加载。</p>
<p>TurboMind 是一款关于 LLM 推理的高效推理引擎，基于英伟达的 <a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a> 研发而成。它的主要功能包括：LLaMa 结构模型的支持，persistent batch 推理模式和可扩展的 KV 缓存管理器。</p>
<h4 id="2-1-1-在线转换"><a href="#2-1-1-在线转换" class="headerlink" title="2.1.1 在线转换"></a>2.1.1 在线转换</h4><p>lmdeploy 支持直接读取 Huggingface 模型权重，目前共支持三种类型：</p>
<ul>
<li>在 huggingface.co 上面通过 lmdeploy 量化的模型，如 <a href="https://huggingface.co/lmdeploy/llama2-chat-70b-4bit">llama2-70b-4bit</a>, <a href="https://huggingface.co/internlm/internlm-chat-20b-4bit">internlm-chat-20b-4bit</a></li>
<li>huggingface.co 上面其他 LM 模型，如 Qwen/Qwen-7B-Chat</li>
</ul>
<p>示例如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 需要能访问 Huggingface 的网络环境</span><br>lmdeploy chat turbomind internlm/internlm-chat-20b-4bit --model-name internlm-chat-20b<br>lmdeploy chat turbomind Qwen/Qwen-7B-Chat --model-name qwen-7b<br></code></pre></td></tr></table></figure>
<p>上面两行命令分别展示了如何直接加载 Huggingface 的模型，第一条命令是加载使用 lmdeploy 量化的版本，第二条命令是加载其他 LLM 模型。</p>
<p>我们也可以直接启动本地的 Huggingface 模型，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/  --model-name internlm-chat-7b<br></code></pre></td></tr></table></figure>
<p>以上命令都会启动一个本地对话界面，通过 Bash 可以与 LLM 进行对话。</p>
<h4 id="2-1-2-离线转换"><a href="#2-1-2-离线转换" class="headerlink" title="2.1.2 离线转换"></a>2.1.2 离线转换</h4><p>离线转换需要在启动服务之前，将模型转为 lmdeploy TurboMind  的格式，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 转换模型（FastTransformer格式） TurboMind</span><br>lmdeploy convert internlm-chat-7b /path/to/internlm-chat-7b<br></code></pre></td></tr></table></figure>
<p>这里我们使用官方提供的模型文件，就在用户根目录执行，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy convert internlm-chat-7b  /root/share/temp/model_repos/internlm-chat-7b/<br></code></pre></td></tr></table></figure>
<p><img  src="convert.png"  ><span class="image-caption">转换模型</span></p>
<p>执行完成后将会在当前目录生成一个 <code>workspace</code> 的文件夹。这里面包含的就是 TurboMind 和 Triton “模型推理”需要到的文件。</p>
<p>目录如下图所示。</p>
<p><img  src="tree1.png"  ><span class="image-caption">转换后的模型</span></p>
<p><code>weights</code> 和 <code>tokenizer</code> 目录分别放的是拆分后的参数和 Tokenizer。如果我们进一步查看 <code>weights</code> 的目录，就会发现参数是按层和模块拆开的，如下图所示。</p>
<p><img  src="tree2.png"  ><span class="image-caption">转换后的权重</span></p>
<p>每一份参数第一个 0 表示“层”的索引，后面的那个0表示 Tensor 并行的索引，因为我们只有一张卡，所以被拆分成 1 份。如果有两张卡可以用来推理，则会生成0和1两份，也就是说，会把同一个参数拆成两份。比如 <code>layers.0.attention.w_qkv.0.weight</code> 会变成 <code>layers.0.attention.w_qkv.0.weight</code> 和 <code>layers.0.attention.w_qkv.1.weight</code>。执行 <code>lmdeploy convert</code> 命令时，可以通过 <code>--tp</code> 指定（tp 表示 tensor parallel），该参数默认值为1（也就是一张卡）。</p>
<p><strong>关于Tensor并行</strong></p>
<p>Tensor并行一般分为行并行或列并行，原理如下图所示。</p>
<p><img  src="col.png"  ><span class="image-caption">列并行</span></p>
<p><img  src="row.png"  ><span class="image-caption">行并行</span></p>
<p>简单来说，就是把一个大的张量（参数）分到多张卡上，分别计算各部分的结果，然后再同步汇总。</p>
<h3 id="2-2-TurboMind-推理-命令行本地对话"><a href="#2-2-TurboMind-推理-命令行本地对话" class="headerlink" title="2.2  TurboMind 推理+命令行本地对话"></a>2.2  TurboMind 推理+命令行本地对话</h3><p>模型转换完成后，我们就具备了使用模型推理的条件，接下来就可以进行真正的模型推理环节。</p>
<p>我们先尝试本地对话（<code>Bash Local Chat</code>），下面用（Local Chat 表示）在这里其实是跳过 API Server 直接调用 TurboMind。简单来说，就是命令行代码直接执行 TurboMind。所以说，实际和前面的架构图是有区别的。</p>
<p>这里支持多种方式运行，比如Turbomind、PyTorch、DeepSpeed。但 PyTorch 和 DeepSpeed 调用的其实都是 Huggingface 的 Transformers 包，PyTorch表示原生的 Transformer 包，DeepSpeed 表示使用了 DeepSpeed 作为推理框架。Pytorch/DeepSpeed 目前功能都比较弱，不具备生产能力，不推荐使用。</p>
<p>执行命令如下。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Turbomind + Bash Local Chat</span><br>lmdeploy chat turbomind ./workspace<br></code></pre></td></tr></table></figure>
<p>启动后就可以和它进行对话了，如下图所示。</p>
<p><img  src="chat.png"  ><span class="image-caption">与本地部署的模型进行对话</span></p>
<p>输入后两次回车，退出时输入<code>exit</code> 回车两次即可。此时，Server 就是本地跑起来的模型（TurboMind），命令行可以看作是前端。</p>
<h3 id="2-3-TurboMind推理-API服务"><a href="#2-3-TurboMind推理-API服务" class="headerlink" title="2.3 TurboMind推理+API服务"></a>2.3 TurboMind推理+API服务</h3><p>在上面的部分我们尝试了直接用命令行启动 Client，接下来我们尝试如何运用 lmdepoy 进行服务化。</p>
<p>”模型推理/服务“目前提供了 Turbomind 和 TritonServer 两种服务化方式。此时，Server 是 TurboMind 或 TritonServer，API Server 可以提供对外的 API 服务。我们推荐使用 TurboMind，TritonServer 使用方式详见《附录1》。</p>
<p>首先，通过下面命令启动服务。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ApiServer+Turbomind   api_server =&gt; AsyncEngine =&gt; TurboMind</span><br>lmdeploy serve api_server ./workspace \<br>	--server_name 0.0.0.0 \<br>	--server_port 23333 \<br>	--instance_num 64 \<br>	--tp 1<br></code></pre></td></tr></table></figure>
<p>上面的参数中 <code>server_name</code> 和 <code>server_port</code> 分别表示服务地址和端口，<code>tp</code> 参数我们之前已经提到过了，表示 Tensor 并行。还剩下一个 <code>instance_num</code> 参数，表示实例数，可以理解成 Batch 的大小。执行后如下图所示。</p>
<p><img  src="api-deploy.png"  ><span class="image-caption">部署 api server</span></p>
<p>然后，我们可以新开一个窗口，执行下面的 Client 命令。如果使用官方机器，可以打开 vscode 的 Terminal，执行下面的命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ChatApiClient+ApiServer（注意是http协议，需要加http）</span><br>lmdeploy serve api_client http://localhost:23333<br></code></pre></td></tr></table></figure>
<p>如下图所示。</p>
<p><img  src="test-apiserver.png"  ><span class="image-caption">测试 api server</span></p>
<p>当然，刚刚我们启动的是 API Server，自然也有相应的接口。可以直接打开 <code>http://&#123;host&#125;:23333</code> 查看，如下图所示。</p>
<p><img  src="fastapi.png"  ><span class="image-caption">fastapi 演示</span></p>
<p>这里一共提供了 4 个 HTTP 的接口，任何语言都可以对其进行调用，我们以 <code>v1/chat/completions</code> 接口为例，简单试一下。</p>
<p>接口请求参数如下：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;model&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;internlm-chat-7b&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;messages&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;写一首冬天的诗&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;temperature&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0.7</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;top_p&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;n&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;max_tokens&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">512</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;stop&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;stream&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;presence_penalty&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;frequency_penalty&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;user&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;string&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;repetition_penalty&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;renew_session&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;ignore_eos&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure>
<p>请求结果如下。</p>
<p><img  src="test-fastapi.png"  ><span class="image-caption">测试 api</span></p>
<h3 id="2-4-网页-Demo-演示"><a href="#2-4-网页-Demo-演示" class="headerlink" title="2.4 网页 Demo 演示"></a>2.4 网页 Demo 演示</h3><p>这一部分主要是将 Gradio 作为前端 Demo 演示。在上一节的基础上，我们不执行后面的 <code>api_client</code> 或 <code>triton_client</code>，而是执行 <code>gradio</code>。</p>
<h4 id="2-4-1-TurboMind-服务作为后端"><a href="#2-4-1-TurboMind-服务作为后端" class="headerlink" title="2.4.1 TurboMind 服务作为后端"></a>2.4.1 TurboMind 服务作为后端</h4><p>API Server 的启动和上一节一样，这里直接启动作为前端的 Gradio。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Gradio+ApiServer。必须先开启 Server，此时 Gradio 为 Client</span><br>lmdeploy serve gradio http://0.0.0.0:23333 \<br>	--server_name 0.0.0.0 \<br>	--server_port 6006 \<br>	--restful_api True<br></code></pre></td></tr></table></figure>
<h4 id="2-4-2-TurboMind-推理作为后端"><a href="#2-4-2-TurboMind-推理作为后端" class="headerlink" title="2.4.2 TurboMind 推理作为后端"></a>2.4.2 TurboMind 推理作为后端</h4><p>当然，Gradio 也可以直接和 TurboMind 连接，如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Gradio+Turbomind(local)</span><br>lmdeploy serve gradio ./workspace<br></code></pre></td></tr></table></figure>
<p>可以直接启动 Gradio，此时没有 API Server，TurboMind 直接与 Gradio 通信。</p>
<h3 id="2-5-TurboMind-推理-Python-代码集成"><a href="#2-5-TurboMind-推理-Python-代码集成" class="headerlink" title="2.5 TurboMind 推理 + Python 代码集成"></a>2.5 TurboMind 推理 + Python 代码集成</h3><p>前面介绍的都是通过 API 或某种前端与”模型推理/服务“进行交互，lmdeploy 还支持 Python 直接与 TurboMind 进行交互，如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> lmdeploy <span class="hljs-keyword">import</span> turbomind <span class="hljs-keyword">as</span> tm<br><br><span class="hljs-comment"># load model</span><br>model_path = <span class="hljs-string">&quot;/root/share/temp/model_repos/internlm-chat-7b/&quot;</span><br>tm_model = tm.TurboMind.from_pretrained(model_path, model_name=<span class="hljs-string">&#x27;internlm-chat-20b&#x27;</span>)<br>generator = tm_model.create_instance()<br><br><span class="hljs-comment"># process query</span><br>query = <span class="hljs-string">&quot;你好啊兄嘚&quot;</span><br>prompt = tm_model.model.get_prompt(query)<br>input_ids = tm_model.tokenizer.encode(prompt)<br><br><span class="hljs-comment"># inference</span><br><span class="hljs-keyword">for</span> outputs <span class="hljs-keyword">in</span> generator.stream_infer(<br>        session_id=<span class="hljs-number">0</span>,<br>        input_ids=[input_ids]):<br>    res, tokens = outputs[<span class="hljs-number">0</span>]<br><br>response = tm_model.tokenizer.decode(res.tolist())<br><span class="hljs-built_in">print</span>(response)<br></code></pre></td></tr></table></figure>
<p>在上面的代码中，我们首先加载模型，然后构造输入，最后执行推理。</p>
<p>加载模型可以显式指定模型路径，也可以直接指定 Huggingface 的 repo_id，还可以使用上面生成过的 <code>workspace</code>。这里的 <code>tm.TurboMind</code> 其实是对 C++ TurboMind 的封装。</p>
<p>构造输入这里主要是把用户的 query 构造成 InternLLM 支持的输入格式，比如上面的例子中， <code>query</code> 是“你好啊兄嘚”，构造好的 Prompt 如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">&lt;|System|&gt;:You are an AI assistant whose name is InternLM (书生·浦语).</span><br><span class="hljs-string">- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span><br><span class="hljs-string">- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span><br><span class="hljs-string"></span><br><span class="hljs-string">&lt;|User|&gt;:你好啊兄嘚</span><br><span class="hljs-string">&lt;|Bot|&gt;:</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>
<p>Prompt 其实就是增加了 <code>&lt;|System|&gt;</code> 消息和 <code>&lt;|User|&gt;</code> 消息（即用户的 <code>query</code>），以及一个 <code>&lt;|Bot|&gt;</code> 的标记，表示接下来该模型输出响应了。最终输出的响应内容如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;你好啊，有什么我可以帮助你的吗？&quot;</span><br></code></pre></td></tr></table></figure>
<h3 id="2-6-最佳实践"><a href="#2-6-最佳实践" class="headerlink" title="2.6 最佳实践"></a>2.6 最佳实践</h3><h4 id="2-6-1-方案实践"><a href="#2-6-1-方案实践" class="headerlink" title="2.6.1 方案实践"></a>2.6.1 方案实践</h4><p>首先说 “模型推理/服务”，推荐使用 TurboMind，使用简单，性能良好，相关的 Benchmark 对比如下。</p>
<p><img  src="benchmark.png"  ><span class="image-caption">Benchmark 效果</span></p>
<p>上面的性能对比包括两个场景：</p>
<ul>
<li>场景一（前4张图）：固定的输入、输出 token 数（分别1和2048），测试Token输出吞吐量（output token throughput）。</li>
<li>场景二（第5张图）：使用真实数据，测试吞吐量（request throughput）。</li>
</ul>
<p>场景一中，BatchSize=64时，TurboMind 的吞吐量超过 2000 token/s，整体比 DeepSpeed 提升约 5% - 15%；BatchSize=32时，比 Huggingface 的Transformers 提升约 3 倍；其他BatchSize时 TurboMind 也表现出优异的性能。</p>
<p>场景二中，对比了 TurboMind 和 vLLM 在真实数据上的吞吐量（request throughput）指标，TurboMind 的效率比 vLLM 高 30%。</p>
<p>大家不妨亲自使用本地对话（Local Chat）感受一下性能差别（2.2 节），也可以执行我们提供的 <code>infer_compare.py</code> 脚本，示例如下。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 执行 Huggingface 的 Transformer</span><br>python infer_compare.py hf<br><span class="hljs-comment"># 执行LMDeploy</span><br>python infer_compare.py lmdeploy<br></code></pre></td></tr></table></figure>
<p>LMDeploy应该是Transformers的3-5倍左右。</p>
<p>后面的 API 服务和 Client 就得分场景了。</p>
<ul>
<li>我想对外提供类似 OpenAI 那样的 HTTP 接口服务。推荐使用 TurboMind推理 + API 服务（2.3）。</li>
<li>我想做一个演示 Demo，Gradio 无疑是比 Local Chat 更友好的。推荐使用 TurboMind 推理作为后端的Gradio进行演示（2.4.2）。</li>
<li>我想直接在自己的 Python 项目中使用大模型功能。推荐使用 TurboMind推理 + Python（2.5）。</li>
<li>我想在自己的其他非 Python 项目中使用大模型功能。推荐直接通过 HTTP 接口调用服务。也就是用本列表第一条先启动一个 HTTP API 服务，然后在项目中直接调用接口。</li>
</ul>
<h4 id="2-6-2-模型配置实践"><a href="#2-6-2-模型配置实践" class="headerlink" title="2.6.2 模型配置实践"></a>2.6.2 模型配置实践</h4><p>不知道大家还有没有印象，在离线转换（2.1.2）一节，我们查看了 <code>weights</code> 的目录，里面存放的是模型按层、按并行卡拆分的参数，不过还有一个文件 <code>config.ini</code> 并不是模型参数，它里面存的主要是模型相关的配置信息。下面是一个示例。</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-section">[llama]</span><br><span class="hljs-attr">model_name</span> = internlm-chat-<span class="hljs-number">7</span>b<br><span class="hljs-attr">tensor_para_size</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">head_num</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">kv_head_num</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">vocab_size</span> = <span class="hljs-number">103168</span><br><span class="hljs-attr">num_layer</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">inter_size</span> = <span class="hljs-number">11008</span><br><span class="hljs-attr">norm_eps</span> = <span class="hljs-number">1</span>e-<span class="hljs-number">06</span><br><span class="hljs-attr">attn_bias</span> = <span class="hljs-number">0</span><br><span class="hljs-attr">start_id</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">end_id</span> = <span class="hljs-number">2</span><br><span class="hljs-attr">session_len</span> = <span class="hljs-number">2056</span><br><span class="hljs-attr">weight_type</span> = fp16<br><span class="hljs-attr">rotary_embedding</span> = <span class="hljs-number">128</span><br><span class="hljs-attr">rope_theta</span> = <span class="hljs-number">10000.0</span><br><span class="hljs-attr">size_per_head</span> = <span class="hljs-number">128</span><br><span class="hljs-attr">group_size</span> = <span class="hljs-number">0</span><br><span class="hljs-attr">max_batch_size</span> = <span class="hljs-number">64</span><br><span class="hljs-attr">max_context_token_num</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">step_length</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">cache_max_entry_count</span> = <span class="hljs-number">0.5</span><br><span class="hljs-attr">cache_block_seq_len</span> = <span class="hljs-number">128</span><br><span class="hljs-attr">cache_chunk_size</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">use_context_fmha</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">quant_policy</span> = <span class="hljs-number">0</span><br><span class="hljs-attr">max_position_embeddings</span> = <span class="hljs-number">2048</span><br><span class="hljs-attr">rope_scaling_factor</span> = <span class="hljs-number">0.0</span><br><span class="hljs-attr">use_logn_attn</span> = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>
<p>其中，模型属性相关的参数不可更改，主要包括下面这些。</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">model_name</span> = llama2<br><span class="hljs-attr">head_num</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">kv_head_num</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">vocab_size</span> = <span class="hljs-number">103168</span><br><span class="hljs-attr">num_layer</span> = <span class="hljs-number">32</span><br><span class="hljs-attr">inter_size</span> = <span class="hljs-number">11008</span><br><span class="hljs-attr">norm_eps</span> = <span class="hljs-number">1</span>e-<span class="hljs-number">06</span><br><span class="hljs-attr">attn_bias</span> = <span class="hljs-number">0</span><br><span class="hljs-attr">start_id</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">end_id</span> = <span class="hljs-number">2</span><br><span class="hljs-attr">rotary_embedding</span> = <span class="hljs-number">128</span><br><span class="hljs-attr">rope_theta</span> = <span class="hljs-number">10000.0</span><br><span class="hljs-attr">size_per_head</span> = <span class="hljs-number">128</span><br></code></pre></td></tr></table></figure>
<p>和数据类型相关的参数也不可更改，主要包括两个。</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">weight_type</span> = fp16<br><span class="hljs-attr">group_size</span> = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>
<p><code>weight_type</code> 表示权重的数据类型。目前支持 fp16 和 int4。int4 表示 4bit 权重。当 <code>weight_type</code> 为 4bit 权重时，<code>group_size</code> 表示 <code>awq</code> 量化权重时使用的 group 大小。</p>
<p>剩余参数包括下面几个。</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">tensor_para_size</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">session_len</span> = <span class="hljs-number">2056</span><br><span class="hljs-attr">max_batch_size</span> = <span class="hljs-number">64</span><br><span class="hljs-attr">max_context_token_num</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">step_length</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">cache_max_entry_count</span> = <span class="hljs-number">0.5</span><br><span class="hljs-attr">cache_block_seq_len</span> = <span class="hljs-number">128</span><br><span class="hljs-attr">cache_chunk_size</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">use_context_fmha</span> = <span class="hljs-number">1</span><br><span class="hljs-attr">quant_policy</span> = <span class="hljs-number">0</span><br><span class="hljs-attr">max_position_embeddings</span> = <span class="hljs-number">2048</span><br><span class="hljs-attr">rope_scaling_factor</span> = <span class="hljs-number">0.0</span><br><span class="hljs-attr">use_logn_attn</span> = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>
<p>一般情况下，我们并不需要对这些参数进行修改，但有时候为了满足特定需要，可能需要调整其中一部分配置值。这里主要介绍三个可能需要调整的参数。</p>
<ul>
<li>KV int8 开关：<ul>
<li>对应参数为 <code>quant_policy</code>，默认值为 0，表示不使用 KV Cache，如果需要开启，则将该参数设置为 4。</li>
<li>KV Cache 是对序列生成过程中的 K 和 V 进行量化，用以节省显存。我们下一部分会介绍具体的量化过程。</li>
<li>当显存不足，或序列比较长时，建议打开此开关。</li>
</ul>
</li>
<li>外推能力开关：<ul>
<li>对应参数为 <code>rope_scaling_factor</code>，默认值为 0.0，表示不具备外推能力，设置为 1.0，可以开启 RoPE 的 Dynamic NTK 功能，支持长文本推理。另外，<code>use_logn_attn</code> 参数表示 Attention 缩放，默认值为 0，如果要开启，可以将其改为 1。</li>
<li>外推能力是指推理时上下文的长度超过训练时的最大长度时模型生成的能力。如果没有外推能力，当推理时上下文长度超过训练时的最大长度，效果会急剧下降。相反，则下降不那么明显，当然如果超出太多，效果也会下降的厉害。</li>
<li>当推理文本非常长（明显超过了训练时的最大长度）时，建议开启外推能力。</li>
</ul>
</li>
<li>批处理大小：<ul>
<li>对应参数为 <code>max_batch_size</code>，默认为 64，也就是我们在 API Server 启动时的 <code>instance_num</code> 参数。</li>
<li>该参数值越大，吞度量越大（同时接受的请求数），但也会占用更多显存。</li>
<li>建议根据请求量和最大的上下文长度，按实际情况调整。</li>
</ul>
</li>
</ul>
<h2 id="3-模型量化"><a href="#3-模型量化" class="headerlink" title="3 模型量化"></a>3 模型量化</h2><p>本部分内容主要介绍如何对模型进行量化。主要包括 KV Cache 量化和模型参数量化。总的来说，量化是一种以参数或计算中间结果精度下降换空间节省（以及同时带来的性能提升）的策略。</p>
<p>正式介绍 LMDeploy 量化方案前，需要先介绍两个概念：</p>
<ul>
<li>计算密集（compute-bound）: 指推理过程中，绝大部分时间消耗在数值计算上；针对计算密集型场景，可以通过使用更快的硬件计算单元来提升计算速。</li>
<li>访存密集（memory-bound）: 指推理过程中，绝大部分时间消耗在数据读取上；针对访存密集型场景，一般通过减少访存次数、提高计算访存比或降低访存量来优化。</li>
</ul>
<p>常见的 LLM 模型由于 Decoder Only 架构的特性，实际推理时大多数的时间都消耗在了逐 Token 生成阶段（Decoding 阶段），是典型的访存密集型场景。</p>
<p>那么，如何优化 LLM 模型推理中的访存密集问题呢？ 我们可以使用 <strong>KV Cache 量化</strong>和 <strong>4bit Weight Only 量化（W4A16）</strong>。KV Cache 量化是指将逐 Token（Decoding）生成过程中的上下文 K 和 V 中间结果进行 INT8 量化（计算时再反量化），以降低生成过程中的显存占用。4bit Weight 量化，将 FP16 的模型权重量化为 INT4，Kernel 计算时，访存量直接降为 FP16 模型的 1/4，大幅降低了访存成本。Weight Only 是指仅量化权重，数值计算依然采用 FP16（需要将 INT4 权重反量化）。</p>
<h3 id="3-1-KV-Cache-量化"><a href="#3-1-KV-Cache-量化" class="headerlink" title="3.1 KV Cache 量化"></a>3.1 KV Cache 量化</h3><h4 id="3-1-1-量化步骤"><a href="#3-1-1-量化步骤" class="headerlink" title="3.1.1 量化步骤"></a>3.1.1 量化步骤</h4><p>KV Cache 量化是将已经生成序列的 KV 变成 Int8，使用过程一共包括三步：</p>
<p>第一步：计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况。</p>
<ul>
<li>对于 Attention 的 K 和 V：取每个 Head 各自维度在所有Token的最大、最小和绝对值最大值。对每一层来说，上面三组值都是 <code>(num_heads, head_dim)</code> 的矩阵。这里的统计结果将用于本小节的 KV Cache。</li>
<li>对于模型每层的输入：取对应维度的最大、最小、均值、绝对值最大和绝对值均值。每一层每个位置的输入都有对应的统计值，它们大多是 <code>(hidden_dim, )</code> 的一维向量，当然在 FFN 层由于结构是先变宽后恢复，因此恢复的位置维度并不相同。这里的统计结果用于下个小节的模型参数量化，主要用在缩放环节（回顾PPT内容）。</li>
</ul>
<p>第一步执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 计算 minmax</span><br>lmdeploy lite calibrate \<br>  --model  /root/share/temp/model_repos/internlm-chat-7b/ \<br>  --calib_dataset <span class="hljs-string">&quot;c4&quot;</span> \<br>  --calib_samples 128 \<br>  --calib_seqlen 2048 \<br>  --work_dir ./quant_output<br></code></pre></td></tr></table></figure>
<p>在这个命令行中，会选择 128 条输入样本，每条样本长度为 2048，数据集选择 C4，输入模型后就会得到上面的各种统计值。值得说明的是，如果显存不足，可以适当调小 samples 的数量或 sample 的长度。</p>
<blockquote>
<p>这一步由于默认需要从 Huggingface 下载数据集，国内经常不成功。所以我们导出了需要的数据，大家需要对读取数据集的代码文件做一下替换。共包括两步：</p>
<ul>
<li>第一步：复制 <code>calib_dataloader.py</code> 到安装目录替换该文件：<code>cp /root/share/temp/datasets/c4/calib_dataloader.py  /root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/utils/</code></li>
<li>第二步：将用到的数据集（c4）复制到下面的目录：<code>cp -r /root/share/temp/datasets/c4/ /root/.cache/huggingface/datasets/</code> </li>
</ul>
</blockquote>
<p>第二步：通过 minmax 获取量化参数。主要就是利用下面这个公式，获取每一层的 K V 中心值（zp）和缩放值（scale）。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">zp = (min+max) / 2<br>scale = (max-min) / 255<br>quant: q = round( (f-zp) / scale)<br>dequant: f = q * scale + zp<br></code></pre></td></tr></table></figure>
<p>有这两个值就可以进行量化和解量化操作了。具体来说，就是对历史的 K 和 V 存储 quant 后的值，使用时在 dequant。</p>
<p>第二步的执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 通过 minmax 获取量化参数</span><br>lmdeploy lite kv_qparams \<br>  --work_dir ./quant_output  \<br>  --turbomind_dir workspace/triton_models/weights/ \<br>  --kv_sym False \<br>  --num_tp 1<br></code></pre></td></tr></table></figure>
<p>在这个命令中，<code>num_tp</code> 的含义前面介绍过，表示 Tensor 的并行数。每一层的中心值和缩放值会存储到 <code>workspace</code> 的参数目录中以便后续使用。<code>kv_sym</code> 为 <code>True</code> 时会使用另一种（对称）量化方法，它用到了第一步存储的绝对值最大值，而不是最大值和最小值。</p>
<p>第三步：修改配置。也就是修改 <code>weights/config.ini</code> 文件，这个我们在《2.6.2 模型配置实践》中已经提到过了（KV int8 开关），只需要把 <code>quant_policy</code> 改为 4 即可。</p>
<p>这一步需要额外说明的是，如果用的是 TurboMind1.0，还需要修改参数 <code>use_context_fmha</code>，将其改为 0。</p>
<p>接下来就可以正常运行前面的各种服务了，只不过咱们现在可是用上了 KV Cache 量化，能更省（运行时）显存了。</p>
<h4 id="3-1-2-量化效果"><a href="#3-1-2-量化效果" class="headerlink" title="3.1.2 量化效果"></a>3.1.2 量化效果</h4><p>官方给出了 <a href="https://huggingface.co/internlm/internlm-chat-7b">internlm-chat-7b</a> 模型在 KV Cache 量化前后的显存对比情况，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>batch_size</th>
<th>fp16 memory(MiB)</th>
<th>int8 memory(MiB)</th>
<th>diff(MiB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>22337</td>
<td>18241</td>
<td>-4096</td>
</tr>
<tr>
<td>16</td>
<td>30593</td>
<td>22369</td>
<td>-8224</td>
</tr>
<tr>
<td>32</td>
<td>47073</td>
<td>30625</td>
<td>-16448</td>
</tr>
<tr>
<td>48</td>
<td>63553</td>
<td>38881</td>
<td>-24672</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，KV Cache 可以节约大约 20% 的显存。</p>
<p>同时，还在 <a href="https://github.com/open-compass/opencompass">opencompass</a> 平台上测试了量化前后的精准度（Accuracy）对比情况，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>task</th>
<th>dataset</th>
<th>metric</th>
<th>int8</th>
<th>fp16</th>
<th>diff</th>
</tr>
</thead>
<tbody>
<tr>
<td>Language</td>
<td>winogrande</td>
<td>accuracy</td>
<td>60.77</td>
<td>61.48</td>
<td>-0.71</td>
</tr>
<tr>
<td>Knowledge</td>
<td>nq</td>
<td>score</td>
<td>2.69</td>
<td>2.60</td>
<td>+0.09</td>
</tr>
<tr>
<td>Reasoning</td>
<td>gsm8k</td>
<td>accuracy</td>
<td>33.28</td>
<td>34.72</td>
<td>-1.44</td>
</tr>
<tr>
<td>Reasoning</td>
<td>bbh</td>
<td>naive_average</td>
<td>20.12</td>
<td>20.51</td>
<td>-0.39</td>
</tr>
<tr>
<td>Understanding</td>
<td>openbookqa_fact</td>
<td>accuracy</td>
<td>82.40</td>
<td>82.20</td>
<td>+0.20</td>
</tr>
<tr>
<td>Understanding</td>
<td>eprstmt-dev</td>
<td>accuracy</td>
<td>90.62</td>
<td>88.75</td>
<td>+1.87</td>
</tr>
<tr>
<td>Safety</td>
<td>crows_pairs</td>
<td>accuracy</td>
<td>32.56</td>
<td>31.43</td>
<td>+1.13</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，精度不仅没有明显下降，相反在不少任务上还有一定的提升。可能的原因是，量化会导致一定的误差，有时候这种误差可能会减少模型对训练数据的拟合，从而提高泛化性能。量化可以被视为引入轻微噪声的正则化方法。或者，也有可能量化后的模型正好对某些数据集具有更好的性能。</p>
<p>总结一下，KV Cache 量化既能明显降低显存占用，还有可能同时带来精准度（Accuracy）的提升。</p>
<h3 id="3-2-W4A16-量化"><a href="#3-2-W4A16-量化" class="headerlink" title="3.2 W4A16 量化"></a>3.2 W4A16 量化</h3><h4 id="3-2-1-量化步骤"><a href="#3-2-1-量化步骤" class="headerlink" title="3.2.1 量化步骤"></a>3.2.1 量化步骤</h4><p>W4A16中的A是指Activation，保持FP16，只对参数进行 4bit 量化。使用过程也可以看作是三步。</p>
<p>第一步：同 3.1.1，不再赘述。</p>
<p>第二步：量化权重模型。利用第一步得到的统计值对参数进行量化，具体又包括两小步：</p>
<ul>
<li>缩放参数。主要是性能上的考虑（回顾 PPT）。</li>
<li>整体量化。</li>
</ul>
<p>第二步的执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 量化权重模型</span><br>lmdeploy lite auto_awq \<br>  --model  /root/share/temp/model_repos/internlm-chat-7b/ \<br>  --w_bits 4 \<br>  --w_group_size 128 \<br>  --work_dir ./quant_output<br></code></pre></td></tr></table></figure>
<p>命令中 <code>w_bits</code> 表示量化的位数，<code>w_group_size</code> 表示量化分组统计的尺寸，<code>work_dir</code> 是量化后模型输出的位置。这里需要特别说明的是，因为没有 <code>torch.int4</code>，所以实际存储时，8个 4bit 权重会被打包到一个 int32 值中。所以，如果你把这部分量化后的参数加载进来就会发现它们是 int32 类型的。</p>
<p>最后一步：转换成 TurboMind 格式。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 转换模型的layout，存放在默认路径 ./workspace 下</span><br>lmdeploy convert  internlm-chat-7b ./quant_output \<br>    --model-format awq \<br>    --group-size 128<br></code></pre></td></tr></table></figure>
<p>这个 <code>group-size</code> 就是上一步的那个 <code>w_group_size</code>。如果不想和之前的 <code>workspace</code> 重复，可以指定输出目录：<code>--dst_path</code>，比如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy convert  internlm-chat-7b ./quant_output \<br>    --model-format awq \<br>    --group-size 128 \<br>    --dst_path ./workspace_quant<br></code></pre></td></tr></table></figure>
<p>接下来和上一节一样，可以正常运行前面的各种服务了，不过咱们现在用的是量化后的模型。</p>
<p>最后再补充一点，量化模型和 KV Cache 量化也可以一起使用，以达到最大限度节省显存。</p>
<h4 id="3-2-2-量化效果"><a href="#3-2-2-量化效果" class="headerlink" title="3.2.2 量化效果"></a>3.2.2 量化效果</h4><p>官方在 NVIDIA GeForce RTX 4090 上测试了 4-bit 的 Llama-2-7B-chat 和 Llama-2-13B-chat 模型的 token 生成速度。测试配置为 BatchSize = 1，prompt_tokens=1，completion_tokens=512，结果如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>model</th>
<th>llm-awq</th>
<th>mlc-llm</th>
<th>turbomind</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-7B-chat</td>
<td>112.9</td>
<td>159.4</td>
<td>206.4</td>
</tr>
<tr>
<td>Llama-2-13B-chat</td>
<td>N/A</td>
<td>90.7</td>
<td>115.8</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，TurboMind 相比其他框架速度优势非常显著，比 mlc-llm 快了将近 30%。</p>
<p>另外，也测试了 TurboMind 在不同精度和上下文长度下的显存占用情况，如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>model(context length)</th>
<th>16bit(2048)</th>
<th>4bit(2048)</th>
<th>16bit(4096)</th>
<th>4bit(4096)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-2-7B-chat</td>
<td>15.1</td>
<td>6.3</td>
<td>16.2</td>
<td>7.5</td>
</tr>
<tr>
<td>Llama-2-13B-chat</td>
<td>OOM</td>
<td>10.3</td>
<td>OOM</td>
<td>12.0</td>
</tr>
</tbody>
</table>
</div>
<p>可以看出，4bit 模型可以降低 50-60% 的显存占用，效果非常明显。</p>
<p>总而言之，W4A16 参数量化后能极大地降低显存，同时相比其他框架推理速度具有明显优势。</p>
<h3 id="3-3-最佳实践"><a href="#3-3-最佳实践" class="headerlink" title="3.3 最佳实践"></a>3.3 最佳实践</h3><p>本节是针对《模型量化》部分的最佳实践。</p>
<p>首先我们需要明白一点，服务部署和量化是没有直接关联的，量化的最主要目的是降低显存占用，主要包括两方面的显存：模型参数和中间过程计算结果。前者对应《3.2 W4A16 量化》，后者对应《3.1 KV Cache 量化》。</p>
<p>量化在降低显存的同时，一般还能带来性能的提升，因为更小精度的浮点数要比高精度的浮点数计算效率高，而整型要比浮点数高很多。</p>
<p>所以我们的建议是：在各种配置下尝试，看效果能否满足需要。这一般需要在自己的数据集上进行测试。具体步骤如下。</p>
<ul>
<li>Step1：优先尝试正常（非量化）版本，评估效果。<ul>
<li>如果效果不行，需要尝试更大参数模型或者微调。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step2：尝试正常版本+KV Cache 量化，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step3：尝试量化版本，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，跳到下一步。</li>
</ul>
</li>
<li>Step4：尝试量化版本+ KV Cache 量化，评估效果。<ul>
<li>如果效果不行，回到上一步。</li>
<li>如果效果可以，使用方案。</li>
</ul>
</li>
</ul>
<p>简单流程如下图所示。</p>
<p><img  src="quant.png"  ><span class="image-caption">量化流程图</span></p>
<p>另外需要补充说明的是，使用哪种量化版本、开启哪些功能，除了上述流程外，<strong>还需要考虑框架、显卡的支持情况</strong>，比如有些框架可能不支持 W4A16 的推理，那即便转换好了也用不了。</p>
<p>根据实践经验，一般情况下：</p>
<ul>
<li>精度越高，显存占用越多，推理效率越低，但一般效果较好。</li>
<li>Server 端推理一般用非量化版本或半精度、BF16、Int8 等精度的量化版本，比较少使用更低精度的量化版本。</li>
<li>端侧推理一般都使用量化版本，且大多是低精度的量化版本。这主要是因为计算资源所限。</li>
</ul>
<p>以上是针对项目开发情况，如果是自己尝试（玩儿）的话：</p>
<ul>
<li>如果资源足够（有GPU卡很重要），那就用非量化的正常版本。</li>
<li>如果没有 GPU 卡，只有 CPU（不管什么芯片），那还是尝试量化版本。</li>
<li>如果生成文本长度很长，显存不够，就开启 KV Cache。</li>
</ul>
<p>建议大家根据实际情况灵活选择方案。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://github.com/InternLM/lmdeploy/">InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs.</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/665725861">仅需一块 3090 显卡，高效部署 InternLM-20B 模型 - 知乎</a></li>
</ul>
<h2 id="附录1：TritonServer-作为推理引擎"><a href="#附录1：TritonServer-作为推理引擎" class="headerlink" title="附录1：TritonServer 作为推理引擎"></a>附录1：TritonServer 作为推理引擎</h2><h3 id="TritonServer环境配置"><a href="#TritonServer环境配置" class="headerlink" title="TritonServer环境配置"></a>TritonServer环境配置</h3><blockquote>
<p>注意：本部分内容仅支持物理机上执行，不支持虚拟主机。</p>
</blockquote>
<p>使用 Triton Server 需要安装一下 Docker 及其他依赖。</p>
<p>先装一些基本的依赖。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">apt-get update<br>apt-get install cmake sudo -y<br></code></pre></td></tr></table></figure>
<p>然后是 Docker 安装。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Add Docker&#x27;s official GPG key:</span><br>sudo apt-get install ca-certificates curl gnupg<br>sudo install -m 0755 -d /etc/apt/keyrings<br>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg<br>sudo <span class="hljs-built_in">chmod</span> a+r /etc/apt/keyrings/docker.gpg<br><br><span class="hljs-comment"># Add the repository to Apt sources:</span><br><span class="hljs-built_in">echo</span> \<br>  <span class="hljs-string">&quot;deb [arch=<span class="hljs-subst">$(dpkg --print-architecture)</span> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \</span><br><span class="hljs-string">  <span class="hljs-subst">$(. /etc/os-release &amp;&amp; echo <span class="hljs-string">&quot;<span class="hljs-variable">$VERSION_CODENAME</span>&quot;</span>)</span> stable&quot;</span> | \<br>  sudo <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/docker.list &gt; /dev/null<br>sudo apt-get update<br><br><span class="hljs-comment"># install</span><br>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin<br></code></pre></td></tr></table></figure>
<p>安装后我们跑一个 HelloWorld。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># helloworld</span><br>sudo docker run hello-world<br></code></pre></td></tr></table></figure>
<p>可以看到类似下面的画面，表示运行成功。</p>
<p><img  src="triton-server.png"  ><span class="image-caption">triton server</span></p>
<h3 id="TritonServer推理-API服务"><a href="#TritonServer推理-API服务" class="headerlink" title="TritonServer推理+API服务"></a>TritonServer推理+API服务</h3><blockquote>
<p>注意：这部分需要 Docker 服务。</p>
</blockquote>
<p>这里我们把提供模型推理服务的引擎从 TurboMind 换成了 TritonServer，启动命令就一行。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ApiServer+Triton</span><br>bash workspace/service_docker_up.sh<br></code></pre></td></tr></table></figure>
<p>这里会启动一个 TritonServer 的容器，如下图所示。</p>
<p><img src="triton-server-run.png" alt=""></p>
<p>可以再开一个窗口执行 Client 命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ChatTritonClient + TritonServer（注意是gRPC协议，不要用http）</span><br>lmdeploy serve triton_client  localhost:33337<br></code></pre></td></tr></table></figure>
<p>结果如下图所示。</p>
<p><img src="triton-client.png" alt=""></p>
<h3 id="TritonServer-服务作为后端"><a href="#TritonServer-服务作为后端" class="headerlink" title="TritonServer 服务作为后端"></a>TritonServer 服务作为后端</h3><p>使用过程同 2.4.1 小节。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Gradio+TritonServer（注意是gRPC协议，不要用http）</span><br>lmdeploy serve gradio localhost:33337 \<br>	--server_name 0.0.0.0 \<br>	--server_port 6006<br></code></pre></td></tr></table></figure>
<p>结果如下图所示。</p>
<p><img src="triton-client-web.png" alt=""></p>
<h2 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h2><h3 id="TurboMind推理-API服务"><a href="#TurboMind推理-API服务" class="headerlink" title="TurboMind推理+API服务"></a>TurboMind推理+API服务</h3><p><img  src="story.png"  ><span class="image-caption">部署并使用 api 生成 300 字小故事</span></p>
<h3 id="TurboMind-推理-命令行本地对话"><a href="#TurboMind-推理-命令行本地对话" class="headerlink" title="TurboMind 推理+命令行本地对话"></a>TurboMind 推理+命令行本地对话</h3><p><img  src="chat.png"  ><span class="image-caption">与本地部署的模型进行对话</span></p>
<h3 id="TurboMind-推理-gradio"><a href="#TurboMind-推理-gradio" class="headerlink" title="TurboMind 推理+gradio"></a>TurboMind 推理+gradio</h3><p><img  src="gradio.png"  ><span class="image-caption">Gradio 部署</span></p>
<h3 id="KV-Cache-量化部署"><a href="#KV-Cache-量化部署" class="headerlink" title="KV Cache 量化部署"></a>KV Cache 量化部署</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 计算 minmax</span><br>lmdeploy lite calibrate \<br>  --model  /root/share/temp/model_repos/internlm-chat-7b/ \<br>  --calib_dataset <span class="hljs-string">&quot;c4&quot;</span> \<br>  --calib_samples 128 \<br>  --calib_seqlen 2048 \<br>  --work_dir ./quant_output<br></code></pre></td></tr></table></figure>
<p>这里在计算的时候需要下载 calibrate 数据集 c4，国内经常不成功。所以需要手动下载后对读取数据集的代码文件做一下替换。共包括两步：</p>
<ul>
<li>第一步：复制 <code>calib_dataloader.py</code> 到安装目录替换该文件：<code>cp /root/share/temp/datasets/c4/calib_dataloader.py  /root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/utils/</code></li>
<li>第二步：将用到的数据集（c4）复制到下面的目录：<code>cp -r /root/share/temp/datasets/c4/ /root/.cache/huggingface/datasets/</code></li>
</ul>
<p><img  src="max.png"  ><span class="image-caption">计算每一层的最大 GPU 占用</span></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 通过 minmax 获取量化参数</span><br>lmdeploy lite kv_qparams \<br>  --work_dir ./quant_output  \<br>  --turbomind_dir workspace/triton_models/weights/ \<br>  --kv_sym False \<br>  --num_tp 1<br></code></pre></td></tr></table></figure>
<p><img  src="qparam.png"  ><span class="image-caption">获取量化参数</span></p>
<p>之后修改 <code>weights/config.ini</code> 文件 <code>quant_policy=4</code> 表示开启 KV Cache 量化。</p>
<p>部署运行内存占用</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">watch vgpu-smi<br></code></pre></td></tr></table></figure>
<p><img  src="kv-cache-memory.png"  ><span class="image-caption">KV Cache 内存占用</span></p>
<h3 id="W4A16-量化"><a href="#W4A16-量化" class="headerlink" title="W4A16 量化"></a>W4A16 量化</h3><figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 计算 minmax</span><br>lmdeploy lite calibrate \<br>  --model  /root/share/temp/model_repos/internlm-chat-7b/ \<br>  --calib_dataset <span class="hljs-string">&quot;c4&quot;</span> \<br>  --calib_samples 128 \<br>  --calib_seqlen 2048 \<br>  --work_dir ./quant_output_awq<br></code></pre></td></tr></table></figure>
<p>之后量化权重</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 量化权重模型</span><br>lmdeploy lite auto_awq \<br>  --model  /root/share/temp/model_repos/internlm-chat-7b/ \<br>  --w_bits 4 \<br>  --w_group_size 128 \<br>  --work_dir ./quant_output_awq<br></code></pre></td></tr></table></figure>
<p>命令中 <code>w_bits</code> 表示量化的位数，<code>w_group_size</code> 表示量化分组统计的尺寸，<code>work_dir</code> 是量化后模型输出的位置。这里需要特别说明的是，因为没有 <code>torch.int4</code>，所以实际存储时，8个 4bit 权重会被打包到一个 int32 值中。所以，如果你把这部分量化后的参数加载进来就会发现它们是 int32 类型的。</p>
<p><img  src="awq.png"  ><span class="image-caption">AWQ 量化</span></p>
<p>最后一步：转换成 TurboMind 格式。</p>
<p>这个 <code>group-size</code> 就是上一步的那个 <code>w_group_size</code>。如果不想和之前的 <code>workspace</code> 重复，可以指定输出目录：<code>--dst_path</code>，比如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy convert  internlm-chat-7b ./quant_output_awq \<br>    --model-format awq \<br>    --group-size 128 \<br>    --dst_path ./workspace_quant_awq4<br></code></pre></td></tr></table></figure>
<p><img  src="convert-awq.png"  ><span class="image-caption">转换为 TurboMind 格式</span></p>
<p>目录结构如下：</p>
<p><img  src="tree-awq.png"  ><span class="image-caption">转换为 TurboMind 格式的 AWQ 模型结构</span></p>
<p>部署使用</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">lmdeploy chat turbomind ./workspace_quant_awq4<br></code></pre></td></tr></table></figure>
<p>内存占用</p>
<p><img  src="awq-memory.png"  ><span class="image-caption">AWQ 量化内存占用</span></p>
<p>可见内存占用只有 6G。</p>
]]></content>
      <categories>
        <category>internlm</category>
      </categories>
  </entry>
</search>
